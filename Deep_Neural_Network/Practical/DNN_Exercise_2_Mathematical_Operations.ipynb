{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸ§® T3-Exercise-2: Mathematical Operations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ LEARNING OBJECTIVES\n",
        "By the end of this exercise, you will:\n",
        "- âš¡ Master element-wise operations (the building blocks of neural computations)\n",
        "- ğŸ¯ Understand matrix multiplication (the heart of neural networks)\n",
        "- ğŸ”„ Learn broadcasting (making tensors work together efficiently)\n",
        "- ğŸ§  Apply mathematical operations in real neural network scenarios\n",
        "- ğŸ” Debug shape mismatches and mathematical errors\n",
        "\n",
        "## ğŸ”— CONNECTION TO NEURAL NETWORKS\n",
        "Mathematics is the **engine** that powers neural networks:\n",
        "- **Element-wise operations** â†’ Activation functions, normalization\n",
        "- **Matrix multiplication** â†’ Layer transformations (input Ã— weights)\n",
        "- **Broadcasting** â†’ Efficient batch processing\n",
        "- **Reduction operations** â†’ Loss calculation, metrics\n",
        "\n",
        "**Real Example:** When an image passes through a neural layer:  \n",
        "`output = activation(input @ weights + bias)` â† All math operations!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## âš™ï¸ SETUP & ENVIRONMENT CHECK\n",
        "ğŸš€ Let's power up our mathematical toolkit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "setup_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04da68e-36e6-4fca-97d0-398c56b9ff4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ MATHEMATICAL TOOLKIT CHECK\n",
            "===================================\n",
            "ğŸ Python: 3.12.11\n",
            "ğŸ”¥ TensorFlow: 2.19.0\n",
            "ğŸ”¢ NumPy: 2.0.2\n",
            "ğŸ’» CPU computation: READY (Perfect for learning)\n",
            "\n",
            "ğŸ‰ Ready to explore the mathematics of intelligence!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ› ï¸ Essential imports for mathematical operations\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# ğŸ”§ Environment verification\n",
        "print(\"ğŸ”§ MATHEMATICAL TOOLKIT CHECK\")\n",
        "print(\"=\" * 35)\n",
        "print(f\"ğŸ Python: {sys.version.split()[0]}\")\n",
        "print(f\"ğŸ”¥ TensorFlow: {tf.__version__}\")\n",
        "print(f\"ğŸ”¢ NumPy: {np.__version__}\")\n",
        "\n",
        "# ğŸ® Check computational capabilities\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"ğŸš€ GPU acceleration: AVAILABLE (Lightning fast!)\")\n",
        "else:\n",
        "    print(\"ğŸ’» CPU computation: READY (Perfect for learning)\")\n",
        "\n",
        "print(\"\\nğŸ‰ Ready to explore the mathematics of intelligence!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concepts"
      },
      "source": [
        "## ğŸ§  CORE CONCEPTS: The Mathematics of Neural Networks\n",
        "\n",
        "### ğŸ­ TWO TYPES OF OPERATIONS:\n",
        "\n",
        "#### 1ï¸âƒ£ **Element-wise Operations** (Broadcasting Magic)\n",
        "- **What:** Operations between corresponding elements\n",
        "- **Example:** `[1,2] + [3,4] = [4,6]`\n",
        "- **Neural Networks:** Activation functions, normalization, element-wise gates\n",
        "\n",
        "#### 2ï¸âƒ£ **Matrix Operations** (Linear Transformations)\n",
        "- **What:** Mathematical combinations following matrix rules\n",
        "- **Example:** Matrix multiplication for layer transformations\n",
        "- **Neural Networks:** Weight Ã— input computations\n",
        "\n",
        "### ğŸ”„ BROADCASTING: TensorFlow's Superpower\n",
        "**Broadcasting** lets you operate on tensors of different shapes efficiently:\n",
        "- Add a bias vector to a batch of data\n",
        "- Scale entire tensors with single values\n",
        "- Normalize across different dimensions\n",
        "\n",
        "### ğŸ¯ WHY THIS MATTERS:\n",
        "Every forward pass in a neural network is a **chain of mathematical operations**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_title"
      },
      "source": [
        "## ğŸ”¥ STEP 1: Element-wise Operations\n",
        "### ğŸ§® The Building Blocks of Neural Computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "step1_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf466f1-7ecb-4803-b6e3-ef693b8d52cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ² Creating Sample Tensors for Mathematical Adventures\n",
            "=======================================================\n",
            "ğŸ…°ï¸ Tensor A (imagine: activations from layer 1):\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
            "   Shape: (2, 3) (2 samples, 3 features each)\n",
            "\n",
            "ğŸ…±ï¸ Tensor B (imagine: activations from layer 2):\n",
            "tf.Tensor(\n",
            "[[2. 1. 4.]\n",
            " [3. 6. 2.]], shape=(2, 3), dtype=float32)\n",
            "   Shape: (2, 3) (2 samples, 3 features each)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ² Let's create sample tensors to work with\n",
        "print(\"ğŸ² Creating Sample Tensors for Mathematical Adventures\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Think of these as activations from two different neurons,\n",
        "tensor_A = tf.constant([[1.0, 2.0, 3.0],\n",
        "                        [4.0, 5.0, 6.0]])\n",
        "\n",
        "tensor_B = tf.constant([[2.0, 1.0, 4.0],\n",
        "                        [3.0, 6.0, 2.0]])\n",
        "\n",
        "print(\"ğŸ…°ï¸ Tensor A (imagine: activations from layer 1):\")\n",
        "print(tensor_A)\n",
        "print(f\"   Shape: {tensor_A.shape} (2 samples, 3 features each)\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ…±ï¸ Tensor B (imagine: activations from layer 2):\")\n",
        "print(tensor_B)\n",
        "print(f\"   Shape: {tensor_B.shape} (2 samples, 3 features each)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "step1_addition",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00aa6cac-334d-459c-ba93-74393ae5d361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â• ELEMENT-WISE ADDITION\n",
            "=========================\n",
            "Formula: A + B (element by element)\n",
            "Result:\n",
            "[[ 3.  3.  7.]\n",
            " [ 7. 11.  8.]]\n",
            "\n",
            "ğŸ§  Neural Network Use Case:\n",
            "   Combining activations from different pathways\n",
            "   Adding residual connections (like in ResNet)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# â• ADDITION: Element-wise addition\n",
        "print(\"â• ELEMENT-WISE ADDITION\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "addition_result = tf.add(tensor_A, tensor_B)  # or simply: tensor_A + tensor_B\n",
        "\n",
        "print(\"Formula: A + B (element by element)\")\n",
        "print(f\"Result:\\n{addition_result}\")\n",
        "print()\n",
        "print(\"ğŸ§  Neural Network Use Case:\")\n",
        "print(\"   Combining activations from different pathways\")\n",
        "print(\"   Adding residual connections (like in ResNet)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "step1_multiplication",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ecd62a-585d-475b-9831-120a59aa43c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ–ï¸ ELEMENT-WISE MULTIPLICATION\n",
            "================================\n",
            "Formula: A âŠ™ B (Hadamard product)\n",
            "Result:\n",
            "[[ 2.  2. 12.]\n",
            " [12. 30. 12.]]\n",
            "\n",
            "ğŸ§  Neural Network Use Case:\n",
            "   Attention mechanisms (scaling features)\n",
            "   Gating mechanisms (LSTM, GRU gates)\n",
            "   Dropout masks during training\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# âœ–ï¸ ELEMENT-WISE MULTIPLICATION\n",
        "print(\"âœ–ï¸ ELEMENT-WISE MULTIPLICATION\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "element_mult = tf.multiply(tensor_A, tensor_B)  # or: tensor_A * tensor_B\n",
        "\n",
        "print(\"Formula: A âŠ™ B (Hadamard product)\")\n",
        "print(f\"Result:\\n{element_mult}\")\n",
        "print()\n",
        "print(\"ğŸ§  Neural Network Use Case:\")\n",
        "print(\"   Attention mechanisms (scaling features)\")\n",
        "print(\"   Gating mechanisms (LSTM, GRU gates)\")\n",
        "print(\"   Dropout masks during training\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "step1_more_ops",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d0f3fc-4b10-40bb-dd04-bec6d4f3e3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ MORE ELEMENT-WISE OPERATIONS\n",
            "================================\n",
            "ğŸ”¢ Square root of A:\n",
            "[[1.        1.4142135 1.7320508]\n",
            " [2.        2.236068  2.4494898]]\n",
            "   Use case: Standard deviation calculations\n",
            "\n",
            "ğŸ“ˆ Exponential of A:\n",
            "[[  2.7182817   7.389056   20.085537 ]\n",
            " [ 54.59815   148.41316   403.4288   ]]\n",
            "   Use case: Softmax activation function\n",
            "\n",
            "âš¡ A squared (AÂ²):\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "   Use case: Mean squared error calculations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¯ MORE USEFUL ELEMENT-WISE OPERATIONS\n",
        "print(\"ğŸ¯ MORE ELEMENT-WISE OPERATIONS\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "# Square root (useful for normalization)\n",
        "sqrt_result = tf.sqrt(tensor_A)\n",
        "print(f\"ğŸ”¢ Square root of A:\\n{sqrt_result}\")\n",
        "print(\"   Use case: Standard deviation calculations\")\n",
        "print()\n",
        "\n",
        "# Exponential (used in softmax)\n",
        "exp_result = tf.exp(tensor_A)\n",
        "print(f\"ğŸ“ˆ Exponential of A:\\n{exp_result}\")\n",
        "print(\"   Use case: Softmax activation function\")\n",
        "print()\n",
        "\n",
        "# Power operations\n",
        "power_result = tf.pow(tensor_A, 2)\n",
        "print(f\"âš¡ A squared (AÂ²):\\n{power_result}\")\n",
        "print(\"   Use case: Mean squared error calculations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_title"
      },
      "source": [
        "## ğŸ¯ STEP 2: Matrix Multiplication - The Heart of Neural Networks\n",
        "### ğŸ’ª Where the real magic happens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "step2_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c86d1c1-92bf-4917-b7ec-0810fe0dcaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—ï¸ MATRIX MULTIPLICATION SETUP\n",
            "===================================\n",
            "ğŸ“Š Input data shape: (3, 4)\n",
            "   (3 samples, 4 features each - like 3 images with 4 pixels)\n",
            "\n",
            "âš–ï¸ Weights shape: (4, 3)\n",
            "   (4 inputs, 3 outputs - transforming 4 features to 3)\n",
            "\n",
            "ğŸ” Input Data:\n",
            "tf.Tensor(\n",
            "[[1. 2. 3. 4.]\n",
            " [2. 3. 4. 5.]\n",
            " [3. 4. 5. 6.]], shape=(3, 4), dtype=float32)\n",
            "\n",
            "ğŸ” Weight Matrix:\n",
            "tf.Tensor(\n",
            "[[0.1 0.2 0.3]\n",
            " [0.4 0.5 0.6]\n",
            " [0.7 0.8 0.9]\n",
            " [0.2 0.3 0.4]], shape=(4, 3), dtype=float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ—ï¸ Setting up matrices for neural network simulation\n",
        "print(\"ğŸ—ï¸ MATRIX MULTIPLICATION SETUP\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Simulate input data (batch of 3 samples, each with 4 features)\n",
        "input_data = tf.constant([[1.0, 2.0, 3.0, 4.0],   # Sample 1\n",
        "                          [2.0, 3.0, 4.0, 5.0],   # Sample 2\n",
        "                          [3.0, 4.0, 5.0, 6.0]])  # Sample 3\n",
        "\n",
        "# Simulate weight matrix (4 inputs â†’ 3 outputs)\n",
        "weights = tf.constant([[0.1, 0.2, 0.3],  # Weights for input 1 â†’ all outputs\n",
        "                       [0.4, 0.5, 0.6],  # Weights for input 2 â†’ all outputs\n",
        "                       [0.7, 0.8, 0.9],  # Weights for input 3 â†’ all outputs\n",
        "                       [0.2, 0.3, 0.4]]) # Weights for input 4 â†’ all outputs\n",
        "\n",
        "print(f\"ğŸ“Š Input data shape: {input_data.shape}\")\n",
        "print(f\"   (3 samples, 4 features each - like 3 images with 4 pixels)\")\n",
        "print()\n",
        "print(f\"âš–ï¸ Weights shape: {weights.shape}\")\n",
        "print(f\"   (4 inputs, 3 outputs - transforming 4 features to 3)\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ” Input Data:\")\n",
        "print(input_data)\n",
        "print()\n",
        "print(\"ğŸ” Weight Matrix:\")\n",
        "print(weights)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "step2_matmul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9508d67a-8ac1-424c-b400-aa675cbed5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ MATRIX MULTIPLICATION - THE NEURAL NETWORK CORE\n",
            "====================================================\n",
            "ğŸ”„ Operation: input_data @ weights\n",
            "ğŸ“ Shape transformation: (3, 4) Ã— (4, 3) = (3, 3)\n",
            "\n",
            "âœ¨ Result (Linear transformation):\n",
            "tf.Tensor(\n",
            "[[ 3.8000002  4.8        5.8      ]\n",
            " [ 5.2        6.6000004  8.       ]\n",
            " [ 6.6        8.4       10.200001 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "ğŸ§  What just happened?\n",
            "   â€¢ Each input sample got transformed from 4 features to 3 features\n",
            "   â€¢ This is EXACTLY what happens in a neural network layer\n",
            "   â€¢ The weights learned how to combine input features meaningfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¯ THE MAGIC: Matrix Multiplication\n",
        "print(\"ğŸ¯ MATRIX MULTIPLICATION - THE NEURAL NETWORK CORE\")\n",
        "print(\"=\" * 52)\n",
        "\n",
        "# This is what happens in every neural network layer!\n",
        "output = tf.matmul(input_data, weights)\n",
        "\n",
        "print(\"ğŸ”„ Operation: input_data @ weights\")\n",
        "print(f\"ğŸ“ Shape transformation: {input_data.shape} Ã— {weights.shape} = {output.shape}\")\n",
        "print()\n",
        "print(\"âœ¨ Result (Linear transformation):\")\n",
        "print(output)\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"ğŸ§  What just happened?\")\n",
        "print(\"   â€¢ Each input sample got transformed from 4 features to 3 features\")\n",
        "print(\"   â€¢ This is EXACTLY what happens in a neural network layer\")\n",
        "print(\"   â€¢ The weights learned how to combine input features meaningfully\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "step2_shapes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa74b485-6f1f-499c-9f5e-0acf39b039f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ SHAPE COMPATIBILITY CHECK\n",
            "==============================\n",
            "(3, 4) Ã— (4, 3) = (3, 3) âœ… Neural layer transformation\n",
            "(32, 784) Ã— (784, 128) = (32, 128) âœ… MNIST â†’ Hidden layer\n",
            "(5, 10) Ã— (10, 1) = (5, 1) âœ… Multi-class â†’ Binary output\n",
            "(2, 3) Ã— (4, 2) = IMPOSSIBLE! âŒ Incompatible shapes\n",
            "\n",
            "ğŸ’¡ Rule: For A Ã— B, the last dimension of A must equal first dimension of B\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“ UNDERSTANDING MATRIX MULTIPLICATION SHAPES\n",
        "print(\"ğŸ“ SHAPE COMPATIBILITY CHECK\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "def check_matmul_compatibility(A_shape, B_shape):\n",
        "    \"\"\"Helper function to check if matrices can be multiplied\"\"\"\n",
        "    can_multiply = A_shape[-1] == B_shape[0]\n",
        "    if can_multiply:\n",
        "        result_shape = (*A_shape[:-1], B_shape[1])\n",
        "        return True, result_shape\n",
        "    return False, None\n",
        "\n",
        "# Test different shape combinations\n",
        "test_cases = [\n",
        "    ((3, 4), (4, 3), \"âœ… Neural layer transformation\"),\n",
        "    ((32, 784), (784, 128), \"âœ… MNIST â†’ Hidden layer\"),\n",
        "    ((5, 10), (10, 1), \"âœ… Multi-class â†’ Binary output\"),\n",
        "    ((2, 3), (4, 2), \"âŒ Incompatible shapes\"),\n",
        "]\n",
        "\n",
        "for A_shape, B_shape, description in test_cases:\n",
        "    compatible, result_shape = check_matmul_compatibility(A_shape, B_shape)\n",
        "\n",
        "    if compatible:\n",
        "        print(f\"{A_shape} Ã— {B_shape} = {result_shape} {description}\")\n",
        "    else:\n",
        "        print(f\"{A_shape} Ã— {B_shape} = IMPOSSIBLE! {description}\")\n",
        "\n",
        "print()\n",
        "print(\"ğŸ’¡ Rule: For A Ã— B, the last dimension of A must equal first dimension of B\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_title"
      },
      "source": [
        "## ğŸ”„ STEP 3: Broadcasting - TensorFlow's Superpower\n",
        "### ğŸª Making tensors of different shapes work together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "step3_bias",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8b1857-d385-4560-8663-c5ae8b7d76e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ­ BROADCASTING MAGIC: Adding Bias to Neural Network Output\n",
            "==========================================================\n",
            "ğŸ”¢ Network output shape: (3, 3)\n",
            "Network output:\n",
            "[[ 3.8000002  4.8        5.8      ]\n",
            " [ 5.2        6.6000004  8.       ]\n",
            " [ 6.6        8.4       10.200001 ]]\n",
            "\n",
            "âš–ï¸ Bias shape: (3,)\n",
            "Bias: [0.1 0.2 0.3]\n",
            "\n",
            "âœ¨ After adding bias (broadcasting (3, 3) + (3,)):\n",
            "tf.Tensor(\n",
            "[[ 3.9        5.         6.1000004]\n",
            " [ 5.2999997  6.8        8.3      ]\n",
            " [ 6.7        8.599999  10.500001 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "ğŸª„ What happened? The bias vector was automatically\n",
            "   'broadcasted' (repeated) for each sample in the batch!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ­ BROADCASTING EXAMPLE 1: Adding Bias\n",
        "print(\"ğŸ­ BROADCASTING MAGIC: Adding Bias to Neural Network Output\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "# Our previous neural network output\n",
        "network_output = tf.matmul(input_data, weights)\n",
        "print(f\"ğŸ”¢ Network output shape: {network_output.shape}\")\n",
        "print(f\"Network output:\\n{network_output}\")\n",
        "print()\n",
        "\n",
        "# Bias vector (one bias per output neuron)\n",
        "bias = tf.constant([0.1, 0.2, 0.3])\n",
        "print(f\"âš–ï¸ Bias shape: {bias.shape}\")\n",
        "print(f\"Bias: {bias}\")\n",
        "print()\n",
        "\n",
        "# Broadcasting magic! bias gets added to each sample\n",
        "output_with_bias = network_output + bias\n",
        "print(f\"âœ¨ After adding bias (broadcasting {network_output.shape} + {bias.shape}):\")\n",
        "print(output_with_bias)\n",
        "print()\n",
        "print(\"ğŸª„ What happened? The bias vector was automatically\")\n",
        "print(\"   'broadcasted' (repeated) for each sample in the batch!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "step3_scaling",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9511d0-73f9-4ef5-f8ab-0bfc5f42edcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š BROADCASTING: Scaling Entire Tensors\n",
            "========================================\n",
            "ğŸšï¸ Original weights shape: (4, 3)\n",
            "ğŸ“‰ Learning rate (scalar): 0.01\n",
            "âš¡ Scaled weights (for gradient descent):\n",
            "tf.Tensor(\n",
            "[[0.001 0.002 0.003]\n",
            " [0.004 0.005 0.006]\n",
            " [0.007 0.008 0.009]\n",
            " [0.002 0.003 0.004]], shape=(4, 3), dtype=float32)\n",
            "\n",
            "ğŸ’¡ Use case: Gradient descent weight updates!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š BROADCASTING EXAMPLE 2: Scaling Operations\n",
        "print(\"ğŸ“Š BROADCASTING: Scaling Entire Tensors\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Scaling with a single number (scalar broadcasting)\n",
        "learning_rate = 0.01\n",
        "scaled_weights = weights * learning_rate\n",
        "\n",
        "print(f\"ğŸšï¸ Original weights shape: {weights.shape}\")\n",
        "print(f\"ğŸ“‰ Learning rate (scalar): {learning_rate}\")\n",
        "print(f\"âš¡ Scaled weights (for gradient descent):\")\n",
        "print(scaled_weights)\n",
        "print()\n",
        "print(\"ğŸ’¡ Use case: Gradient descent weight updates!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "step3_normalization",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a783a3bb-67ba-4469-999b-adc2c93c1218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ BROADCASTING: Data Normalization\n",
            "===================================\n",
            "ğŸ“Š Original data shape: (3, 4)\n",
            "ğŸ“ˆ Feature means: [2. 3. 4. 5.] (shape: (4,))\n",
            "ğŸ“ Feature stds: [0.8164966 0.8164966 0.8164966 0.8164966] (shape: (4,))\n",
            "\n",
            "âœ¨ Normalized data (zero mean, unit variance per feature):\n",
            "tf.Tensor(\n",
            "[[-1.2247448 -1.2247448 -1.2247448 -1.2247448]\n",
            " [ 0.         0.         0.         0.       ]\n",
            " [ 1.2247448  1.2247448  1.2247448  1.2247448]], shape=(3, 4), dtype=float32)\n",
            "\n",
            "ğŸ§  Neural Network Benefit: Helps with training stability!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¯ BROADCASTING EXAMPLE 3: Normalization\n",
        "print(\"ğŸ¯ BROADCASTING: Data Normalization\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Normalize each feature across the batch\n",
        "feature_means = tf.reduce_mean(input_data, axis=0)  # Mean of each feature\n",
        "feature_stds = tf.math.reduce_std(input_data, axis=0)  # Std of each feature\n",
        "\n",
        "print(f\"ğŸ“Š Original data shape: {input_data.shape}\")\n",
        "print(f\"ğŸ“ˆ Feature means: {feature_means} (shape: {feature_means.shape})\")\n",
        "print(f\"ğŸ“ Feature stds: {feature_stds} (shape: {feature_stds.shape})\")\n",
        "print()\n",
        "\n",
        "# Normalize using broadcasting\n",
        "normalized_data = (input_data - feature_means) / feature_stds\n",
        "\n",
        "print(\"âœ¨ Normalized data (zero mean, unit variance per feature):\")\n",
        "print(normalized_data)\n",
        "print()\n",
        "print(\"ğŸ§  Neural Network Benefit: Helps with training stability!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_title"
      },
      "source": [
        "## âš¡ STEP 4: Advanced Operations - The Neural Network Toolkit\n",
        "### ğŸ› ï¸ Operations you'll use in every neural network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "step4_transpose",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f41f79d-4464-4aad-a62b-aa8c36970917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ TRANSPOSE: Flipping Matrix Dimensions\n",
            "==========================================\n",
            "ğŸ“‹ Original (2, 3):\n",
            "tf.Tensor(\n",
            "[[1 2 3]\n",
            " [4 5 6]], shape=(2, 3), dtype=int32)\n",
            "\n",
            "ğŸ”„ Transposed (3, 2):\n",
            "tf.Tensor(\n",
            "[[1 4]\n",
            " [2 5]\n",
            " [3 6]], shape=(3, 2), dtype=int32)\n",
            "\n",
            "ğŸ§  Neural Network Use Cases:\n",
            "   â€¢ Backpropagation (computing gradients)\n",
            "   â€¢ Weight matrix operations\n",
            "   â€¢ Attention mechanisms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”„ TRANSPOSE OPERATIONS\n",
        "print(\"ğŸ”„ TRANSPOSE: Flipping Matrix Dimensions\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "original_matrix = tf.constant([[1, 2, 3],\n",
        "                               [4, 5, 6]])\n",
        "transposed = tf.transpose(original_matrix)\n",
        "\n",
        "print(f\"ğŸ“‹ Original {original_matrix.shape}:\")\n",
        "print(original_matrix)\n",
        "print()\n",
        "print(f\"ğŸ”„ Transposed {transposed.shape}:\")\n",
        "print(transposed)\n",
        "print()\n",
        "print(\"ğŸ§  Neural Network Use Cases:\")\n",
        "print(\"   â€¢ Backpropagation (computing gradients)\")\n",
        "print(\"   â€¢ Weight matrix operations\")\n",
        "print(\"   â€¢ Attention mechanisms\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "step4_reductions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa15b7d-062d-4a32-83bf-0fc33787e5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š REDUCTION OPERATIONS: Summarizing Data\n",
            "==========================================\n",
            "ğŸ“ˆ Batch data (3, 3):\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "â• Total sum: 45.0\n",
            "ğŸ“Š Mean: 5.0\n",
            "â¬†ï¸ Maximum: 9.0\n",
            "â¬‡ï¸ Minimum: 1.0\n",
            "\n",
            "ğŸ”½ Row sums (axis=1): [ 6. 15. 24.]\n",
            "â¡ï¸ Column means (axis=0): [4. 5. 6.]\n",
            "\n",
            "ğŸ§  Neural Network Applications:\n",
            "   â€¢ Loss function calculations\n",
            "   â€¢ Batch statistics for normalization\n",
            "   â€¢ Attention weight computation\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š REDUCTION OPERATIONS\n",
        "print(\"ğŸ“Š REDUCTION OPERATIONS: Summarizing Data\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "# Sample batch of data (like loss values or predictions)\n",
        "batch_data = tf.constant([[1.0, 2.0, 3.0],\n",
        "                          [4.0, 5.0, 6.0],\n",
        "                          [7.0, 8.0, 9.0]])\n",
        "\n",
        "print(f\"ğŸ“ˆ Batch data {batch_data.shape}:\")\n",
        "print(batch_data)\n",
        "print()\n",
        "\n",
        "# Different reduction operations\n",
        "total_sum = tf.reduce_sum(batch_data)\n",
        "batch_mean = tf.reduce_mean(batch_data)\n",
        "max_value = tf.reduce_max(batch_data)\n",
        "min_value = tf.reduce_min(batch_data)\n",
        "\n",
        "print(f\"â• Total sum: {total_sum}\")\n",
        "print(f\"ğŸ“Š Mean: {batch_mean}\")\n",
        "print(f\"â¬†ï¸ Maximum: {max_value}\")\n",
        "print(f\"â¬‡ï¸ Minimum: {min_value}\")\n",
        "print()\n",
        "\n",
        "# Axis-specific reductions\n",
        "row_sums = tf.reduce_sum(batch_data, axis=1)  # Sum across columns\n",
        "col_means = tf.reduce_mean(batch_data, axis=0)  # Mean across rows\n",
        "\n",
        "print(f\"ğŸ”½ Row sums (axis=1): {row_sums}\")\n",
        "print(f\"â¡ï¸ Column means (axis=0): {col_means}\")\n",
        "print()\n",
        "print(\"ğŸ§  Neural Network Applications:\")\n",
        "print(\"   â€¢ Loss function calculations\")\n",
        "print(\"   â€¢ Batch statistics for normalization\")\n",
        "print(\"   â€¢ Attention weight computation\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_title"
      },
      "source": [
        "## ğŸ® STEP 5: Real Neural Network Simulation\n",
        "### ğŸ—ï¸ Building a complete forward pass with all operations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "step5_network",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5cc613-c1cf-42e1-8586-688a2838ec42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—ï¸ BUILDING A COMPLETE NEURAL NETWORK LAYER\n",
            "===============================================\n",
            "ğŸ¯ Network Architecture: 4 â†’ 3 â†’ 2\n",
            "\n",
            "ğŸ“Š Input shape: (3, 4)\n",
            "âš–ï¸ W1 shape: (4, 3), b1 shape: (3,)\n",
            "âš–ï¸ W2 shape: (3, 2), b2 shape: (2,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ—ï¸ COMPLETE NEURAL NETWORK LAYER SIMULATION\n",
        "print(\"ğŸ—ï¸ BUILDING A COMPLETE NEURAL NETWORK LAYER\")\n",
        "print(\"=\" * 47)\n",
        "\n",
        "# Network architecture: 4 inputs â†’ 3 hidden â†’ 2 outputs\n",
        "print(\"ğŸ¯ Network Architecture: 4 â†’ 3 â†’ 2\")\n",
        "print()\n",
        "\n",
        "# Input batch (3 samples, 4 features each)\n",
        "inputs = tf.constant([[1.0, 2.0, 3.0, 4.0],\n",
        "                      [2.0, 3.0, 4.0, 5.0],\n",
        "                      [0.5, 1.5, 2.5, 3.5]])\n",
        "\n",
        "# Layer 1: Input â†’ Hidden (4 â†’ 3)\n",
        "W1 = tf.random.normal([4, 3], stddev=0.1)\n",
        "b1 = tf.constant([0.1, 0.2, 0.3])\n",
        "\n",
        "# Layer 2: Hidden â†’ Output (3 â†’ 2)\n",
        "W2 = tf.random.normal([3, 2], stddev=0.1)\n",
        "b2 = tf.constant([0.05, 0.15])\n",
        "\n",
        "print(f\"ğŸ“Š Input shape: {inputs.shape}\")\n",
        "print(f\"âš–ï¸ W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
        "print(f\"âš–ï¸ W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "step5_forward",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb8086a-676a-431c-a3d8-19dc31fd7f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ FORWARD PASS EXECUTION\n",
            "===========================\n",
            "ğŸ“ Step 1: Linear transformation (Layer 1)\n",
            "   Linear output shape: (3, 3)\n",
            "   Sample values: [ 0.81007755  0.3584847  -0.14984661]\n",
            "\n",
            "ğŸ“ Step 2: Apply activation function (ReLU)\n",
            "   Activated shape: (3, 3)\n",
            "   Sample values: [0.81007755 0.3584847  0.        ]\n",
            "\n",
            "ğŸ“ Step 3: Second linear transformation (Layer 2)\n",
            "   Output linear shape: (3, 2)\n",
            "   Sample values: [0.21121259 0.13047361]\n",
            "\n",
            "ğŸ“ Step 4: Final activation (Sigmoid for binary classification)\n",
            "   Final output shape: (3, 2)\n",
            "   Predictions for all samples:\n",
            "tf.Tensor(\n",
            "[[0.5526077  0.5325722 ]\n",
            " [0.56321675 0.53125054]\n",
            " [0.54728454 0.53323287]], shape=(3, 2), dtype=float32)\n",
            "\n",
            "ğŸ‰ COMPLETE! We just simulated a 2-layer neural network!\n",
            "   Used: Matrix multiplication, broadcasting, element-wise operations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ FORWARD PASS: Step by step\n",
        "print(\"ğŸš€ FORWARD PASS EXECUTION\")\n",
        "print(\"=\" * 27)\n",
        "\n",
        "print(\"ğŸ“ Step 1: Linear transformation (Layer 1)\")\n",
        "hidden_linear = tf.matmul(inputs, W1) + b1  # Matrix mult + Broadcasting\n",
        "print(f\"   Linear output shape: {hidden_linear.shape}\")\n",
        "print(f\"   Sample values: {hidden_linear[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“ Step 2: Apply activation function (ReLU)\")\n",
        "hidden_activated = tf.nn.relu(hidden_linear)  # Element-wise operation\n",
        "print(f\"   Activated shape: {hidden_activated.shape}\")\n",
        "print(f\"   Sample values: {hidden_activated[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“ Step 3: Second linear transformation (Layer 2)\")\n",
        "output_linear = tf.matmul(hidden_activated, W2) + b2\n",
        "print(f\"   Output linear shape: {output_linear.shape}\")\n",
        "print(f\"   Sample values: {output_linear[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“ Step 4: Final activation (Sigmoid for binary classification)\")\n",
        "final_output = tf.nn.sigmoid(output_linear)\n",
        "print(f\"   Final output shape: {final_output.shape}\")\n",
        "print(f\"   Predictions for all samples:\")\n",
        "print(final_output)\n",
        "print()\n",
        "\n",
        "print(\"ğŸ‰ COMPLETE! We just simulated a 2-layer neural network!\")\n",
        "print(\"   Used: Matrix multiplication, broadcasting, element-wise operations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validation_title"
      },
      "source": [
        "## âœ… VALIDATION & DEBUGGING\n",
        "### ğŸ” Let's test your mathematical mastery!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "validation_shapes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b518fa-2902-4dc4-dd01-f1d43f2d1a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§© MATHEMATICAL DEBUGGING CHALLENGE\n",
            "======================================\n",
            "ğŸ” Checking common neural network shape issues...\n",
            "\n",
            "âœ… Test 1 PASSED: (32, 784) Ã— (784, 128) = (32, 128)\n",
            "âœ… Test 2 PASSED: Broadcasting (10, 5) + (5,) = (10, 5)\n",
            "\n",
            "ğŸ¯ Key Debugging Skills:\n",
            "   â€¢ Always check tensor shapes before operations\n",
            "   â€¢ Remember matrix multiplication rules\n",
            "   â€¢ Understand broadcasting patterns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ§© SHAPE DEBUGGING CHALLENGE\n",
        "print(\"ğŸ§© MATHEMATICAL DEBUGGING CHALLENGE\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "# Create some \"problematic\" scenarios\n",
        "print(\"ğŸ” Checking common neural network shape issues...\")\n",
        "print()\n",
        "\n",
        "# Test case 1: Batch size compatibility\n",
        "batch1 = tf.random.normal([32, 784])  # 32 samples, 784 features (like MNIST)\n",
        "weights1 = tf.random.normal([784, 128])  # 784 â†’ 128 transformation\n",
        "\n",
        "try:\n",
        "    result1 = tf.matmul(batch1, weights1)\n",
        "    print(f\"âœ… Test 1 PASSED: {batch1.shape} Ã— {weights1.shape} = {result1.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Test 1 FAILED: {e}\")\n",
        "\n",
        "# Test case 2: Broadcasting bias addition\n",
        "output = tf.random.normal([10, 5])  # 10 samples, 5 outputs\n",
        "bias = tf.random.normal([5])  # 5 bias values\n",
        "\n",
        "try:\n",
        "    result2 = output + bias\n",
        "    print(f\"âœ… Test 2 PASSED: Broadcasting {output.shape} + {bias.shape} = {result2.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Test 2 FAILED: {e}\")\n",
        "\n",
        "print()\n",
        "print(\"ğŸ¯ Key Debugging Skills:\")\n",
        "print(\"   â€¢ Always check tensor shapes before operations\")\n",
        "print(\"   â€¢ Remember matrix multiplication rules\")\n",
        "print(\"   â€¢ Understand broadcasting patterns\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "validation_practical",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b08e2b-e0df-4000-ed06-2ca7c928d20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸª PRACTICAL SCENARIO: Image Classification Pipeline\n",
            "======================================================\n",
            "ğŸ“¸ Image batch: (5, 784) (5 images, 784 pixels each)\n",
            "ğŸ¯ Classifier weights: (784, 10)\n",
            "âš–ï¸ Classifier bias: (10,)\n",
            "\n",
            "ğŸ“Š Raw scores (logits): (5, 10)\n",
            "ğŸ² Probability predictions: (5, 10)\n",
            "\n",
            "ğŸ” Sample prediction (probabilities for 10 classes):\n",
            "tf.Tensor(\n",
            "[0.10473149 0.09422571 0.09968884 0.11491498 0.11673317 0.09374061\n",
            " 0.09624387 0.09522426 0.08923497 0.09526205], shape=(10,), dtype=float32)\n",
            "ğŸ“ˆ Probabilities sum to: 1.000\n",
            "\n",
            "ğŸ† Predicted classes for all 5 images: [4 4 4 1 4]\n",
            "\n",
            "ğŸ‰ SUCCESS! You've implemented a complete classification pipeline!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸª PRACTICAL SCENARIO: Image Classification\n",
        "print(\"ğŸª PRACTICAL SCENARIO: Image Classification Pipeline\")\n",
        "print(\"=\" * 54)\n",
        "\n",
        "# Simulate a batch of flattened images (like MNIST)\n",
        "batch_size = 5\n",
        "image_pixels = 28 * 28  # 784 pixels per image\n",
        "num_classes = 10\n",
        "\n",
        "# Fake image data\n",
        "images = tf.random.uniform([batch_size, image_pixels], 0, 1)\n",
        "print(f\"ğŸ“¸ Image batch: {images.shape} (5 images, 784 pixels each)\")\n",
        "\n",
        "# Classification weights and bias\n",
        "classifier_weights = tf.random.normal([image_pixels, num_classes], stddev=0.01)\n",
        "classifier_bias = tf.zeros([num_classes])\n",
        "\n",
        "print(f\"ğŸ¯ Classifier weights: {classifier_weights.shape}\")\n",
        "print(f\"âš–ï¸ Classifier bias: {classifier_bias.shape}\")\n",
        "print()\n",
        "\n",
        "# Forward pass\n",
        "logits = tf.matmul(images, classifier_weights) + classifier_bias\n",
        "predictions = tf.nn.softmax(logits)  # Convert to probabilities\n",
        "\n",
        "print(f\"ğŸ“Š Raw scores (logits): {logits.shape}\")\n",
        "print(f\"ğŸ² Probability predictions: {predictions.shape}\")\n",
        "print()\n",
        "print(\"ğŸ” Sample prediction (probabilities for 10 classes):\")\n",
        "print(predictions[0])  # First image's predictions\n",
        "print(f\"ğŸ“ˆ Probabilities sum to: {tf.reduce_sum(predictions[0]):.3f}\")\n",
        "print()\n",
        "\n",
        "# Find predicted class\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "print(f\"ğŸ† Predicted classes for all 5 images: {predicted_classes}\")\n",
        "print()\n",
        "print(\"ğŸ‰ SUCCESS! You've implemented a complete classification pipeline!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takeaways"
      },
      "source": [
        "## ğŸ” KEY TAKEAWAYS\n",
        "\n",
        "### ğŸ§® **Mathematical Operations Mastery:**\n",
        "1. **Element-wise operations** work on corresponding elements (broadcasting magic)\n",
        "2. **Matrix multiplication** transforms data between layers (the core of neural networks)\n",
        "3. **Broadcasting** lets different shaped tensors work together efficiently\n",
        "4. **Reduction operations** summarize data (losses, statistics, attention)\n",
        "\n",
        "### ğŸ§  **Neural Network Applications:**\n",
        "- **Forward pass** = Chain of matrix multiplications + activations\n",
        "- **Bias addition** uses broadcasting for efficiency\n",
        "- **Normalization** uses element-wise operations and broadcasting\n",
        "- **Shape compatibility** is crucial for debugging\n",
        "\n",
        "### ğŸ’¡ **Pro Tips:**\n",
        "- Always check tensor shapes before operations\n",
        "- Use broadcasting to avoid explicit loops\n",
        "- Matrix multiplication: `(m,n) Ã— (n,k) = (m,k)`\n",
        "- Element-wise operations preserve shape\n",
        "\n",
        "### ğŸ¤” **Questions to Ponder:**\n",
        "- How would you implement batch normalization using these operations?\n",
        "\n",
        "\n",
        "- What happens to shapes during backpropagation?\n",
        "\n",
        "\n",
        "- How do attention mechanisms use these mathematical operations?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}