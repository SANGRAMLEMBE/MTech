{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🧮 T3-Exercise-2: Mathematical Operations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 LEARNING OBJECTIVES\n",
        "By the end of this exercise, you will:\n",
        "- ⚡ Master element-wise operations (the building blocks of neural computations)\n",
        "- 🎯 Understand matrix multiplication (the heart of neural networks)\n",
        "- 🔄 Learn broadcasting (making tensors work together efficiently)\n",
        "- 🧠 Apply mathematical operations in real neural network scenarios\n",
        "- 🔍 Debug shape mismatches and mathematical errors\n",
        "\n",
        "## 🔗 CONNECTION TO NEURAL NETWORKS\n",
        "Mathematics is the **engine** that powers neural networks:\n",
        "- **Element-wise operations** → Activation functions, normalization\n",
        "- **Matrix multiplication** → Layer transformations (input × weights)\n",
        "- **Broadcasting** → Efficient batch processing\n",
        "- **Reduction operations** → Loss calculation, metrics\n",
        "\n",
        "**Real Example:** When an image passes through a neural layer:  \n",
        "`output = activation(input @ weights + bias)` ← All math operations!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## ⚙️ SETUP & ENVIRONMENT CHECK\n",
        "🚀 Let's power up our mathematical toolkit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "setup_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04da68e-36e6-4fca-97d0-398c56b9ff4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 MATHEMATICAL TOOLKIT CHECK\n",
            "===================================\n",
            "🐍 Python: 3.12.11\n",
            "🔥 TensorFlow: 2.19.0\n",
            "🔢 NumPy: 2.0.2\n",
            "💻 CPU computation: READY (Perfect for learning)\n",
            "\n",
            "🎉 Ready to explore the mathematics of intelligence!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🛠️ Essential imports for mathematical operations\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# 🔧 Environment verification\n",
        "print(\"🔧 MATHEMATICAL TOOLKIT CHECK\")\n",
        "print(\"=\" * 35)\n",
        "print(f\"🐍 Python: {sys.version.split()[0]}\")\n",
        "print(f\"🔥 TensorFlow: {tf.__version__}\")\n",
        "print(f\"🔢 NumPy: {np.__version__}\")\n",
        "\n",
        "# 🎮 Check computational capabilities\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"🚀 GPU acceleration: AVAILABLE (Lightning fast!)\")\n",
        "else:\n",
        "    print(\"💻 CPU computation: READY (Perfect for learning)\")\n",
        "\n",
        "print(\"\\n🎉 Ready to explore the mathematics of intelligence!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concepts"
      },
      "source": [
        "## 🧠 CORE CONCEPTS: The Mathematics of Neural Networks\n",
        "\n",
        "### 🎭 TWO TYPES OF OPERATIONS:\n",
        "\n",
        "#### 1️⃣ **Element-wise Operations** (Broadcasting Magic)\n",
        "- **What:** Operations between corresponding elements\n",
        "- **Example:** `[1,2] + [3,4] = [4,6]`\n",
        "- **Neural Networks:** Activation functions, normalization, element-wise gates\n",
        "\n",
        "#### 2️⃣ **Matrix Operations** (Linear Transformations)\n",
        "- **What:** Mathematical combinations following matrix rules\n",
        "- **Example:** Matrix multiplication for layer transformations\n",
        "- **Neural Networks:** Weight × input computations\n",
        "\n",
        "### 🔄 BROADCASTING: TensorFlow's Superpower\n",
        "**Broadcasting** lets you operate on tensors of different shapes efficiently:\n",
        "- Add a bias vector to a batch of data\n",
        "- Scale entire tensors with single values\n",
        "- Normalize across different dimensions\n",
        "\n",
        "### 🎯 WHY THIS MATTERS:\n",
        "Every forward pass in a neural network is a **chain of mathematical operations**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_title"
      },
      "source": [
        "## 🔥 STEP 1: Element-wise Operations\n",
        "### 🧮 The Building Blocks of Neural Computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "step1_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf466f1-7ecb-4803-b6e3-ef693b8d52cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎲 Creating Sample Tensors for Mathematical Adventures\n",
            "=======================================================\n",
            "🅰️ Tensor A (imagine: activations from layer 1):\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
            "   Shape: (2, 3) (2 samples, 3 features each)\n",
            "\n",
            "🅱️ Tensor B (imagine: activations from layer 2):\n",
            "tf.Tensor(\n",
            "[[2. 1. 4.]\n",
            " [3. 6. 2.]], shape=(2, 3), dtype=float32)\n",
            "   Shape: (2, 3) (2 samples, 3 features each)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎲 Let's create sample tensors to work with\n",
        "print(\"🎲 Creating Sample Tensors for Mathematical Adventures\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Think of these as activations from two different neurons,\n",
        "tensor_A = tf.constant([[1.0, 2.0, 3.0],\n",
        "                        [4.0, 5.0, 6.0]])\n",
        "\n",
        "tensor_B = tf.constant([[2.0, 1.0, 4.0],\n",
        "                        [3.0, 6.0, 2.0]])\n",
        "\n",
        "print(\"🅰️ Tensor A (imagine: activations from layer 1):\")\n",
        "print(tensor_A)\n",
        "print(f\"   Shape: {tensor_A.shape} (2 samples, 3 features each)\")\n",
        "print()\n",
        "\n",
        "print(\"🅱️ Tensor B (imagine: activations from layer 2):\")\n",
        "print(tensor_B)\n",
        "print(f\"   Shape: {tensor_B.shape} (2 samples, 3 features each)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "step1_addition",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00aa6cac-334d-459c-ba93-74393ae5d361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➕ ELEMENT-WISE ADDITION\n",
            "=========================\n",
            "Formula: A + B (element by element)\n",
            "Result:\n",
            "[[ 3.  3.  7.]\n",
            " [ 7. 11.  8.]]\n",
            "\n",
            "🧠 Neural Network Use Case:\n",
            "   Combining activations from different pathways\n",
            "   Adding residual connections (like in ResNet)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ➕ ADDITION: Element-wise addition\n",
        "print(\"➕ ELEMENT-WISE ADDITION\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "addition_result = tf.add(tensor_A, tensor_B)  # or simply: tensor_A + tensor_B\n",
        "\n",
        "print(\"Formula: A + B (element by element)\")\n",
        "print(f\"Result:\\n{addition_result}\")\n",
        "print()\n",
        "print(\"🧠 Neural Network Use Case:\")\n",
        "print(\"   Combining activations from different pathways\")\n",
        "print(\"   Adding residual connections (like in ResNet)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "step1_multiplication",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ecd62a-585d-475b-9831-120a59aa43c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✖️ ELEMENT-WISE MULTIPLICATION\n",
            "================================\n",
            "Formula: A ⊙ B (Hadamard product)\n",
            "Result:\n",
            "[[ 2.  2. 12.]\n",
            " [12. 30. 12.]]\n",
            "\n",
            "🧠 Neural Network Use Case:\n",
            "   Attention mechanisms (scaling features)\n",
            "   Gating mechanisms (LSTM, GRU gates)\n",
            "   Dropout masks during training\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ✖️ ELEMENT-WISE MULTIPLICATION\n",
        "print(\"✖️ ELEMENT-WISE MULTIPLICATION\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "element_mult = tf.multiply(tensor_A, tensor_B)  # or: tensor_A * tensor_B\n",
        "\n",
        "print(\"Formula: A ⊙ B (Hadamard product)\")\n",
        "print(f\"Result:\\n{element_mult}\")\n",
        "print()\n",
        "print(\"🧠 Neural Network Use Case:\")\n",
        "print(\"   Attention mechanisms (scaling features)\")\n",
        "print(\"   Gating mechanisms (LSTM, GRU gates)\")\n",
        "print(\"   Dropout masks during training\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "step1_more_ops",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d0f3fc-4b10-40bb-dd04-bec6d4f3e3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 MORE ELEMENT-WISE OPERATIONS\n",
            "================================\n",
            "🔢 Square root of A:\n",
            "[[1.        1.4142135 1.7320508]\n",
            " [2.        2.236068  2.4494898]]\n",
            "   Use case: Standard deviation calculations\n",
            "\n",
            "📈 Exponential of A:\n",
            "[[  2.7182817   7.389056   20.085537 ]\n",
            " [ 54.59815   148.41316   403.4288   ]]\n",
            "   Use case: Softmax activation function\n",
            "\n",
            "⚡ A squared (A²):\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "   Use case: Mean squared error calculations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎯 MORE USEFUL ELEMENT-WISE OPERATIONS\n",
        "print(\"🎯 MORE ELEMENT-WISE OPERATIONS\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "# Square root (useful for normalization)\n",
        "sqrt_result = tf.sqrt(tensor_A)\n",
        "print(f\"🔢 Square root of A:\\n{sqrt_result}\")\n",
        "print(\"   Use case: Standard deviation calculations\")\n",
        "print()\n",
        "\n",
        "# Exponential (used in softmax)\n",
        "exp_result = tf.exp(tensor_A)\n",
        "print(f\"📈 Exponential of A:\\n{exp_result}\")\n",
        "print(\"   Use case: Softmax activation function\")\n",
        "print()\n",
        "\n",
        "# Power operations\n",
        "power_result = tf.pow(tensor_A, 2)\n",
        "print(f\"⚡ A squared (A²):\\n{power_result}\")\n",
        "print(\"   Use case: Mean squared error calculations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_title"
      },
      "source": [
        "## 🎯 STEP 2: Matrix Multiplication - The Heart of Neural Networks\n",
        "### 💪 Where the real magic happens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "step2_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c86d1c1-92bf-4917-b7ec-0810fe0dcaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏗️ MATRIX MULTIPLICATION SETUP\n",
            "===================================\n",
            "📊 Input data shape: (3, 4)\n",
            "   (3 samples, 4 features each - like 3 images with 4 pixels)\n",
            "\n",
            "⚖️ Weights shape: (4, 3)\n",
            "   (4 inputs, 3 outputs - transforming 4 features to 3)\n",
            "\n",
            "🔍 Input Data:\n",
            "tf.Tensor(\n",
            "[[1. 2. 3. 4.]\n",
            " [2. 3. 4. 5.]\n",
            " [3. 4. 5. 6.]], shape=(3, 4), dtype=float32)\n",
            "\n",
            "🔍 Weight Matrix:\n",
            "tf.Tensor(\n",
            "[[0.1 0.2 0.3]\n",
            " [0.4 0.5 0.6]\n",
            " [0.7 0.8 0.9]\n",
            " [0.2 0.3 0.4]], shape=(4, 3), dtype=float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🏗️ Setting up matrices for neural network simulation\n",
        "print(\"🏗️ MATRIX MULTIPLICATION SETUP\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Simulate input data (batch of 3 samples, each with 4 features)\n",
        "input_data = tf.constant([[1.0, 2.0, 3.0, 4.0],   # Sample 1\n",
        "                          [2.0, 3.0, 4.0, 5.0],   # Sample 2\n",
        "                          [3.0, 4.0, 5.0, 6.0]])  # Sample 3\n",
        "\n",
        "# Simulate weight matrix (4 inputs → 3 outputs)\n",
        "weights = tf.constant([[0.1, 0.2, 0.3],  # Weights for input 1 → all outputs\n",
        "                       [0.4, 0.5, 0.6],  # Weights for input 2 → all outputs\n",
        "                       [0.7, 0.8, 0.9],  # Weights for input 3 → all outputs\n",
        "                       [0.2, 0.3, 0.4]]) # Weights for input 4 → all outputs\n",
        "\n",
        "print(f\"📊 Input data shape: {input_data.shape}\")\n",
        "print(f\"   (3 samples, 4 features each - like 3 images with 4 pixels)\")\n",
        "print()\n",
        "print(f\"⚖️ Weights shape: {weights.shape}\")\n",
        "print(f\"   (4 inputs, 3 outputs - transforming 4 features to 3)\")\n",
        "print()\n",
        "\n",
        "print(\"🔍 Input Data:\")\n",
        "print(input_data)\n",
        "print()\n",
        "print(\"🔍 Weight Matrix:\")\n",
        "print(weights)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "step2_matmul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9508d67a-8ac1-424c-b400-aa675cbed5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 MATRIX MULTIPLICATION - THE NEURAL NETWORK CORE\n",
            "====================================================\n",
            "🔄 Operation: input_data @ weights\n",
            "📏 Shape transformation: (3, 4) × (4, 3) = (3, 3)\n",
            "\n",
            "✨ Result (Linear transformation):\n",
            "tf.Tensor(\n",
            "[[ 3.8000002  4.8        5.8      ]\n",
            " [ 5.2        6.6000004  8.       ]\n",
            " [ 6.6        8.4       10.200001 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "🧠 What just happened?\n",
            "   • Each input sample got transformed from 4 features to 3 features\n",
            "   • This is EXACTLY what happens in a neural network layer\n",
            "   • The weights learned how to combine input features meaningfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎯 THE MAGIC: Matrix Multiplication\n",
        "print(\"🎯 MATRIX MULTIPLICATION - THE NEURAL NETWORK CORE\")\n",
        "print(\"=\" * 52)\n",
        "\n",
        "# This is what happens in every neural network layer!\n",
        "output = tf.matmul(input_data, weights)\n",
        "\n",
        "print(\"🔄 Operation: input_data @ weights\")\n",
        "print(f\"📏 Shape transformation: {input_data.shape} × {weights.shape} = {output.shape}\")\n",
        "print()\n",
        "print(\"✨ Result (Linear transformation):\")\n",
        "print(output)\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"🧠 What just happened?\")\n",
        "print(\"   • Each input sample got transformed from 4 features to 3 features\")\n",
        "print(\"   • This is EXACTLY what happens in a neural network layer\")\n",
        "print(\"   • The weights learned how to combine input features meaningfully\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "step2_shapes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa74b485-6f1f-499c-9f5e-0acf39b039f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📐 SHAPE COMPATIBILITY CHECK\n",
            "==============================\n",
            "(3, 4) × (4, 3) = (3, 3) ✅ Neural layer transformation\n",
            "(32, 784) × (784, 128) = (32, 128) ✅ MNIST → Hidden layer\n",
            "(5, 10) × (10, 1) = (5, 1) ✅ Multi-class → Binary output\n",
            "(2, 3) × (4, 2) = IMPOSSIBLE! ❌ Incompatible shapes\n",
            "\n",
            "💡 Rule: For A × B, the last dimension of A must equal first dimension of B\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📐 UNDERSTANDING MATRIX MULTIPLICATION SHAPES\n",
        "print(\"📐 SHAPE COMPATIBILITY CHECK\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "def check_matmul_compatibility(A_shape, B_shape):\n",
        "    \"\"\"Helper function to check if matrices can be multiplied\"\"\"\n",
        "    can_multiply = A_shape[-1] == B_shape[0]\n",
        "    if can_multiply:\n",
        "        result_shape = (*A_shape[:-1], B_shape[1])\n",
        "        return True, result_shape\n",
        "    return False, None\n",
        "\n",
        "# Test different shape combinations\n",
        "test_cases = [\n",
        "    ((3, 4), (4, 3), \"✅ Neural layer transformation\"),\n",
        "    ((32, 784), (784, 128), \"✅ MNIST → Hidden layer\"),\n",
        "    ((5, 10), (10, 1), \"✅ Multi-class → Binary output\"),\n",
        "    ((2, 3), (4, 2), \"❌ Incompatible shapes\"),\n",
        "]\n",
        "\n",
        "for A_shape, B_shape, description in test_cases:\n",
        "    compatible, result_shape = check_matmul_compatibility(A_shape, B_shape)\n",
        "\n",
        "    if compatible:\n",
        "        print(f\"{A_shape} × {B_shape} = {result_shape} {description}\")\n",
        "    else:\n",
        "        print(f\"{A_shape} × {B_shape} = IMPOSSIBLE! {description}\")\n",
        "\n",
        "print()\n",
        "print(\"💡 Rule: For A × B, the last dimension of A must equal first dimension of B\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_title"
      },
      "source": [
        "## 🔄 STEP 3: Broadcasting - TensorFlow's Superpower\n",
        "### 🎪 Making tensors of different shapes work together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "step3_bias",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8b1857-d385-4560-8663-c5ae8b7d76e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎭 BROADCASTING MAGIC: Adding Bias to Neural Network Output\n",
            "==========================================================\n",
            "🔢 Network output shape: (3, 3)\n",
            "Network output:\n",
            "[[ 3.8000002  4.8        5.8      ]\n",
            " [ 5.2        6.6000004  8.       ]\n",
            " [ 6.6        8.4       10.200001 ]]\n",
            "\n",
            "⚖️ Bias shape: (3,)\n",
            "Bias: [0.1 0.2 0.3]\n",
            "\n",
            "✨ After adding bias (broadcasting (3, 3) + (3,)):\n",
            "tf.Tensor(\n",
            "[[ 3.9        5.         6.1000004]\n",
            " [ 5.2999997  6.8        8.3      ]\n",
            " [ 6.7        8.599999  10.500001 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "🪄 What happened? The bias vector was automatically\n",
            "   'broadcasted' (repeated) for each sample in the batch!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎭 BROADCASTING EXAMPLE 1: Adding Bias\n",
        "print(\"🎭 BROADCASTING MAGIC: Adding Bias to Neural Network Output\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "# Our previous neural network output\n",
        "network_output = tf.matmul(input_data, weights)\n",
        "print(f\"🔢 Network output shape: {network_output.shape}\")\n",
        "print(f\"Network output:\\n{network_output}\")\n",
        "print()\n",
        "\n",
        "# Bias vector (one bias per output neuron)\n",
        "bias = tf.constant([0.1, 0.2, 0.3])\n",
        "print(f\"⚖️ Bias shape: {bias.shape}\")\n",
        "print(f\"Bias: {bias}\")\n",
        "print()\n",
        "\n",
        "# Broadcasting magic! bias gets added to each sample\n",
        "output_with_bias = network_output + bias\n",
        "print(f\"✨ After adding bias (broadcasting {network_output.shape} + {bias.shape}):\")\n",
        "print(output_with_bias)\n",
        "print()\n",
        "print(\"🪄 What happened? The bias vector was automatically\")\n",
        "print(\"   'broadcasted' (repeated) for each sample in the batch!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "step3_scaling",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9511d0-73f9-4ef5-f8ab-0bfc5f42edcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 BROADCASTING: Scaling Entire Tensors\n",
            "========================================\n",
            "🎚️ Original weights shape: (4, 3)\n",
            "📉 Learning rate (scalar): 0.01\n",
            "⚡ Scaled weights (for gradient descent):\n",
            "tf.Tensor(\n",
            "[[0.001 0.002 0.003]\n",
            " [0.004 0.005 0.006]\n",
            " [0.007 0.008 0.009]\n",
            " [0.002 0.003 0.004]], shape=(4, 3), dtype=float32)\n",
            "\n",
            "💡 Use case: Gradient descent weight updates!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📊 BROADCASTING EXAMPLE 2: Scaling Operations\n",
        "print(\"📊 BROADCASTING: Scaling Entire Tensors\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Scaling with a single number (scalar broadcasting)\n",
        "learning_rate = 0.01\n",
        "scaled_weights = weights * learning_rate\n",
        "\n",
        "print(f\"🎚️ Original weights shape: {weights.shape}\")\n",
        "print(f\"📉 Learning rate (scalar): {learning_rate}\")\n",
        "print(f\"⚡ Scaled weights (for gradient descent):\")\n",
        "print(scaled_weights)\n",
        "print()\n",
        "print(\"💡 Use case: Gradient descent weight updates!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "step3_normalization",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a783a3bb-67ba-4469-999b-adc2c93c1218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 BROADCASTING: Data Normalization\n",
            "===================================\n",
            "📊 Original data shape: (3, 4)\n",
            "📈 Feature means: [2. 3. 4. 5.] (shape: (4,))\n",
            "📏 Feature stds: [0.8164966 0.8164966 0.8164966 0.8164966] (shape: (4,))\n",
            "\n",
            "✨ Normalized data (zero mean, unit variance per feature):\n",
            "tf.Tensor(\n",
            "[[-1.2247448 -1.2247448 -1.2247448 -1.2247448]\n",
            " [ 0.         0.         0.         0.       ]\n",
            " [ 1.2247448  1.2247448  1.2247448  1.2247448]], shape=(3, 4), dtype=float32)\n",
            "\n",
            "🧠 Neural Network Benefit: Helps with training stability!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎯 BROADCASTING EXAMPLE 3: Normalization\n",
        "print(\"🎯 BROADCASTING: Data Normalization\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Normalize each feature across the batch\n",
        "feature_means = tf.reduce_mean(input_data, axis=0)  # Mean of each feature\n",
        "feature_stds = tf.math.reduce_std(input_data, axis=0)  # Std of each feature\n",
        "\n",
        "print(f\"📊 Original data shape: {input_data.shape}\")\n",
        "print(f\"📈 Feature means: {feature_means} (shape: {feature_means.shape})\")\n",
        "print(f\"📏 Feature stds: {feature_stds} (shape: {feature_stds.shape})\")\n",
        "print()\n",
        "\n",
        "# Normalize using broadcasting\n",
        "normalized_data = (input_data - feature_means) / feature_stds\n",
        "\n",
        "print(\"✨ Normalized data (zero mean, unit variance per feature):\")\n",
        "print(normalized_data)\n",
        "print()\n",
        "print(\"🧠 Neural Network Benefit: Helps with training stability!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_title"
      },
      "source": [
        "## ⚡ STEP 4: Advanced Operations - The Neural Network Toolkit\n",
        "### 🛠️ Operations you'll use in every neural network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "step4_transpose",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f41f79d-4464-4aad-a62b-aa8c36970917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 TRANSPOSE: Flipping Matrix Dimensions\n",
            "==========================================\n",
            "📋 Original (2, 3):\n",
            "tf.Tensor(\n",
            "[[1 2 3]\n",
            " [4 5 6]], shape=(2, 3), dtype=int32)\n",
            "\n",
            "🔄 Transposed (3, 2):\n",
            "tf.Tensor(\n",
            "[[1 4]\n",
            " [2 5]\n",
            " [3 6]], shape=(3, 2), dtype=int32)\n",
            "\n",
            "🧠 Neural Network Use Cases:\n",
            "   • Backpropagation (computing gradients)\n",
            "   • Weight matrix operations\n",
            "   • Attention mechanisms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🔄 TRANSPOSE OPERATIONS\n",
        "print(\"🔄 TRANSPOSE: Flipping Matrix Dimensions\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "original_matrix = tf.constant([[1, 2, 3],\n",
        "                               [4, 5, 6]])\n",
        "transposed = tf.transpose(original_matrix)\n",
        "\n",
        "print(f\"📋 Original {original_matrix.shape}:\")\n",
        "print(original_matrix)\n",
        "print()\n",
        "print(f\"🔄 Transposed {transposed.shape}:\")\n",
        "print(transposed)\n",
        "print()\n",
        "print(\"🧠 Neural Network Use Cases:\")\n",
        "print(\"   • Backpropagation (computing gradients)\")\n",
        "print(\"   • Weight matrix operations\")\n",
        "print(\"   • Attention mechanisms\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "step4_reductions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa15b7d-062d-4a32-83bf-0fc33787e5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 REDUCTION OPERATIONS: Summarizing Data\n",
            "==========================================\n",
            "📈 Batch data (3, 3):\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "➕ Total sum: 45.0\n",
            "📊 Mean: 5.0\n",
            "⬆️ Maximum: 9.0\n",
            "⬇️ Minimum: 1.0\n",
            "\n",
            "🔽 Row sums (axis=1): [ 6. 15. 24.]\n",
            "➡️ Column means (axis=0): [4. 5. 6.]\n",
            "\n",
            "🧠 Neural Network Applications:\n",
            "   • Loss function calculations\n",
            "   • Batch statistics for normalization\n",
            "   • Attention weight computation\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📊 REDUCTION OPERATIONS\n",
        "print(\"📊 REDUCTION OPERATIONS: Summarizing Data\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "# Sample batch of data (like loss values or predictions)\n",
        "batch_data = tf.constant([[1.0, 2.0, 3.0],\n",
        "                          [4.0, 5.0, 6.0],\n",
        "                          [7.0, 8.0, 9.0]])\n",
        "\n",
        "print(f\"📈 Batch data {batch_data.shape}:\")\n",
        "print(batch_data)\n",
        "print()\n",
        "\n",
        "# Different reduction operations\n",
        "total_sum = tf.reduce_sum(batch_data)\n",
        "batch_mean = tf.reduce_mean(batch_data)\n",
        "max_value = tf.reduce_max(batch_data)\n",
        "min_value = tf.reduce_min(batch_data)\n",
        "\n",
        "print(f\"➕ Total sum: {total_sum}\")\n",
        "print(f\"📊 Mean: {batch_mean}\")\n",
        "print(f\"⬆️ Maximum: {max_value}\")\n",
        "print(f\"⬇️ Minimum: {min_value}\")\n",
        "print()\n",
        "\n",
        "# Axis-specific reductions\n",
        "row_sums = tf.reduce_sum(batch_data, axis=1)  # Sum across columns\n",
        "col_means = tf.reduce_mean(batch_data, axis=0)  # Mean across rows\n",
        "\n",
        "print(f\"🔽 Row sums (axis=1): {row_sums}\")\n",
        "print(f\"➡️ Column means (axis=0): {col_means}\")\n",
        "print()\n",
        "print(\"🧠 Neural Network Applications:\")\n",
        "print(\"   • Loss function calculations\")\n",
        "print(\"   • Batch statistics for normalization\")\n",
        "print(\"   • Attention weight computation\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_title"
      },
      "source": [
        "## 🎮 STEP 5: Real Neural Network Simulation\n",
        "### 🏗️ Building a complete forward pass with all operations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "step5_network",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5cc613-c1cf-42e1-8586-688a2838ec42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏗️ BUILDING A COMPLETE NEURAL NETWORK LAYER\n",
            "===============================================\n",
            "🎯 Network Architecture: 4 → 3 → 2\n",
            "\n",
            "📊 Input shape: (3, 4)\n",
            "⚖️ W1 shape: (4, 3), b1 shape: (3,)\n",
            "⚖️ W2 shape: (3, 2), b2 shape: (2,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🏗️ COMPLETE NEURAL NETWORK LAYER SIMULATION\n",
        "print(\"🏗️ BUILDING A COMPLETE NEURAL NETWORK LAYER\")\n",
        "print(\"=\" * 47)\n",
        "\n",
        "# Network architecture: 4 inputs → 3 hidden → 2 outputs\n",
        "print(\"🎯 Network Architecture: 4 → 3 → 2\")\n",
        "print()\n",
        "\n",
        "# Input batch (3 samples, 4 features each)\n",
        "inputs = tf.constant([[1.0, 2.0, 3.0, 4.0],\n",
        "                      [2.0, 3.0, 4.0, 5.0],\n",
        "                      [0.5, 1.5, 2.5, 3.5]])\n",
        "\n",
        "# Layer 1: Input → Hidden (4 → 3)\n",
        "W1 = tf.random.normal([4, 3], stddev=0.1)\n",
        "b1 = tf.constant([0.1, 0.2, 0.3])\n",
        "\n",
        "# Layer 2: Hidden → Output (3 → 2)\n",
        "W2 = tf.random.normal([3, 2], stddev=0.1)\n",
        "b2 = tf.constant([0.05, 0.15])\n",
        "\n",
        "print(f\"📊 Input shape: {inputs.shape}\")\n",
        "print(f\"⚖️ W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
        "print(f\"⚖️ W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "step5_forward",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb8086a-676a-431c-a3d8-19dc31fd7f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 FORWARD PASS EXECUTION\n",
            "===========================\n",
            "📍 Step 1: Linear transformation (Layer 1)\n",
            "   Linear output shape: (3, 3)\n",
            "   Sample values: [ 0.81007755  0.3584847  -0.14984661]\n",
            "\n",
            "📍 Step 2: Apply activation function (ReLU)\n",
            "   Activated shape: (3, 3)\n",
            "   Sample values: [0.81007755 0.3584847  0.        ]\n",
            "\n",
            "📍 Step 3: Second linear transformation (Layer 2)\n",
            "   Output linear shape: (3, 2)\n",
            "   Sample values: [0.21121259 0.13047361]\n",
            "\n",
            "📍 Step 4: Final activation (Sigmoid for binary classification)\n",
            "   Final output shape: (3, 2)\n",
            "   Predictions for all samples:\n",
            "tf.Tensor(\n",
            "[[0.5526077  0.5325722 ]\n",
            " [0.56321675 0.53125054]\n",
            " [0.54728454 0.53323287]], shape=(3, 2), dtype=float32)\n",
            "\n",
            "🎉 COMPLETE! We just simulated a 2-layer neural network!\n",
            "   Used: Matrix multiplication, broadcasting, element-wise operations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🚀 FORWARD PASS: Step by step\n",
        "print(\"🚀 FORWARD PASS EXECUTION\")\n",
        "print(\"=\" * 27)\n",
        "\n",
        "print(\"📍 Step 1: Linear transformation (Layer 1)\")\n",
        "hidden_linear = tf.matmul(inputs, W1) + b1  # Matrix mult + Broadcasting\n",
        "print(f\"   Linear output shape: {hidden_linear.shape}\")\n",
        "print(f\"   Sample values: {hidden_linear[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"📍 Step 2: Apply activation function (ReLU)\")\n",
        "hidden_activated = tf.nn.relu(hidden_linear)  # Element-wise operation\n",
        "print(f\"   Activated shape: {hidden_activated.shape}\")\n",
        "print(f\"   Sample values: {hidden_activated[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"📍 Step 3: Second linear transformation (Layer 2)\")\n",
        "output_linear = tf.matmul(hidden_activated, W2) + b2\n",
        "print(f\"   Output linear shape: {output_linear.shape}\")\n",
        "print(f\"   Sample values: {output_linear[0]}\")\n",
        "print()\n",
        "\n",
        "print(\"📍 Step 4: Final activation (Sigmoid for binary classification)\")\n",
        "final_output = tf.nn.sigmoid(output_linear)\n",
        "print(f\"   Final output shape: {final_output.shape}\")\n",
        "print(f\"   Predictions for all samples:\")\n",
        "print(final_output)\n",
        "print()\n",
        "\n",
        "print(\"🎉 COMPLETE! We just simulated a 2-layer neural network!\")\n",
        "print(\"   Used: Matrix multiplication, broadcasting, element-wise operations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validation_title"
      },
      "source": [
        "## ✅ VALIDATION & DEBUGGING\n",
        "### 🔍 Let's test your mathematical mastery!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "validation_shapes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b518fa-2902-4dc4-dd01-f1d43f2d1a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧩 MATHEMATICAL DEBUGGING CHALLENGE\n",
            "======================================\n",
            "🔍 Checking common neural network shape issues...\n",
            "\n",
            "✅ Test 1 PASSED: (32, 784) × (784, 128) = (32, 128)\n",
            "✅ Test 2 PASSED: Broadcasting (10, 5) + (5,) = (10, 5)\n",
            "\n",
            "🎯 Key Debugging Skills:\n",
            "   • Always check tensor shapes before operations\n",
            "   • Remember matrix multiplication rules\n",
            "   • Understand broadcasting patterns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🧩 SHAPE DEBUGGING CHALLENGE\n",
        "print(\"🧩 MATHEMATICAL DEBUGGING CHALLENGE\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "# Create some \"problematic\" scenarios\n",
        "print(\"🔍 Checking common neural network shape issues...\")\n",
        "print()\n",
        "\n",
        "# Test case 1: Batch size compatibility\n",
        "batch1 = tf.random.normal([32, 784])  # 32 samples, 784 features (like MNIST)\n",
        "weights1 = tf.random.normal([784, 128])  # 784 → 128 transformation\n",
        "\n",
        "try:\n",
        "    result1 = tf.matmul(batch1, weights1)\n",
        "    print(f\"✅ Test 1 PASSED: {batch1.shape} × {weights1.shape} = {result1.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1 FAILED: {e}\")\n",
        "\n",
        "# Test case 2: Broadcasting bias addition\n",
        "output = tf.random.normal([10, 5])  # 10 samples, 5 outputs\n",
        "bias = tf.random.normal([5])  # 5 bias values\n",
        "\n",
        "try:\n",
        "    result2 = output + bias\n",
        "    print(f\"✅ Test 2 PASSED: Broadcasting {output.shape} + {bias.shape} = {result2.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2 FAILED: {e}\")\n",
        "\n",
        "print()\n",
        "print(\"🎯 Key Debugging Skills:\")\n",
        "print(\"   • Always check tensor shapes before operations\")\n",
        "print(\"   • Remember matrix multiplication rules\")\n",
        "print(\"   • Understand broadcasting patterns\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "validation_practical",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b08e2b-e0df-4000-ed06-2ca7c928d20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎪 PRACTICAL SCENARIO: Image Classification Pipeline\n",
            "======================================================\n",
            "📸 Image batch: (5, 784) (5 images, 784 pixels each)\n",
            "🎯 Classifier weights: (784, 10)\n",
            "⚖️ Classifier bias: (10,)\n",
            "\n",
            "📊 Raw scores (logits): (5, 10)\n",
            "🎲 Probability predictions: (5, 10)\n",
            "\n",
            "🔍 Sample prediction (probabilities for 10 classes):\n",
            "tf.Tensor(\n",
            "[0.10473149 0.09422571 0.09968884 0.11491498 0.11673317 0.09374061\n",
            " 0.09624387 0.09522426 0.08923497 0.09526205], shape=(10,), dtype=float32)\n",
            "📈 Probabilities sum to: 1.000\n",
            "\n",
            "🏆 Predicted classes for all 5 images: [4 4 4 1 4]\n",
            "\n",
            "🎉 SUCCESS! You've implemented a complete classification pipeline!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🎪 PRACTICAL SCENARIO: Image Classification\n",
        "print(\"🎪 PRACTICAL SCENARIO: Image Classification Pipeline\")\n",
        "print(\"=\" * 54)\n",
        "\n",
        "# Simulate a batch of flattened images (like MNIST)\n",
        "batch_size = 5\n",
        "image_pixels = 28 * 28  # 784 pixels per image\n",
        "num_classes = 10\n",
        "\n",
        "# Fake image data\n",
        "images = tf.random.uniform([batch_size, image_pixels], 0, 1)\n",
        "print(f\"📸 Image batch: {images.shape} (5 images, 784 pixels each)\")\n",
        "\n",
        "# Classification weights and bias\n",
        "classifier_weights = tf.random.normal([image_pixels, num_classes], stddev=0.01)\n",
        "classifier_bias = tf.zeros([num_classes])\n",
        "\n",
        "print(f\"🎯 Classifier weights: {classifier_weights.shape}\")\n",
        "print(f\"⚖️ Classifier bias: {classifier_bias.shape}\")\n",
        "print()\n",
        "\n",
        "# Forward pass\n",
        "logits = tf.matmul(images, classifier_weights) + classifier_bias\n",
        "predictions = tf.nn.softmax(logits)  # Convert to probabilities\n",
        "\n",
        "print(f\"📊 Raw scores (logits): {logits.shape}\")\n",
        "print(f\"🎲 Probability predictions: {predictions.shape}\")\n",
        "print()\n",
        "print(\"🔍 Sample prediction (probabilities for 10 classes):\")\n",
        "print(predictions[0])  # First image's predictions\n",
        "print(f\"📈 Probabilities sum to: {tf.reduce_sum(predictions[0]):.3f}\")\n",
        "print()\n",
        "\n",
        "# Find predicted class\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "print(f\"🏆 Predicted classes for all 5 images: {predicted_classes}\")\n",
        "print()\n",
        "print(\"🎉 SUCCESS! You've implemented a complete classification pipeline!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takeaways"
      },
      "source": [
        "## 🔍 KEY TAKEAWAYS\n",
        "\n",
        "### 🧮 **Mathematical Operations Mastery:**\n",
        "1. **Element-wise operations** work on corresponding elements (broadcasting magic)\n",
        "2. **Matrix multiplication** transforms data between layers (the core of neural networks)\n",
        "3. **Broadcasting** lets different shaped tensors work together efficiently\n",
        "4. **Reduction operations** summarize data (losses, statistics, attention)\n",
        "\n",
        "### 🧠 **Neural Network Applications:**\n",
        "- **Forward pass** = Chain of matrix multiplications + activations\n",
        "- **Bias addition** uses broadcasting for efficiency\n",
        "- **Normalization** uses element-wise operations and broadcasting\n",
        "- **Shape compatibility** is crucial for debugging\n",
        "\n",
        "### 💡 **Pro Tips:**\n",
        "- Always check tensor shapes before operations\n",
        "- Use broadcasting to avoid explicit loops\n",
        "- Matrix multiplication: `(m,n) × (n,k) = (m,k)`\n",
        "- Element-wise operations preserve shape\n",
        "\n",
        "### 🤔 **Questions to Ponder:**\n",
        "- How would you implement batch normalization using these operations?\n",
        "\n",
        "\n",
        "- What happens to shapes during backpropagation?\n",
        "\n",
        "\n",
        "- How do attention mechanisms use these mathematical operations?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}