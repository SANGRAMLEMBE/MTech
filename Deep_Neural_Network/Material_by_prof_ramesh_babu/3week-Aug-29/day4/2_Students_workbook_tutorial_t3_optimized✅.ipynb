{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“– Students Interactive Workbook: Neural Networks & Tensors\n",
    "## Tutorial T3 - Week 3, Day 4 | Live Instruction Session\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š **Course Information**\n",
    "**Deep Neural Network Architectures (21CSE558T)**  \n",
    "**SRM University | M.Tech Data Science & Business Systems**\n",
    "\n",
    "### ğŸ‘¨â€ğŸ« **Instructor & Copyright**\n",
    "**Â© 2025 Prof. Ramesh Babu**  \n",
    "*This workbook is the intellectual property of Prof. Ramesh Babu, SRM University*  \n",
    "*Unauthorized reproduction or distribution is prohibited*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ **Student Information**\n",
    "**Please fill in your details:**\n",
    "\n",
    "| Field | Your Information |\n",
    "|-------|------------------|\n",
    "| **Student Name** | _________________________________ |\n",
    "| **Registration Number** | _________________________________ |\n",
    "| **Branch & Year** | _________________________________ |\n",
    "| **Session Date** | _________________________________ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **How This Session Works**\n",
    "\n",
    "### ğŸ“± **IMPORTANT: Code Distribution Method**\n",
    "ğŸ”¥ **All code will be shared via Google Meet Chat** ğŸ”¥\n",
    "\n",
    "**Process:**\n",
    "1. Prof. Ramesh Babu will **explain the concept**\n",
    "2. Code will be **posted in Google Meet chat**\n",
    "3. **Copy and paste** the code into the cell below\n",
    "4. **Execute together** and discuss results\n",
    "5. **Take notes** in the provided spaces\n",
    "\n",
    "### â° **Session Timeline (60 minutes)**\n",
    "- **0-5 min**: Setup & Introduction\n",
    "- **5-25 min**: Activation Functions (Sigmoid, ReLU, Gradients)\n",
    "- **25-40 min**: Tensor Operations & Shape Manipulation\n",
    "- **40-55 min**: Building Neural Layers & Networks\n",
    "- **55-60 min**: Assessment & Summary\n",
    "\n",
    "### ğŸ¯ **Learning Objectives**\n",
    "By the end of this session, you will understand:\n",
    "1. ğŸ”Œ **Activation Functions**: How neural \"switches\" work (car accelerator vs one-way door)\n",
    "2. ğŸ“ **Tensors**: Multi-dimensional data handling (like spreadsheets and matrices)\n",
    "3. ğŸ§  **Neural Layers**: Information processing units (factory assembly line stations)\n",
    "4. ğŸ—ï¸ **Networks**: Connected systems for problem-solving (restaurant kitchen workflow)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Section 1: Environment Setup (5 minutes)\n",
    "\n",
    "## ğŸ› ï¸ Loading Our Mathematical Tools\n",
    "\n",
    "Just like opening your toolbox before starting any engineering project, we need to load our computational tools.\n",
    "\n",
    "### ğŸŒ **Real-World Analogy:**\n",
    "Think of this like **preparing for a cooking recipe** - you gather all ingredients and tools before you start cooking!\n",
    "\n",
    "**What each tool does:**\n",
    "- **NumPy**: Mathematical calculations (like a calculator)\n",
    "- **Matplotlib**: Creating graphs and charts (like graph paper)\n",
    "- **TensorFlow**: Neural network operations (like specialized equipment)\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will now share the setup code in Google Meet Chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 1\n# ğŸ“‹ COPY CODE FROM GOOGLE MEET CHAT AND PASTE HERE\n# Prof. Ramesh Babu will share: \"Please run Code Cell 1\"\n# Expected code: import numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n# Also includes: plt.style.use, plt.rcParams, random seeds, and setup confirmation\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### âœï¸ **Your Setup Notes:**\n",
    "**What did you observe after running the setup?**\n",
    "- NumPy version: _________________\n",
    "- TensorFlow version: _________________\n",
    "- Any warnings or messages: _________________\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "# Section 2: Activation Functions - Smart Switches (20 minutes)\n",
    "\n",
    "## ğŸ”Œ What Are Activation Functions?\n",
    "\n",
    "### ğŸŒ **Everyday Analogies Everyone Can Understand:**\n",
    "\n",
    "#### ğŸš— **Sigmoid = Car Accelerator Pedal**\n",
    "- Press lightly â†’ car moves slowly\n",
    "- Press harder â†’ car speeds up\n",
    "- **Never goes from 0 to full speed instantly** - always smooth!\n",
    "\n",
    "#### ğŸšª **ReLU = One-Way Door**\n",
    "- Push from right side â†’ door opens (signal passes)\n",
    "- Push from left side â†’ door stays closed (signal blocked)\n",
    "- **Simple: either fully open or fully closed**\n",
    "\n",
    "### ğŸ’­ **Quick Discussion:**\n",
    "*In your daily life, what other things behave like \"smart switches\"?*\n",
    "**Your examples:** _________________________________\n",
    "\n",
    "## ğŸ“ˆ Understanding Sigmoid: The Smooth Switch\n",
    "\n",
    "### ğŸ“š **Mathematical Formula:** \n",
    "Ïƒ(x) = 1/(1+e^(-x))\n",
    "\n",
    "### ğŸ” **Properties:**\n",
    "- **Input:** Any number (can be huge or tiny)\n",
    "- **Output:** Always between 0 and 1 (like a percentage)\n",
    "- **Behavior:** Smooth S-shaped curve\n",
    "\n",
    "### ğŸ¤” **Before We Code - Prediction Exercise:**\n",
    "**What do you think these outputs will be?**\n",
    "- sigmoid(-5) â‰ˆ _______ (very negative input)\n",
    "- sigmoid(0) = _______ (zero input)\n",
    "- sigmoid(5) â‰ˆ _______ (very positive input)\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will share the sigmoid code in chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 2\n# ğŸ“‹ COPY SIGMOID FUNCTION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 2\"\n# Expected code: sigmoid_function(x) implementation with 1/(1+np.exp(-x))\n# Also includes: test_values array, loop to test sigmoid with different inputs\n# Shows: Input â†’ Output table and key insights about sigmoid behavior"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 3\n# ğŸ“‹ COPY SIGMOID VISUALIZATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 3\"\n# Expected code: plt.figure, plt.subplot(1,2,1), sigmoid plotting with annotations\n# Also includes: ECE analogy plot, key points marking, and insights\n# Shows: Sigmoid curve, soft-limiting amplifier analogy, key observations"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### âœï¸ **Sigmoid Results & Observations:**\n",
    "**Were your predictions correct?**\n",
    "- sigmoid(-5) = _______ (Was I close: Y/N)\n",
    "- sigmoid(0) = _______ (Was I close: Y/N)\n",
    "- sigmoid(5) = _______ (Was I close: Y/N)\n",
    "\n",
    "**What did you notice about the pattern?** _________________________________\n",
    "\n",
    "### ğŸ¤” Concept Check: Sigmoid\n",
    "Before moving on, let's make sure you understand:\n",
    "1. **What happens when x = 0?** (Answer: sigmoid(0) = 0.5)\n",
    "2. **What happens with very large positive x?** (Answer: sigmoid approaches 1)\n",
    "3. **What happens with very large negative x?** (Answer: sigmoid approaches 0)\n",
    "4. **Why is this useful in neural networks?** (Answer: It gives a probability-like output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## âš¡ Understanding ReLU: The One-Way Valve\n",
    "\n",
    "### ğŸ“š **Mathematical Formula:** \n",
    "f(x) = max(0, x) - *\"Keep the larger of 0 or x\"*\n",
    "\n",
    "### ğŸŒ **Real-World Examples:**\n",
    "- **ğŸš° Water faucet**: Turn left (negative) â†’ no water, turn right (positive) â†’ water flows\n",
    "- **ğŸ›£ï¸ One-way street**: Can only go in one direction\n",
    "- **âš¡ Circuit breaker**: Blocks negative current, allows positive current\n",
    "\n",
    "### ğŸ¤” **Before We Code - Prediction Exercise:**\n",
    "**What do you think ReLU will output?**\n",
    "- ReLU(-3) = _______ (negative input)\n",
    "- ReLU(0) = _______ (zero input)\n",
    "- ReLU(7) = _______ (positive input)\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will share the ReLU code in chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 4\n# ğŸ“‹ COPY RELU FUNCTION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 4\"\n# Expected code: relu_function(x) with np.maximum(0, x)\n# Also includes: test_values, loop to test ReLU, key observations\n# Shows: Input â†’ Output table and ReLU behavior insights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 5\n# ğŸ“‹ COPY RELU VISUALIZATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 5\"\n# Expected code: plt.figure(figsize=(14,5)), three subplots showing:\n# 1. ReLU function with kink annotation, 2. ECE diode analogy, 3. ReLU vs Sigmoid comparison\n# Shows: One-way valve behavior, hard vs soft switch comparison"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### âœï¸ **ReLU vs Sigmoid Comparison:**\n",
    "**Fill in what you observed:**\n",
    "\n",
    "| Aspect | Sigmoid (Car Accelerator) | ReLU (One-Way Door) |\n",
    "|--------|---------------------------|--------------------||\n",
    "| **Negative Inputs** | _____________ | _____________ |\n",
    "| **Zero Input** | _____________ | _____________ |\n",
    "| **Positive Inputs** | _____________ | _____________ |\n",
    "| **Shape** | _____________ | _____________ |\n",
    "\n",
    "### ğŸ’¡ **Key Insight Discussion:**\n",
    "*Which activation would you choose for a simple on/off switch? Why?*\n",
    "**Your reasoning:** _________________________________\n",
    "\n",
    "## ğŸ“ˆ Understanding Gradients (Slopes)\n",
    "\n",
    "### ğŸŒ **Simple Analogy: Hill Climbing**\n",
    "Imagine you're **climbing a hill in fog** and want to reach the top:\n",
    "- **Steep slope** â†’ you know which way to go, take big steps\n",
    "- **Gentle slope** â†’ you're unsure, take small steps\n",
    "- **Flat ground** â†’ you're lost, don't know which way to go\n",
    "\n",
    "### ğŸ§  **In Neural Networks:**\n",
    "- **Large gradient** â†’ fast learning (big steps toward solution)\n",
    "- **Small gradient** â†’ slow learning (small steps)\n",
    "- **Zero gradient** â†’ no learning (stuck!)\n",
    "\n",
    "### âš ï¸ **The Problem:**\n",
    "Sigmoid has a **vanishing gradient problem** - slopes become very flat, making learning slow!\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will demonstrate gradient functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 6\n# ğŸ“‹ COPY GRADIENT FUNCTIONS CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 6\"\n# Expected code: sigmoid_gradient(x) = s*(1-s), relu_gradient(x) with np.where\n# Also includes: test_values array, testing both gradient functions\n# Shows: Gradient comparison and vanishing gradient problem explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 7\n# ğŸ“‹ COPY GRADIENT VISUALIZATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 7\"\n# Expected code: plt.figure(figsize=(15,5)), three subplots showing:\n# 1. Sigmoid function vs gradient, 2. ReLU function vs gradient, 3. Gradient comparison\n# Shows: Vanishing gradient problem visualization and why ReLU is better"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### âœï¸ **Gradient Insights:**\n",
    "**After seeing the gradient plots:**\n",
    "1. What happens to sigmoid gradients at the edges? _____________________\n",
    "2. What is ReLU's gradient for positive inputs? _____________________\n",
    "3. Why does this make ReLU better for learning? _____________________\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "# Section 3: Tensors - Just Fancy Arrays (15 minutes)\n",
    "\n",
    "## ğŸ“ What Are Tensors?\n",
    "\n",
    "**Don't panic!** Tensors are just **organized collections of numbers**.\n",
    "\n",
    "### ğŸŒ **Everyday Examples:**\n",
    "- **ğŸ“Š Spreadsheet** (2D tensor): Rows and columns of data\n",
    "- **ğŸ“š Address book** (1D tensor): List of phone numbers\n",
    "- **ğŸ¢ Office building** (3D tensor): Floor â†’ Room â†’ Person\n",
    "\n",
    "### ğŸ“š **You Already Know These:**\n",
    "- **Number**: `5` (just one value)\n",
    "- **List**: `[1, 2, 3]` (row of values)\n",
    "- **Table**: `[[1, 2], [3, 4]]` (rows and columns)\n",
    "\n",
    "### ğŸ’­ **Quick Brainstorm:**\n",
    "*What data in your field has multiple dimensions?*\n",
    "**Your examples:** _________________________________\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will demonstrate tensor creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 8\n# ğŸ“‹ COPY TENSOR CREATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 8\"\n# Expected code: tf.constant for scalar, vector, matrix, 3D tensor\n# Also includes: print statements showing values, shapes, and real-world analogies\n# Shows: Different tensor types with shape explanations"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ğŸ”„ Basic Tensor Operations\n",
    "\n",
    "### ğŸŒ **Matrix Operations - Like Recipe Instructions:**\n",
    "\n",
    "#### ğŸ¥— **Element-wise multiplication** (like seasoning):\n",
    "- Add salt to **each ingredient separately**\n",
    "- A[1,1] Ã— B[1,1], A[1,2] Ã— B[1,2], etc.\n",
    "\n",
    "#### ğŸ³ **Matrix multiplication** (like combining ingredients):\n",
    "- **Mix ingredients according to a recipe**\n",
    "- Complex combination following specific rules\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will demonstrate matrix operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 9\n# ğŸ“‹ COPY MATRIX OPERATIONS CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 9\"\n# Expected code: tf.constant for matrices A and B, tf.multiply (element-wise)\n# tf.matmul (matrix multiplication), manual verification of first element\n# Shows: Element-wise vs matrix multiplication differences"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ğŸ“ Shape Manipulation\n",
    "\n",
    "### ğŸŒ **Analogy: Reorganizing Your Closet**\n",
    "- **Reshape**: Same clothes, different organization (2 rows of 5 shirts â†’ 5 rows of 2 shirts)\n",
    "- **Transpose**: Flip arrangement (tall stack â†’ wide spread)\n",
    "- **Flatten**: Put everything in one pile\n",
    "\n",
    "### ğŸ’¡ **Key Rule:** \n",
    "Total number of items **stays the same** - just organized differently!\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will demonstrate shape operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 10\n# ğŸ“‹ COPY SHAPE MANIPULATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 10\"\n# Expected code: tf.constant original matrix, tf.reshape operations\n# tf.transpose, flatten with [-1], element count verification\n# Shows: Reshape, transpose, flatten operations with explanations"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### âœï¸ **Tensor Operations Summary:**\n",
    "**Fill in what each operation does:**\n",
    "\n",
    "| Operation | What it does | Everyday analogy |\n",
    "|-----------|--------------|------------------|\n",
    "| **Element-wise Ã—** | _________________ | _________________ |\n",
    "| **Matrix Ã—** | _________________ | _________________ |\n",
    "| **Reshape** | _________________ | _________________ |\n",
    "| **Transpose** | _________________ | _________________ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "# Section 4: Building Neural Layers - Processing Units (10 minutes)\n",
    "\n",
    "## ğŸ§  What Is a Neural Layer?\n",
    "\n",
    "### ğŸŒ **Factory Assembly Line Analogy:**\n",
    "A neural layer is like **one station in a factory**:\n",
    "1. **Raw materials come in** (inputs)\n",
    "2. **Workers apply tools** (multiply by weights)\n",
    "3. **Add quality control** (add bias)\n",
    "4. **Apply finishing process** (activation function)\n",
    "5. **Send to next station** (output)\n",
    "\n",
    "### ğŸ“š **Mathematical Process:**\n",
    "```\n",
    "output = activation(input Ã— weights + bias)\n",
    "```\n",
    "\n",
    "### ğŸ”§ **Engineering Examples:**\n",
    "- **Signal amplifier**: Input signal â†’ gain (weights) â†’ offset (bias) â†’ filter (activation)\n",
    "- **Control system**: Sensor input â†’ controller gain â†’ reference point â†’ actuator response\n",
    "\n",
    "### ğŸ¤” **Before Building:**\n",
    "*What systems in your experience take inputs, process them, and produce outputs?*\n",
    "**Your examples:** _________________________________\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will build a neural layer step-by-step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 11\n# ğŸ“‹ COPY NEURAL LAYER BUILDING CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 11\"\n# Expected code: input_size, output_size definition, weights and bias initialization\n# test_input creation, forward pass computation (linear + activation)\n# Shows: Step-by-step layer construction and testing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 12\n# ğŸ“‹ COPY LAYER FUNCTION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 12\"\n# Expected code: function definition with linear transformation and activation\n# test cases with different inputs, comparison of outputs\n# Shows: Reusable layer function and behavior analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 13\n# ğŸ“‹ COPY LAYER VISUALIZATION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 13\"\n# Expected code: input_range creation, output calculation for visualization\n# plt.figure with subplots showing layer response and ReLU effect\n# Shows: How layers transform inputs and effect of activation functions"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### âœï¸ **Neural Layer Understanding:**\n",
    "**Fill in what you observed:**\n",
    "\n",
    "| Component | Purpose | Size for 3â†’2 layer |\n",
    "|-----------|---------|---------------------|\n",
    "| **Weights** | _________________ | _________________ |\n",
    "| **Bias** | _________________ | _________________ |\n",
    "| **Activation** | _________________ | _________________ |\n",
    "\n",
    "**What happened when you tested different inputs?** _________________________________\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "# Section 5: Complete Networks - Connected Systems (10 minutes)\n",
    "\n",
    "## ğŸ—ï¸ Building Neural Networks\n",
    "\n",
    "### ğŸŒ **Restaurant Kitchen Analogy:**\n",
    "A neural network is like a **professional kitchen**:\n",
    "- **Prep station** (Layer 1): Clean and cut ingredients\n",
    "- **Cooking station** (Layer 2): Apply heat and seasoning\n",
    "- **Plating station** (Layer 3): Final presentation\n",
    "\n",
    "Each station takes output from previous station as input!\n",
    "\n",
    "### ğŸ”— **Connection Pattern:**\n",
    "```\n",
    "Input â†’ Layer 1 â†’ Layer 2 â†’ Output\n",
    "  3   â†’    4    â†’    2    (our example)\n",
    "```\n",
    "\n",
    "### ğŸ’­ **Discussion:**\n",
    "*Why use multiple layers instead of one big layer?*\n",
    "**Your thoughts:** _________________________________\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will demonstrate network construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 14\n# ğŸ“‹ COPY NEURAL NETWORK BUILDING CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 14\"\n# Expected code: weights1, bias1, weights2, bias2 initialization\n# network_input definition, forward pass through both layers\n# Shows: Multi-layer network architecture and forward propagation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 15\n# ğŸ“‹ COPY NETWORK FUNCTION CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 15\"\n# Expected code: function definition combining two layers\n# test_cases array with various inputs, network testing loop\n# Shows: Complete network function and comprehensive testing"
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### âœï¸ **Network Architecture Notes:**\n",
    "**Document your network:**\n",
    "\n",
    "| Layer | Input â†’ Output | Activation | What it does |\n",
    "|-------|----------------|------------|-------------|\n",
    "| **Layer 1** | _____ â†’ _____ | _____ | _________________ |\n",
    "| **Layer 2** | _____ â†’ _____ | _____ | _________________ |\n",
    "\n",
    "**How did the complete network behave differently from a single layer?**\n",
    "_________________________________\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "# Section 6: Understanding Check & Assessment (5 minutes)\n",
    "\n",
    "## ğŸ¯ Quick Knowledge Check\n",
    "\n",
    "### ğŸ’­ **Class Discussion - Answer Together:**\n",
    "\n",
    "**1. Sigmoid vs ReLU:**\n",
    "- Which is like a car accelerator? _________________\n",
    "- Which is like a one-way door? _________________\n",
    "- Which is better for learning? _________________ Why? _________________\n",
    "\n",
    "**2. Tensors:**\n",
    "- What's a 1D tensor? _________________\n",
    "- What's a 2D tensor? _________________\n",
    "- Give a real-world example: _________________\n",
    "\n",
    "**3. Neural Layers:**\n",
    "- What are the 4 steps in a layer? _________________\n",
    "- Why do we need activation functions? _________________\n",
    "\n",
    "**4. Networks:**\n",
    "- How do layers connect? _________________\n",
    "- Why multiple layers? _________________\n",
    "\n",
    "### ğŸ’¬ **Prof. Ramesh Babu will run understanding check code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 16\n# ğŸ“‹ COPY UNDERSTANDING CHECK CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 16\"\n# Expected code: simple questions and answers, practical test\n# simple_input test with network, verification of understanding\n# Shows: Concept reinforcement and practical application"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¢ RUNNING CODE CELL 17\n# ğŸ“‹ COPY GENTLE ASSESSMENT CODE FROM GOOGLE MEET CHAT\n# Prof. Ramesh Babu will share: \"Please run Code Cell 17\"\n# Expected code: gentle_assessment() function with friendly tests\n# sigmoid test, ReLU test, tensor shape test, layer test, conceptual understanding\n# Shows: Comprehensive but friendly assessment of learning"
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### âœï¸ **Final Assessment Results:**\n",
    "**What did the assessment show?**\n",
    "- Your score: _____ / 5\n",
    "- Grade received: _____\n",
    "- Areas of strength: _________________________________\n",
    "- Areas to review: _________________________________\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "# ğŸŒŸ Session Summary & Next Steps\n",
    "\n",
    "## âœ… **What We Accomplished Today:**\n",
    "\n",
    "### **Core Concepts Mastered:**\n",
    "- [x] **Activation Functions**: Smart switches (car accelerator vs one-way door)\n",
    "- [x] **Gradients**: Learning slopes (hill climbing analogy)\n",
    "- [x] **Tensors**: Organized data collections (spreadsheets, address books)\n",
    "- [x] **Neural Layers**: Processing units (factory stations, kitchen stations)\n",
    "- [x] **Networks**: Connected systems (assembly line, restaurant kitchen)\n",
    "\n",
    "### **Technical Skills Developed:**\n",
    "- [x] Implementing mathematical functions in Python\n",
    "- [x] Understanding tensor operations and shapes\n",
    "- [x] Building neural network components\n",
    "- [x] Connecting layers into complete systems\n",
    "- [x] Visualizing function behaviors and network responses\n",
    "\n",
    "## ğŸ”— **Real-World Connections:**\n",
    "\n",
    "### âœï¸ **Personal Reflection - Connect to Your Field:**\n",
    "**How do today's concepts relate to your engineering background?**\n",
    "\n",
    "**Neural networks remind me of:** _________________________________\n",
    "\n",
    "**Activation functions are like:** _________________________________\n",
    "\n",
    "**I could use this for:** _________________________________\n",
    "\n",
    "## ğŸš€ **Next Week - Module 2 Preview:**\n",
    "**\"How Neural Networks Learn\"**\n",
    "- How networks adjust weights automatically\n",
    "- Optimization algorithms (the \"learning\" process)\n",
    "- Making networks smarter through training\n",
    "\n",
    "### ğŸ“š **To Prepare:**\n",
    "- [ ] Review today's analogies\n",
    "- [ ] Think about \"trial and error\" learning in your field\n",
    "- [ ] Come with questions about how machines \"learn\"\n",
    "\n",
    "## ğŸ’¡ **Three Key Takeaways:**\n",
    "**Write your most important insights:**\n",
    "\n",
    "1. _________________________________\n",
    "2. _________________________________  \n",
    "3. _________________________________\n",
    "\n",
    "## ğŸ¤” **Questions for Next Time:**\n",
    "**What would you like to explore further?**\n",
    "- _________________________________\n",
    "- _________________________________\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŠ Congratulations!\n",
    "\n",
    "You've successfully learned the fundamentals of neural networks through relatable analogies and hands-on experience!\n",
    "\n",
    "**Remember:** Neural networks are just **mathematical systems** that can be understood through engineering principles you already know.\n",
    "\n",
    "### ğŸ“§ **Contact Information:**\n",
    "**Prof. Ramesh Babu**  \n",
    "Email: [ramesh.babu@srmist.edu.in]  \n",
    "Office Hours: [To be announced]\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ **Â© Copyright Notice**\n",
    "*This workbook and all its contents are the intellectual property of **Prof. Ramesh Babu**, SRM University. This material is provided exclusively for educational use by enrolled students of 21CSE558T - Deep Neural Network Architectures. Unauthorized reproduction, distribution, or commercial use is strictly prohibited.*\n",
    "\n",
    "*For permissions or inquiries, contact Prof. Ramesh Babu directly.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}