{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ogo4vr7xbd0"
      },
      "source": [
        "# Tutorial T3: Building Programs to Perform Basic Operations in Tensors\n",
        "## COMPLETE SOLUTIONS - Instructor Reference\n",
        "**Week 3, Day 4 - Deep Neural Network Architectures**\n",
        "\n",
        "**Duration:** 1 Hour | **Format:** Hands-On Tutorial\n",
        "\n",
        "### Learning Objectives\n",
        "- ✅ Implement custom activation functions from mathematical principles\n",
        "- ✅ Understand gradient computation for backpropagation\n",
        "- ✅ Master tensor operations in TensorFlow\n",
        "- ✅ Build a neural network layer from scratch\n",
        "- ✅ Construct a complete multi-layer network\n",
        "\n",
        "---\n",
        "**🔒 INSTRUCTOR SOLUTIONS - Complete implementations with explanations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YqXfmAOxbd2"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHj7Z0dKxbd2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(\"Environment ready! 🚀\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRFdlyaXxbd2"
      },
      "source": [
        "## Part 1: Custom Activation Functions (20 minutes)\n",
        "\n",
        "### Task 1A: Implement Basic Activation Functions ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pP6AFstxbd2"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation: σ(x) = 1/(1+e^(-x))\n",
        "\n",
        "    Key points:\n",
        "    - Output range: (0, 1)\n",
        "    - Used for binary classification\n",
        "    - Suffers from vanishing gradient problem\n",
        "    \"\"\"\n",
        "    # Clip x to prevent overflow in exp(-x)\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh_custom(x):\n",
        "    \"\"\"Hyperbolic tangent: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))\n",
        "\n",
        "    Key points:\n",
        "    - Output range: (-1, 1)\n",
        "    - Zero-centered (better than sigmoid)\n",
        "    - Still suffers from vanishing gradients\n",
        "    \"\"\"\n",
        "    # Use numpy's built-in tanh for numerical stability\n",
        "    # But showing manual implementation for understanding:\n",
        "    x = np.clip(x, -500, 500)\n",
        "    exp_x = np.exp(x)\n",
        "    exp_neg_x = np.exp(-x)\n",
        "    return (exp_x - exp_neg_x) / (exp_x + exp_neg_x)\n",
        "\n",
        "def relu_custom(x):\n",
        "    \"\"\"ReLU activation: max(0,x)\n",
        "\n",
        "    Key points:\n",
        "    - Output range: [0, ∞)\n",
        "    - No vanishing gradient for positive values\n",
        "    - Can suffer from \"dying ReLU\" problem\n",
        "    - Most popular activation for hidden layers\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu_custom(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU: x if x>0, alpha*x otherwise\n",
        "\n",
        "    Key points:\n",
        "    - Fixes dying ReLU problem\n",
        "    - Small gradient for negative values\n",
        "    - alpha typically 0.01 to 0.3\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Bonus: Advanced activations\n",
        "def elu_custom(x, alpha=1.0):\n",
        "    \"\"\"ELU activation: x if x>0, alpha*(e^x - 1) otherwise\n",
        "\n",
        "    Key points:\n",
        "    - Smooth negative part\n",
        "    - Faster convergence than ReLU\n",
        "    - Computationally more expensive\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha * (np.exp(np.clip(x, -500, 500)) - 1))\n",
        "\n",
        "def swish_custom(x, beta=1.0):\n",
        "    \"\"\"Swish activation: x * sigmoid(beta*x)\n",
        "\n",
        "    Key points:\n",
        "    - Self-gated activation\n",
        "    - Smooth and non-monotonic\n",
        "    - Good performance in deep networks\n",
        "    \"\"\"\n",
        "    return x * sigmoid(beta * x)\n",
        "\n",
        "print(\"✅ All activation functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9fTuIQNxbd3"
      },
      "source": [
        "### Test Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfTTiTk8xbd3"
      },
      "outputs": [],
      "source": [
        "# Test values covering edge cases\n",
        "test_values = np.array([-5, -2, -1, 0, 1, 2, 5])\n",
        "\n",
        "print(\"Testing activation functions with various inputs:\")\n",
        "print(f\"Input: {test_values}\")\n",
        "print(f\"Sigmoid:    {sigmoid(test_values).round(4)}\")\n",
        "print(f\"Tanh:       {tanh_custom(test_values).round(4)}\")\n",
        "print(f\"ReLU:       {relu_custom(test_values).round(4)}\")\n",
        "print(f\"Leaky ReLU: {leaky_relu_custom(test_values).round(4)}\")\n",
        "print(f\"ELU:        {elu_custom(test_values).round(4)}\")\n",
        "print(f\"Swish:      {swish_custom(test_values).round(4)}\")\n",
        "\n",
        "# Test edge cases\n",
        "print(\"\\n🧪 Testing edge cases:\")\n",
        "print(f\"Sigmoid(0) = {sigmoid(0)} (should be 0.5)\")\n",
        "print(f\"Sigmoid(100) = {sigmoid(100)} (should approach 1)\")\n",
        "print(f\"ReLU(-10) = {relu_custom(-10)} (should be 0)\")\n",
        "print(f\"Leaky ReLU(-1, 0.1) = {leaky_relu_custom(-1, 0.1)} (should be -0.1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2epcCCHxbd3"
      },
      "source": [
        "### Task 1B: Gradient Computation ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cphkYqhPxbd3"
      },
      "outputs": [],
      "source": [
        "def sigmoid_gradient(x):\n",
        "    \"\"\"Sigmoid derivative: σ(x) * (1 - σ(x))\n",
        "\n",
        "    Mathematical derivation:\n",
        "    d/dx[1/(1+e^(-x))] = e^(-x)/(1+e^(-x))^2 = σ(x)(1-σ(x))\n",
        "\n",
        "    Key insight: Maximum gradient is 0.25 at x=0\n",
        "    \"\"\"\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh_gradient(x):\n",
        "    \"\"\"Tanh derivative: 1 - tanh²(x)\n",
        "\n",
        "    Mathematical derivation:\n",
        "    d/dx[tanh(x)] = sech²(x) = 1 - tanh²(x)\n",
        "\n",
        "    Key insight: Maximum gradient is 1 at x=0\n",
        "    \"\"\"\n",
        "    t = tanh_custom(x)\n",
        "    return 1 - t**2\n",
        "\n",
        "def relu_gradient(x):\n",
        "    \"\"\"ReLU derivative: 1 if x>0, 0 otherwise\n",
        "\n",
        "    Key insight:\n",
        "    - Gradient is constant (1) for positive inputs\n",
        "    - No vanishing gradient problem for x>0\n",
        "    - Undefined at x=0 (we use 0 by convention)\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "def leaky_relu_gradient(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU derivative: 1 if x>0, alpha otherwise\n",
        "\n",
        "    Key insight: Always has some gradient (prevents dead neurons)\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1.0, alpha)\n",
        "\n",
        "def elu_gradient(x, alpha=1.0):\n",
        "    \"\"\"ELU derivative: 1 if x>0, alpha*e^x otherwise\n",
        "\n",
        "    Key insight: Smooth gradient transition\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1.0, alpha * np.exp(np.clip(x, -500, 500)))\n",
        "\n",
        "def swish_gradient(x, beta=1.0):\n",
        "    \"\"\"Swish derivative: sigmoid(βx) + x*β*sigmoid(βx)*(1-sigmoid(βx))\n",
        "\n",
        "    More complex but provides smooth gradients\n",
        "    \"\"\"\n",
        "    sig = sigmoid(beta * x)\n",
        "    return sig + x * beta * sig * (1 - sig)\n",
        "\n",
        "print(\"✅ All gradient functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFS4O1dSxbd3"
      },
      "source": [
        "### Test Gradient Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNSSYIpVxbd3"
      },
      "outputs": [],
      "source": [
        "# Test gradient functions\n",
        "test_vals = np.array([-2, -1, 0, 1, 2])\n",
        "\n",
        "print(\"Testing gradient functions:\")\n",
        "print(f\"Input:              {test_vals}\")\n",
        "print(f\"Sigmoid gradient:   {sigmoid_gradient(test_vals).round(4)}\")\n",
        "print(f\"Tanh gradient:      {tanh_gradient(test_vals).round(4)}\")\n",
        "print(f\"ReLU gradient:      {relu_gradient(test_vals).round(4)}\")\n",
        "print(f\"Leaky ReLU grad:    {leaky_relu_gradient(test_vals).round(4)}\")\n",
        "\n",
        "# Verify key properties\n",
        "print(\"\\n🧪 Verifying gradient properties:\")\n",
        "print(f\"Max sigmoid gradient: {sigmoid_gradient(0):.4f} (should be 0.25)\")\n",
        "print(f\"Max tanh gradient: {tanh_gradient(0):.4f} (should be 1.0)\")\n",
        "print(f\"ReLU gradient consistency: positive={relu_gradient(1)}, negative={relu_gradient(-1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOSXD0Qjxbd4"
      },
      "source": [
        "### Visualize Activations and Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crmnT0aaxbd4"
      },
      "outputs": [],
      "source": [
        "def plot_activation_and_gradient(func, grad_func, name, color='blue'):\n",
        "    \"\"\"Plot activation function and its gradient side by side\"\"\"\n",
        "    x = np.linspace(-5, 5, 200)\n",
        "    y = func(x)\n",
        "    dy = grad_func(x)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Plot activation function\n",
        "    ax1.plot(x, y, color=color, linewidth=2.5, label=f'{name}')\n",
        "    ax1.set_title(f'{name} Activation Function', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('x', fontsize=12)\n",
        "    ax1.set_ylabel('f(x)', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.axhline(y=0, color='black', linewidth=0.5)\n",
        "    ax1.axvline(x=0, color='black', linewidth=0.5)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot gradient\n",
        "    ax2.plot(x, dy, color='red', linewidth=2.5, label=f\"{name} derivative\")\n",
        "    ax2.set_title(f'{name} Gradient', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('x', fontsize=12)\n",
        "    ax2.set_ylabel(\"f'(x)\", fontsize=12)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.axhline(y=0, color='black', linewidth=0.5)\n",
        "    ax2.axvline(x=0, color='black', linewidth=0.5)\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot all activation functions and their gradients\n",
        "plot_activation_and_gradient(sigmoid, sigmoid_gradient, 'Sigmoid', 'blue')\n",
        "plot_activation_and_gradient(tanh_custom, tanh_gradient, 'Tanh', 'green')\n",
        "plot_activation_and_gradient(relu_custom, relu_gradient, 'ReLU', 'red')\n",
        "plot_activation_and_gradient(lambda x: leaky_relu_custom(x),\n",
        "                            lambda x: leaky_relu_gradient(x), 'Leaky ReLU', 'purple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5xAWNAfxbd4"
      },
      "source": [
        "### Comparative Analysis of Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4yDqtR5xbd4"
      },
      "outputs": [],
      "source": [
        "# Compare all activations on same plot\n",
        "x = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot activations\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, sigmoid(x), 'b-', linewidth=2, label='Sigmoid')\n",
        "plt.plot(x, tanh_custom(x), 'g-', linewidth=2, label='Tanh')\n",
        "plt.plot(x, relu_custom(x), 'r-', linewidth=2, label='ReLU')\n",
        "plt.plot(x, leaky_relu_custom(x), 'purple', linewidth=2, label='Leaky ReLU')\n",
        "plt.title('Activation Functions Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.axhline(y=0, color='black', linewidth=0.5)\n",
        "plt.axvline(x=0, color='black', linewidth=0.5)\n",
        "\n",
        "# Plot gradients\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, sigmoid_gradient(x), 'b-', linewidth=2, label='Sigmoid')\n",
        "plt.plot(x, tanh_gradient(x), 'g-', linewidth=2, label='Tanh')\n",
        "plt.plot(x, relu_gradient(x), 'r-', linewidth=2, label='ReLU')\n",
        "plt.plot(x, leaky_relu_gradient(x), 'purple', linewidth=2, label='Leaky ReLU')\n",
        "plt.title('Gradient Functions Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(\"f'(x)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.axhline(y=0, color='black', linewidth=0.5)\n",
        "plt.axvline(x=0, color='black', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Key Observations:\")\n",
        "print(\"1. Sigmoid & Tanh suffer from vanishing gradients (gradients → 0 for large |x|)\")\n",
        "print(\"2. ReLU has constant gradient (1) for positive inputs, 0 for negative\")\n",
        "print(\"3. Leaky ReLU prevents dead neurons with small negative gradient\")\n",
        "print(\"4. Sigmoid is not zero-centered, Tanh is zero-centered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDOhDfK-xbd4"
      },
      "source": [
        "## Part 2: Tensor Operations & Layer Construction (25 minutes)\n",
        "\n",
        "### Task 2A: Basic Tensor Operations ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgJWyOjkxbd4"
      },
      "outputs": [],
      "source": [
        "# 1. Create tensors of different dimensions\n",
        "scalar = tf.constant(5.0)\n",
        "vector = tf.constant([1, 2, 3], dtype=tf.float32)\n",
        "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32)\n",
        "\n",
        "print(\"📐 Tensor Shapes and Properties:\")\n",
        "print(f\"Scalar: shape={scalar.shape}, ndim={scalar.ndim}, value={scalar.numpy()}\")\n",
        "print(f\"Vector: shape={vector.shape}, ndim={vector.ndim}\")\n",
        "print(f\"Matrix: shape={matrix.shape}, ndim={matrix.ndim}\")\n",
        "print(f\"3D Tensor: shape={tensor_3d.shape}, ndim={tensor_3d.ndim}\")\n",
        "\n",
        "print(f\"\\n🔍 Detailed tensor information:\")\n",
        "print(f\"Vector: {vector.numpy()}\")\n",
        "print(f\"Matrix:\\n{matrix.numpy()}\")\n",
        "print(f\"3D Tensor shape explanation: {tensor_3d.shape} = (batch_size=2, height=2, width=2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOZiCg8zxbd4"
      },
      "outputs": [],
      "source": [
        "# 2. Matrix multiplication exercises - COMPLETE SOLUTIONS\n",
        "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
        "\n",
        "print(\"🔢 Matrix Operations:\")\n",
        "print(f\"Matrix A:\\n{A.numpy()}\")\n",
        "print(f\"Matrix B:\\n{B.numpy()}\")\n",
        "\n",
        "# Element-wise multiplication (Hadamard product)\n",
        "element_wise = tf.multiply(A, B)  # or A * B\n",
        "print(f\"\\nElement-wise multiplication (A ⊙ B):\\n{element_wise.numpy()}\")\n",
        "\n",
        "# Matrix multiplication (dot product)\n",
        "matrix_mult = tf.matmul(A, B)  # or A @ B\n",
        "print(f\"\\nMatrix multiplication (A @ B):\\n{matrix_mult.numpy()}\")\n",
        "\n",
        "# Manual verification of matrix multiplication\n",
        "print(f\"\\n🧮 Manual verification:\")\n",
        "print(f\"A[0,0]*B[0,0] + A[0,1]*B[1,0] = {A[0,0]*B[0,0] + A[0,1]*B[1,0]} = {matrix_mult[0,0]}\")\n",
        "print(f\"A[0,0]*B[0,1] + A[0,1]*B[1,1] = {A[0,0]*B[0,1] + A[0,1]*B[1,1]} = {matrix_mult[0,1]}\")\n",
        "\n",
        "# Other useful operations\n",
        "dot_product = tf.tensordot(A, B, axes=1)  # Tensor contraction\n",
        "print(f\"\\nTensor dot product:\\n{dot_product.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYfXCTynxbd4"
      },
      "outputs": [],
      "source": [
        "# 3. Broadcasting operations - COMPLETE SOLUTIONS\n",
        "vec = tf.constant([1, 2], dtype=tf.float32)  # Shape: (2,)\n",
        "mat = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)  # Shape: (3, 2)\n",
        "\n",
        "print(\"📡 Broadcasting Examples:\")\n",
        "print(f\"Vector shape: {vec.shape}\")\n",
        "print(f\"Matrix shape: {mat.shape}\")\n",
        "print(f\"\\nOriginal matrix:\\n{mat.numpy()}\")\n",
        "print(f\"Vector to broadcast: {vec.numpy()}\")\n",
        "\n",
        "# Broadcasting addition - adds vector to each row\n",
        "broadcasted_add = mat + vec  # Shape: (3,2) + (2,) -> (3,2)\n",
        "print(f\"\\nBroadcasted addition (mat + vec):\\n{broadcasted_add.numpy()}\")\n",
        "\n",
        "# Broadcasting multiplication\n",
        "broadcasted_mult = mat * vec\n",
        "print(f\"\\nBroadcasted multiplication (mat * vec):\\n{broadcasted_mult.numpy()}\")\n",
        "\n",
        "# Explicit reshape for clarity\n",
        "vec_reshaped = tf.reshape(vec, [1, 2])  # Shape: (1, 2)\n",
        "explicit_broadcast = mat + vec_reshaped\n",
        "print(f\"\\nExplicit reshape and broadcast:\\n{explicit_broadcast.numpy()}\")\n",
        "\n",
        "# Broadcasting with different dimensions\n",
        "col_vec = tf.constant([[1], [2], [3]], dtype=tf.float32)  # Shape: (3, 1)\n",
        "broadcast_2d = mat + col_vec  # Shape: (3,2) + (3,1) -> (3,2)\n",
        "print(f\"\\n2D Broadcasting (adding column vector):\\n{broadcast_2d.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX6AFaavxbd5"
      },
      "outputs": [],
      "source": [
        "# 4. Shape manipulation - COMPLETE SOLUTIONS\n",
        "original = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "print(f\"📏 Shape Manipulation:\")\n",
        "print(f\"Original tensor:\\n{original.numpy()}\")\n",
        "print(f\"Original shape: {original.shape}\")\n",
        "\n",
        "# Reshape to 3x2\n",
        "reshaped = tf.reshape(original, [3, 2])\n",
        "print(f\"\\nReshaped to 3x2:\\n{reshaped.numpy()}\")\n",
        "print(f\"New shape: {reshaped.shape}\")\n",
        "\n",
        "# Flatten to 1D\n",
        "flattened = tf.reshape(original, [-1])  # -1 means \"infer this dimension\"\n",
        "print(f\"\\nFlattened to 1D: {flattened.numpy()}\")\n",
        "print(f\"Flattened shape: {flattened.shape}\")\n",
        "\n",
        "# Transpose the matrix\n",
        "transposed = tf.transpose(original)\n",
        "print(f\"\\nTransposed:\\n{transposed.numpy()}\")\n",
        "print(f\"Transposed shape: {transposed.shape}\")\n",
        "\n",
        "# Advanced reshaping\n",
        "expanded = tf.expand_dims(flattened, axis=0)  # Add batch dimension\n",
        "print(f\"\\nExpanded dims (add batch): {expanded.numpy()}\")\n",
        "print(f\"Expanded shape: {expanded.shape}\")\n",
        "\n",
        "# Squeeze (remove dimensions of size 1)\n",
        "squeezed = tf.squeeze(expanded)\n",
        "print(f\"\\nSqueezed back: {squeezed.numpy()}\")\n",
        "print(f\"Squeezed shape: {squeezed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeWFX4Czxbd5"
      },
      "outputs": [],
      "source": [
        "# Advanced tensor operations\n",
        "mat_3d = tf.random.normal([2, 3, 4])  # Random 3D tensor\n",
        "\n",
        "print(\"🔬 Advanced Tensor Operations:\")\n",
        "print(f\"3D tensor shape: {mat_3d.shape}\")\n",
        "\n",
        "# Reduction operations\n",
        "sum_all = tf.reduce_sum(mat_3d)  # Sum all elements\n",
        "sum_axis0 = tf.reduce_sum(mat_3d, axis=0)  # Sum along first axis\n",
        "sum_axis1 = tf.reduce_sum(mat_3d, axis=1)  # Sum along second axis\n",
        "mean_all = tf.reduce_mean(mat_3d)\n",
        "\n",
        "print(f\"Sum of all elements: {sum_all.numpy():.2f}\")\n",
        "print(f\"Sum along axis 0 shape: {sum_axis0.shape}\")\n",
        "print(f\"Sum along axis 1 shape: {sum_axis1.shape}\")\n",
        "print(f\"Mean of all elements: {mean_all.numpy():.2f}\")\n",
        "\n",
        "# Useful for neural networks\n",
        "batch_data = tf.random.normal([32, 784])  # Simulated batch of flattened images\n",
        "batch_mean = tf.reduce_mean(batch_data, axis=0)  # Mean across batch\n",
        "batch_std = tf.math.reduce_std(batch_data, axis=0)  # Std across batch\n",
        "\n",
        "print(f\"\\n👥 Batch operations:\")\n",
        "print(f\"Batch data shape: {batch_data.shape}\")\n",
        "print(f\"Mean across batch shape: {batch_mean.shape}\")\n",
        "print(f\"Std across batch shape: {batch_std.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr3ZWcuMxbd5"
      },
      "source": [
        "### Task 2B: Dense Layer from Scratch ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbqGSOlWxbd5"
      },
      "outputs": [],
      "source": [
        "class SimpleDenseLayer:\n",
        "    \"\"\"Complete implementation of a dense layer from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation='relu',\n",
        "                 weight_init='xavier', use_bias=True):\n",
        "        \"\"\"Initialize a dense layer with weights, bias, and activation\n",
        "\n",
        "        Args:\n",
        "            input_dim: Number of input features\n",
        "            output_dim: Number of output features\n",
        "            activation: Activation function name\n",
        "            weight_init: Weight initialization strategy\n",
        "            use_bias: Whether to use bias term\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation_name = activation\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Weight initialization strategies\n",
        "        if weight_init == 'xavier':\n",
        "            # Xavier/Glorot initialization: good for sigmoid/tanh\n",
        "            limit = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "            self.weights = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
        "        elif weight_init == 'he':\n",
        "            # He initialization: good for ReLU\n",
        "            std = np.sqrt(2.0 / input_dim)\n",
        "            self.weights = np.random.normal(0, std, (input_dim, output_dim))\n",
        "        else:\n",
        "            # Default: Xavier normal\n",
        "            std = np.sqrt(2.0 / (input_dim + output_dim))\n",
        "            self.weights = np.random.normal(0, std, (input_dim, output_dim))\n",
        "\n",
        "        # Bias initialization\n",
        "        if use_bias:\n",
        "            self.bias = np.zeros((1, output_dim))\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # Activation function registry\n",
        "        self.activation_funcs = {\n",
        "            'relu': relu_custom,\n",
        "            'sigmoid': sigmoid,\n",
        "            'tanh': tanh_custom,\n",
        "            'leaky_relu': leaky_relu_custom,\n",
        "            'elu': elu_custom,\n",
        "            'swish': swish_custom,\n",
        "            'linear': lambda x: x,\n",
        "            'softmax': self._softmax\n",
        "        }\n",
        "\n",
        "        self.gradient_funcs = {\n",
        "            'relu': relu_gradient,\n",
        "            'sigmoid': sigmoid_gradient,\n",
        "            'tanh': tanh_gradient,\n",
        "            'leaky_relu': leaky_relu_gradient,\n",
        "            'elu': elu_gradient,\n",
        "            'swish': swish_gradient,\n",
        "            'linear': lambda x: np.ones_like(x)\n",
        "        }\n",
        "\n",
        "        # Store computation for backward pass\n",
        "        self.last_input = None\n",
        "        self.last_z = None\n",
        "        self.last_output = None\n",
        "\n",
        "        print(f\"✅ Dense layer created: {input_dim} -> {output_dim}, activation: {activation}\")\n",
        "        print(f\"   Weights shape: {self.weights.shape}\")\n",
        "        print(f\"   Bias shape: {self.bias.shape if self.bias is not None else 'None'}\")\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        \"\"\"Numerically stable softmax implementation\"\"\"\n",
        "        # Subtract max for numerical stability\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: output = activation(inputs @ weights + bias)\"\"\"\n",
        "        # Store for backward pass\n",
        "        self.last_input = inputs.copy()\n",
        "\n",
        "        # Linear transformation: z = X @ W + b\n",
        "        self.last_z = np.dot(inputs, self.weights)\n",
        "        if self.use_bias:\n",
        "            self.last_z += self.bias\n",
        "\n",
        "        # Apply activation\n",
        "        activation_func = self.activation_funcs.get(self.activation_name, lambda x: x)\n",
        "        self.last_output = activation_func(self.last_z)\n",
        "\n",
        "        return self.last_output\n",
        "\n",
        "    def backward(self, grad_output, learning_rate=0.01):\n",
        "        \"\"\"Backward pass for gradient computation and weight updates\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient from the next layer\n",
        "            learning_rate: Learning rate for weight updates\n",
        "\n",
        "        Returns:\n",
        "            grad_input: Gradient w.r.t input (for previous layer)\n",
        "        \"\"\"\n",
        "        batch_size = self.last_input.shape[0]\n",
        "\n",
        "        # Gradient of activation function\n",
        "        grad_func = self.gradient_funcs.get(self.activation_name)\n",
        "        if grad_func and self.activation_name != 'softmax':\n",
        "            grad_activation = grad_func(self.last_z)\n",
        "            grad_z = grad_output * grad_activation\n",
        "        else:\n",
        "            # For softmax or unknown activations, use grad_output directly\n",
        "            grad_z = grad_output\n",
        "\n",
        "        # Gradients w.r.t parameters\n",
        "        grad_weights = np.dot(self.last_input.T, grad_z) / batch_size\n",
        "        grad_bias = np.sum(grad_z, axis=0, keepdims=True) / batch_size if self.use_bias else None\n",
        "\n",
        "        # Gradient w.r.t input (for backpropagation to previous layer)\n",
        "        grad_input = np.dot(grad_z, self.weights.T)\n",
        "\n",
        "        # Update parameters (simple SGD)\n",
        "        self.weights -= learning_rate * grad_weights\n",
        "        if self.use_bias:\n",
        "            self.bias -= learning_rate * grad_bias\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"Get layer parameters\"\"\"\n",
        "        if self.use_bias:\n",
        "            return {'weights': self.weights, 'bias': self.bias}\n",
        "        else:\n",
        "            return {'weights': self.weights}\n",
        "\n",
        "    def set_params(self, params):\n",
        "        \"\"\"Set layer parameters\"\"\"\n",
        "        self.weights = params['weights']\n",
        "        if self.use_bias and 'bias' in params:\n",
        "            self.bias = params['bias']\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"Make layer callable\"\"\"\n",
        "        return self.forward(inputs)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SimpleDenseLayer({self.input_dim}, {self.output_dim}, activation='{self.activation_name}')\"\n",
        "\n",
        "print(\"✅ SimpleDenseLayer class implemented successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBAYdLqIxbd5"
      },
      "outputs": [],
      "source": [
        "# Test the layer with different configurations\n",
        "print(\"🧪 Testing SimpleDenseLayer:\")\n",
        "\n",
        "# Test 1: Basic layer\n",
        "layer1 = SimpleDenseLayer(input_dim=784, output_dim=128, activation='relu')\n",
        "test_input = np.random.randn(32, 784)  # Batch of 32 samples\n",
        "output1 = layer1(test_input)\n",
        "\n",
        "print(f\"\\nTest 1 - Basic layer:\")\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output1.shape}\")\n",
        "print(f\"Output statistics: mean={output1.mean():.3f}, std={output1.std():.3f}\")\n",
        "print(f\"ReLU property (all non-negative): {np.all(output1 >= 0)}\")\n",
        "\n",
        "# Test 2: Sigmoid layer\n",
        "layer2 = SimpleDenseLayer(input_dim=128, output_dim=64, activation='sigmoid')\n",
        "output2 = layer2(output1)\n",
        "\n",
        "print(f\"\\nTest 2 - Sigmoid layer:\")\n",
        "print(f\"Input shape: {output1.shape}\")\n",
        "print(f\"Output shape: {output2.shape}\")\n",
        "print(f\"Output range: [{output2.min():.3f}, {output2.max():.3f}] (should be in [0,1])\")\n",
        "\n",
        "# Test 3: Softmax output layer\n",
        "layer3 = SimpleDenseLayer(input_dim=64, output_dim=10, activation='softmax')\n",
        "output3 = layer3(output2)\n",
        "\n",
        "print(f\"\\nTest 3 - Softmax layer:\")\n",
        "print(f\"Input shape: {output2.shape}\")\n",
        "print(f\"Output shape: {output3.shape}\")\n",
        "print(f\"Softmax property (sums to 1): {np.allclose(output3.sum(axis=1), 1.0)}\")\n",
        "print(f\"Sample probabilities: {output3[0][:5]}\")\n",
        "print(f\"Sample sum: {output3[0].sum():.6f}\")\n",
        "\n",
        "# Test 4: Different initializations\n",
        "print(f\"\\n🎯 Testing weight initializations:\")\n",
        "layer_xavier = SimpleDenseLayer(100, 50, weight_init='xavier')\n",
        "layer_he = SimpleDenseLayer(100, 50, weight_init='he')\n",
        "\n",
        "print(f\"Xavier init - Weight std: {layer_xavier.weights.std():.4f}\")\n",
        "print(f\"He init - Weight std: {layer_he.weights.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvjq9S2Zxbd5"
      },
      "source": [
        "## Part 3: Complete Neural Network Construction (10 minutes)\n",
        "\n",
        "### Task 3: Build Multi-Layer Network ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZR_VycKxbd5"
      },
      "outputs": [],
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"Complete neural network implementation from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, activations, weight_init='xavier', use_bias=True):\n",
        "        \"\"\"Build a multi-layer neural network\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]\n",
        "            activations: List of activation functions for each layer\n",
        "            weight_init: Weight initialization strategy\n",
        "            use_bias: Whether to use bias terms\n",
        "        \"\"\"\n",
        "        assert len(layer_sizes) >= 2, \"Need at least input and output dimensions\"\n",
        "        assert len(activations) == len(layer_sizes) - 1, \"Need activation for each layer\"\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activations = activations\n",
        "        self.num_layers = len(layer_sizes) - 1\n",
        "\n",
        "        # Create layers\n",
        "        self.layers = []\n",
        "        for i in range(self.num_layers):\n",
        "            layer = SimpleDenseLayer(\n",
        "                input_dim=layer_sizes[i],\n",
        "                output_dim=layer_sizes[i+1],\n",
        "                activation=activations[i],\n",
        "                weight_init=weight_init,\n",
        "                use_bias=use_bias\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Store for training\n",
        "        self.training_history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        print(f\"\\n🏗️ Neural Network Architecture:\")\n",
        "        for i, (size, activation) in enumerate(zip(layer_sizes[1:], activations)):\n",
        "            print(f\"   Layer {i+1}: {layer_sizes[i]} -> {size} ({activation})\")\n",
        "\n",
        "        total_params = sum(layer.weights.size +\n",
        "                          (layer.bias.size if layer.use_bias else 0)\n",
        "                          for layer in self.layers)\n",
        "        print(f\"   Total parameters: {total_params:,}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through all layers\"\"\"\n",
        "        current_input = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            current_input = layer.forward(current_input)\n",
        "        return current_input\n",
        "\n",
        "    def backward(self, y_true, y_pred, learning_rate=0.01):\n",
        "        \"\"\"Backward pass through all layers\n",
        "\n",
        "        Args:\n",
        "            y_true: True labels\n",
        "            y_pred: Predicted values\n",
        "            learning_rate: Learning rate for updates\n",
        "        \"\"\"\n",
        "        # Compute loss gradient (for softmax + cross-entropy)\n",
        "        batch_size = y_true.shape[0]\n",
        "        grad_output = (y_pred - y_true) / batch_size\n",
        "\n",
        "        # Backpropagate through layers (reverse order)\n",
        "        current_grad = grad_output\n",
        "        for layer in reversed(self.layers):\n",
        "            current_grad = layer.backward(current_grad, learning_rate)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "    def predict_classes(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        probs = self.predict(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred, loss_type='categorical_crossentropy'):\n",
        "        \"\"\"Compute loss\"\"\"\n",
        "        if loss_type == 'categorical_crossentropy':\n",
        "            # Avoid log(0) by adding small epsilon\n",
        "            epsilon = 1e-15\n",
        "            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "            return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "        elif loss_type == 'mse':\n",
        "            return np.mean((y_true - y_pred) ** 2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Compute accuracy for classification\"\"\"\n",
        "        y_true_classes = np.argmax(y_true, axis=1)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        return np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    def fit(self, X, y, epochs=100, learning_rate=0.01, batch_size=32, verbose=True):\n",
        "        \"\"\"Train the network\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "            y: True labels (one-hot encoded)\n",
        "            epochs: Number of training epochs\n",
        "            learning_rate: Learning rate\n",
        "            batch_size: Batch size for mini-batch training\n",
        "            verbose: Whether to print progress\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            epoch_acc = 0\n",
        "\n",
        "            # Mini-batch training\n",
        "            for batch in range(n_batches):\n",
        "                start_idx = batch * batch_size\n",
        "                end_idx = min(start_idx + batch_size, n_samples)\n",
        "\n",
        "                X_batch = X_shuffled[start_idx:end_idx]\n",
        "                y_batch = y_shuffled[start_idx:end_idx]\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(X_batch)\n",
        "\n",
        "                # Compute loss and accuracy\n",
        "                batch_loss = self.compute_loss(y_batch, y_pred)\n",
        "                batch_acc = self.compute_accuracy(y_batch, y_pred)\n",
        "\n",
        "                epoch_loss += batch_loss\n",
        "                epoch_acc += batch_acc\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(y_batch, y_pred, learning_rate)\n",
        "\n",
        "            # Average over batches\n",
        "            epoch_loss /= n_batches\n",
        "            epoch_acc /= n_batches\n",
        "\n",
        "            # Store history\n",
        "            self.training_history['loss'].append(epoch_loss)\n",
        "            self.training_history['accuracy'].append(epoch_acc)\n",
        "\n",
        "            # Print progress\n",
        "            if verbose and (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training loss and accuracy\"\"\"\n",
        "        if not self.training_history['loss']:\n",
        "            print(\"No training history to plot. Train the model first.\")\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Plot loss\n",
        "        ax1.plot(self.training_history['loss'], 'b-', linewidth=2)\n",
        "        ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax2.plot(self.training_history['accuracy'], 'g-', linewidth=2)\n",
        "        ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SimpleNeuralNetwork({self.layer_sizes}, {self.activations})\"\n",
        "\n",
        "print(\"✅ SimpleNeuralNetwork class implemented successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xZlpQirxbd6"
      },
      "outputs": [],
      "source": [
        "# Build and test the network\n",
        "print(\"🧪 Testing SimpleNeuralNetwork:\")\n",
        "\n",
        "# Create a network for MNIST-like classification\n",
        "network = SimpleNeuralNetwork(\n",
        "    layer_sizes=[784, 128, 64, 10],\n",
        "    activations=['relu', 'relu', 'softmax'],\n",
        "    weight_init='he'  # He initialization works well with ReLU\n",
        ")\n",
        "\n",
        "# Test with dummy MNIST-like data\n",
        "batch_size = 32\n",
        "test_data = np.random.randn(batch_size, 784)\n",
        "output = network(test_data)\n",
        "\n",
        "print(f\"\\n🔍 Network Testing:\")\n",
        "print(f\"Input shape: {test_data.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output is probability distribution: {np.allclose(output.sum(axis=1), 1.0)}\")\n",
        "print(f\"Sample predictions (first 3 samples):\")\n",
        "for i in range(3):\n",
        "    pred_class = np.argmax(output[i])\n",
        "    confidence = output[i][pred_class]\n",
        "    print(f\"  Sample {i+1}: Class {pred_class}, Confidence: {confidence:.3f}\")\n",
        "\n",
        "# Test different architectures\n",
        "print(f\"\\n🏗️ Testing different architectures:\")\n",
        "\n",
        "# Small network\n",
        "small_net = SimpleNeuralNetwork([10, 5, 2], ['tanh', 'sigmoid'])\n",
        "small_input = np.random.randn(5, 10)\n",
        "small_output = small_net(small_input)\n",
        "print(f\"Small network output range: [{small_output.min():.3f}, {small_output.max():.3f}]\")\n",
        "\n",
        "# Deep network\n",
        "deep_net = SimpleNeuralNetwork([100, 64, 32, 16, 8, 3],\n",
        "                              ['relu', 'relu', 'relu', 'relu', 'softmax'])\n",
        "deep_input = np.random.randn(10, 100)\n",
        "deep_output = deep_net(deep_input)\n",
        "print(f\"Deep network maintains probability distribution: {np.allclose(deep_output.sum(axis=1), 1.0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7r0-L0Gxbd6"
      },
      "source": [
        "### Compare with TensorFlow/Keras Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbTwECFTxbd6"
      },
      "outputs": [],
      "source": [
        "# Build equivalent Keras model for comparison\n",
        "def build_keras_equivalent():\n",
        "    \"\"\"Build equivalent network using Keras\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "keras_model = build_keras_equivalent()\n",
        "keras_output = keras_model(test_data)\n",
        "\n",
        "print(\"🆚 Comparison with Keras:\")\n",
        "print(f\"\\nKeras model output shape: {keras_output.shape}\")\n",
        "print(f\"Keras model parameter count: {keras_model.count_params():,}\")\n",
        "\n",
        "# Calculate our model's parameter count\n",
        "our_params = sum(layer.weights.size +\n",
        "                (layer.bias.size if layer.use_bias else 0)\n",
        "                for layer in network.layers)\n",
        "print(f\"Our model parameter count: {our_params:,}\")\n",
        "print(f\"Parameter count match: {our_params == keras_model.count_params()}\")\n",
        "\n",
        "# Display detailed model architecture comparison\n",
        "print(f\"\\n📊 Detailed Architecture Comparison:\")\n",
        "print(f\"\\nOur Model:\")\n",
        "for i, layer in enumerate(network.layers):\n",
        "    params = layer.weights.size + (layer.bias.size if layer.use_bias else 0)\n",
        "    print(f\"  Layer {i+1}: {layer.input_dim} -> {layer.output_dim} ({layer.activation_name}) | {params:,} params\")\n",
        "\n",
        "print(f\"\\nKeras Model:\")\n",
        "keras_model.summary()\n",
        "\n",
        "# Compare outputs (they will be different due to different weight initialization)\n",
        "print(f\"\\n🔍 Output Comparison:\")\n",
        "print(f\"Our model - first sample probabilities: {network(test_data[:1])[0][:5]}\")\n",
        "print(f\"Keras model - first sample probabilities: {keras_model(test_data[:1]).numpy()[0][:5]}\")\n",
        "print(f\"Note: Outputs differ due to different weight initialization, but shapes and properties match!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQhOLQMhxbd6"
      },
      "source": [
        "## Comprehensive Unit Tests ✅ COMPLETE SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fILoHybsxbd6"
      },
      "outputs": [],
      "source": [
        "def run_comprehensive_unit_tests():\n",
        "    \"\"\"Comprehensive testing suite for all implementations\"\"\"\n",
        "    print(\"🧪 Running Comprehensive Unit Tests...\\n\")\n",
        "\n",
        "    tests_passed = 0\n",
        "    tests_total = 0\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST SECTION 1: ACTIVATION FUNCTIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test 1: Activation functions\n",
        "    try:\n",
        "        tests_total += 6\n",
        "\n",
        "        # Sigmoid tests\n",
        "        assert abs(sigmoid(0) - 0.5) < 1e-6, \"Sigmoid(0) should be 0.5\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Sigmoid(0) = 0.5\")\n",
        "\n",
        "        assert sigmoid(100) > 0.99, \"Sigmoid(100) should approach 1\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Sigmoid saturation behavior\")\n",
        "\n",
        "        # ReLU tests\n",
        "        assert relu_custom(-1) == 0 and relu_custom(1) == 1, \"ReLU test failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ ReLU basic functionality\")\n",
        "\n",
        "        # Leaky ReLU test\n",
        "        assert abs(leaky_relu_custom(-1, 0.01) - (-0.01)) < 1e-6, \"Leaky ReLU test failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Leaky ReLU negative slope\")\n",
        "\n",
        "        # Tanh test\n",
        "        assert abs(tanh_custom(0)) < 1e-6, \"Tanh(0) should be 0\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Tanh zero-centered\")\n",
        "\n",
        "        # Array input test\n",
        "        test_array = np.array([-1, 0, 1])\n",
        "        sigmoid_array = sigmoid(test_array)\n",
        "        assert sigmoid_array.shape == test_array.shape, \"Sigmoid should preserve array shape\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Array input handling\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Activation function test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Activation functions error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST SECTION 2: GRADIENT FUNCTIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test 2: Gradient functions\n",
        "    try:\n",
        "        tests_total += 4\n",
        "\n",
        "        # Sigmoid gradient\n",
        "        assert abs(sigmoid_gradient(0) - 0.25) < 1e-6, \"Sigmoid gradient at 0 should be 0.25\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Sigmoid gradient maximum\")\n",
        "\n",
        "        # Tanh gradient\n",
        "        assert abs(tanh_gradient(0) - 1.0) < 1e-6, \"Tanh gradient at 0 should be 1\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Tanh gradient maximum\")\n",
        "\n",
        "        # ReLU gradient\n",
        "        assert relu_gradient(1) == 1 and relu_gradient(-1) == 0, \"ReLU gradient test failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ ReLU gradient step function\")\n",
        "\n",
        "        # Gradient array handling\n",
        "        test_vals = np.array([-1, 0, 1])\n",
        "        grad_vals = sigmoid_gradient(test_vals)\n",
        "        assert grad_vals.shape == test_vals.shape, \"Gradient should preserve shape\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Gradient array handling\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Gradient function test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gradient functions error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST SECTION 3: TENSOR OPERATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test 3: Tensor operations\n",
        "    try:\n",
        "        tests_total += 5\n",
        "\n",
        "        A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "        B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
        "        C = tf.matmul(A, B)\n",
        "        expected = tf.constant([[19, 22], [43, 50]], dtype=tf.float32)\n",
        "\n",
        "        assert tf.reduce_all(tf.equal(C, expected)), \"Matrix multiplication failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Matrix multiplication\")\n",
        "\n",
        "        # Broadcasting test\n",
        "        vec = tf.constant([1, 2], dtype=tf.float32)\n",
        "        mat = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "        broadcast_result = mat + vec\n",
        "        expected_broadcast = tf.constant([[2, 4], [4, 6]], dtype=tf.float32)\n",
        "\n",
        "        assert tf.reduce_all(tf.equal(broadcast_result, expected_broadcast)), \"Broadcasting failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Broadcasting operations\")\n",
        "\n",
        "        # Shape manipulation\n",
        "        original = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "        reshaped = tf.reshape(original, [3, 2])\n",
        "        assert reshaped.shape == [3, 2], \"Reshape failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Tensor reshaping\")\n",
        "\n",
        "        flattened = tf.reshape(original, [-1])\n",
        "        assert flattened.shape == [6], \"Flatten failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Tensor flattening\")\n",
        "\n",
        "        transposed = tf.transpose(original)\n",
        "        assert transposed.shape == [3, 2], \"Transpose failed\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Tensor transpose\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Tensor operation test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Tensor operations error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST SECTION 4: LAYER CONSTRUCTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test 4: Layer construction\n",
        "    try:\n",
        "        tests_total += 6\n",
        "\n",
        "        layer = SimpleDenseLayer(10, 5, activation='relu')\n",
        "        test_input = np.random.randn(2, 10)\n",
        "        output = layer(test_input)\n",
        "\n",
        "        assert output.shape == (2, 5), f\"Layer output shape mismatch: {output.shape}\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Layer output shape\")\n",
        "\n",
        "        assert np.all(output >= 0), \"ReLU layer should have non-negative outputs\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ ReLU non-negativity\")\n",
        "\n",
        "        # Test different activations\n",
        "        sigmoid_layer = SimpleDenseLayer(5, 3, activation='sigmoid')\n",
        "        sigmoid_output = sigmoid_layer(np.random.randn(4, 5))\n",
        "        assert np.all((sigmoid_output >= 0) & (sigmoid_output <= 1)), \"Sigmoid output should be in [0,1]\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Sigmoid output range\")\n",
        "\n",
        "        # Test softmax layer\n",
        "        softmax_layer = SimpleDenseLayer(8, 3, activation='softmax')\n",
        "        softmax_output = softmax_layer(np.random.randn(2, 8))\n",
        "        assert np.allclose(softmax_output.sum(axis=1), 1.0), \"Softmax outputs should sum to 1\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Softmax probability distribution\")\n",
        "\n",
        "        # Test weight initialization\n",
        "        he_layer = SimpleDenseLayer(100, 50, weight_init='he')\n",
        "        xavier_layer = SimpleDenseLayer(100, 50, weight_init='xavier')\n",
        "        assert he_layer.weights.shape == (100, 50), \"Weight shape incorrect\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Weight initialization\")\n",
        "\n",
        "        # Test bias handling\n",
        "        no_bias_layer = SimpleDenseLayer(5, 3, use_bias=False)\n",
        "        assert no_bias_layer.bias is None, \"No-bias layer should have None bias\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Bias handling\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Layer construction test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Layer construction error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST SECTION 5: COMPLETE NETWORK\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Test 5: Complete network\n",
        "    try:\n",
        "        tests_total += 5\n",
        "\n",
        "        net = SimpleNeuralNetwork([10, 8, 4, 2], ['relu', 'relu', 'softmax'])\n",
        "        test_input = np.random.randn(3, 10)\n",
        "        output = net(test_input)\n",
        "\n",
        "        assert output.shape == (3, 2), f\"Network output shape mismatch: {output.shape}\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Network output shape\")\n",
        "\n",
        "        assert np.allclose(output.sum(axis=1), 1.0, rtol=1e-5), \"Softmax outputs should sum to 1\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Network probability distribution\")\n",
        "\n",
        "        # Test network with different architecture\n",
        "        deep_net = SimpleNeuralNetwork([5, 4, 3, 2, 1], ['tanh', 'relu', 'sigmoid', 'linear'])\n",
        "        deep_output = deep_net(np.random.randn(2, 5))\n",
        "        assert deep_output.shape == (2, 1), \"Deep network output shape incorrect\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Deep network architecture\")\n",
        "\n",
        "        # Test prediction methods\n",
        "        predictions = net.predict(test_input)\n",
        "        assert predictions.shape == output.shape, \"Predict method inconsistent\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Prediction methods\")\n",
        "\n",
        "        class_preds = net.predict_classes(test_input)\n",
        "        assert class_preds.shape == (3,), \"Class predictions shape incorrect\"\n",
        "        tests_passed += 1\n",
        "        print(\"✅ Class prediction method\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Complete network test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Complete network error: {e}\")\n",
        "\n",
        "    # Final results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL TEST RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Tests passed: {tests_passed}/{tests_total}\")\n",
        "    print(f\"Success rate: {tests_passed/tests_total*100:.1f}%\")\n",
        "\n",
        "    if tests_passed == tests_total:\n",
        "        print(\"\\n🎉 🎉 🎉 ALL TESTS PASSED SUCCESSFULLY! 🎉 🎉 🎉\")\n",
        "        print(\"🏆 Congratulations! Your implementation is complete and correct!\")\n",
        "        print(\"🚀 You're ready for Module 2: Optimization Algorithms!\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  {tests_total - tests_passed} tests still need attention.\")\n",
        "        print(\"🔧 Review the failed tests and debug your implementations.\")\n",
        "        print(\"💪 Keep going - you're making great progress!\")\n",
        "\n",
        "    return tests_passed, tests_total\n",
        "\n",
        "# Run all tests\n",
        "passed, total = run_comprehensive_unit_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1U09iNWxbd6"
      },
      "source": [
        "## Bonus: Mini Training Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzlYdL7axbd6"
      },
      "outputs": [],
      "source": [
        "# Create synthetic data for a simple classification problem\n",
        "def create_synthetic_data(n_samples=1000, n_features=20, n_classes=3):\n",
        "    \"\"\"Create synthetic classification data\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate random features\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "    # Create class-dependent patterns\n",
        "    y_classes = np.random.randint(0, n_classes, n_samples)\n",
        "\n",
        "    # Add class-specific biases to make problem learnable\n",
        "    for i in range(n_classes):\n",
        "        mask = y_classes == i\n",
        "        X[mask, :5] += i * 2  # First 5 features are class-dependent\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    y_onehot = np.zeros((n_samples, n_classes))\n",
        "    y_onehot[np.arange(n_samples), y_classes] = 1\n",
        "\n",
        "    return X, y_onehot, y_classes\n",
        "\n",
        "# Create data\n",
        "X_train, y_train, y_train_classes = create_synthetic_data(800, 20, 3)\n",
        "X_test, y_test, y_test_classes = create_synthetic_data(200, 20, 3)\n",
        "\n",
        "print(\"🎯 Training a Neural Network on Synthetic Data\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Features: {X_train.shape[1]}\")\n",
        "print(f\"Classes: {y_train.shape[1]}\")\n",
        "\n",
        "# Create and train network\n",
        "classifier = SimpleNeuralNetwork(\n",
        "    layer_sizes=[20, 32, 16, 3],\n",
        "    activations=['relu', 'relu', 'softmax'],\n",
        "    weight_init='he'\n",
        ")\n",
        "\n",
        "print(\"\\n🏋️ Training the network...\")\n",
        "classifier.fit(X_train, y_train, epochs=50, learning_rate=0.01, batch_size=32, verbose=True)\n",
        "\n",
        "# Test the trained network\n",
        "test_predictions = classifier.predict(X_test)\n",
        "test_accuracy = classifier.compute_accuracy(y_test, test_predictions)\n",
        "\n",
        "print(f\"\\n📊 Final Results:\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
        "\n",
        "# Plot training history\n",
        "classifier.plot_training_history()\n",
        "\n",
        "print(\"\\n🎉 Training demonstration complete!\")\n",
        "print(\"This shows that your implementation can actually learn from data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGJTl5wVxbd6"
      },
      "source": [
        "## Summary and Key Insights 🎯\n",
        "\n",
        "### What We Accomplished Today:\n",
        "\n",
        "#### ✅ **Part 1: Activation Functions Mastery**\n",
        "- Implemented 6 activation functions from mathematical principles\n",
        "- Computed gradients for backpropagation\n",
        "- Visualized functions and their derivatives\n",
        "- **Key Insight**: ReLU prevents vanishing gradients, Sigmoid/Tanh suffer from it\n",
        "\n",
        "#### ✅ **Part 2: Tensor Operations Expertise**\n",
        "- Mastered TensorFlow tensor manipulations\n",
        "- Understood broadcasting and shape operations\n",
        "- Built complete dense layer from scratch\n",
        "- **Key Insight**: Understanding tensors is crucial for neural network implementation\n",
        "\n",
        "#### ✅ **Part 3: Neural Network Construction**\n",
        "- Created complete multi-layer network class\n",
        "- Implemented forward and backward passes\n",
        "- Added training capabilities with mini-batch SGD\n",
        "- **Key Insight**: Networks are just compositions of mathematical operations\n",
        "\n",
        "### 🧠 **Critical Understanding Gained:**\n",
        "\n",
        "1. **Mathematical Foundation**: Every activation function has specific properties that affect learning\n",
        "2. **Implementation Skills**: Can build neural networks from mathematical first principles\n",
        "3. **Framework Knowledge**: Understand what TensorFlow/Keras do \"under the hood\"\n",
        "4. **Debugging Ability**: Can identify and fix issues in neural network code\n",
        "\n",
        "### 🚀 **Preparation for Module 2:**\n",
        "- ✅ Gradient computation ready for optimization algorithms\n",
        "- ✅ Layer implementation ready for advanced architectures\n",
        "- ✅ Understanding of mathematical foundations\n",
        "- ✅ Practical coding skills for complex implementations\n",
        "\n",
        "### 📈 **Course Progress:**\n",
        "- **CO-1 Achievement**: 40% complete (can create and explain neural networks)\n",
        "- **CO-2 Achievement**: 25% complete (can build multi-layer networks)\n",
        "- **Unit Test 1 Readiness**: Fully prepared for assessment\n",
        "\n",
        "**🏆 Congratulations! You've mastered the fundamentals of neural network implementation!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}