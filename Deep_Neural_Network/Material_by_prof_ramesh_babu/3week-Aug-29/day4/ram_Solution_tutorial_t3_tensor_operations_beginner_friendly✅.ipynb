{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T3: Understanding Tensor Operations & Neural Networks\n",
    "## Week 3, Day 4 - Deep Neural Network Architectures (Beginner-Friendly Version)\n",
    "\n",
    "**üëã Welcome!** This tutorial is specifically designed for students from **ECE, Mechanical, and other non-CS backgrounds**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Student Information\n",
    "**Please fill in your details before starting:**\n",
    "\n",
    "| Field | Details |\n",
    "|-------|---------|\n",
    "| **Student Name** | `[ENTER YOUR NAME HERE]` |\n",
    "| **Registration Number** | `[ENTER YOUR REG NO HERE]` |\n",
    "| **Branch & Year** | `[e.g., M.Tech ECE/Mechanical - 1st Year]` |\n",
    "| **Date of Submission** | `[ENTER DATE HERE]` |\n",
    "| **Lab Session** | `Week 3, Day 4 - Tutorial T3 (Beginner)` |\n",
    "\n",
    "---\n",
    "\n",
    "**Duration:** 1 Hour | **Format:** Step-by-Step Guided Tutorial\n",
    "\n",
    "### üéØ Learning Objectives (Simplified)\n",
    "By the end of this tutorial, you will:\n",
    "1. **Understand** what activation functions do (like switches in circuits)\n",
    "2. **Visualize** how these functions transform signals\n",
    "3. **Experience** basic tensor operations (like matrix math you know)\n",
    "4. **Build** a simple neural network layer step-by-step\n",
    "5. **Connect** these concepts to your engineering background\n",
    "\n",
    "### üåü Why This Matters for Your Field:\n",
    "- **ECE Students**: Neural networks process signals just like filters and amplifiers\n",
    "- **Mechanical Students**: Think of neural networks as control systems with adaptive parameters\n",
    "- **All Engineers**: These are mathematical tools for pattern recognition and decision making\n",
    "\n",
    "### üìö Prerequisites:\n",
    "- Basic Python (variables, functions, arrays)\n",
    "- Matrix operations (you learned this in linear algebra)\n",
    "- Mathematical functions (exponentials, trigonometry)\n",
    "\n",
    "### üí° Learning Strategy:\n",
    "1. **Understand the concept first** - We'll explain WHY before HOW\n",
    "2. **See it visually** - Lots of plots and diagrams\n",
    "3. **Try simple examples** - Small, manageable code pieces\n",
    "4. **Build gradually** - From simple to complex\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports\n",
    "\n",
    "Let's start by importing the tools we need. Don't worry about understanding every import - think of these as loading your toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our mathematical tools\n",
    "import numpy as np                # For numerical computations (like MATLAB)\n",
    "import matplotlib.pyplot as plt   # For plotting (like MATLAB plots)\n",
    "import tensorflow as tf           # For neural network operations\n",
    "\n",
    "# Make our plots look nice\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seeds so we get consistent results\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"üîß Environment Setup Complete!\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__} (like MATLAB for Python)\")\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__} (for neural networks)\")\n",
    "print(\"\\nüéØ Ready to learn! Let's start with the basics...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Activation Functions (20 minutes)\n",
    "\n",
    "### ü§î What Are Activation Functions?\n",
    "\n",
    "Think of activation functions as **smart switches** or **signal processors**:\n",
    "\n",
    "- **For ECE Students**: Like op-amps, filters, or signal conditioners that transform input signals\n",
    "- **For Mechanical Students**: Like control valves that regulate flow based on input pressure\n",
    "- **For Everyone**: Mathematical functions that decide \"how much\" a neuron should activate\n",
    "\n",
    "### üéØ Why Do We Need Them?\n",
    "Without activation functions, neural networks would just be linear equations (boring!). Activation functions add **non-linearity**, making networks capable of learning complex patterns.\n",
    "\n",
    "Let's explore the most common ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A: The Sigmoid Function üìà\n",
    "\n",
    "**Concept**: Sigmoid is like a **soft switch** that gradually turns on/off.\n",
    "\n",
    "**Mathematical Formula**: œÉ(x) = 1/(1+e^(-x))\n",
    "\n",
    "**Real-world Analogy**: \n",
    "- **ECE**: Like a soft-limiting amplifier or a smooth rectifier\n",
    "- **Mechanical**: Like a pressure relief valve that gradually opens\n",
    "\n",
    "**Properties**:\n",
    "- Input: Any real number (-‚àû to +‚àû)\n",
    "- Output: Always between 0 and 1 (like a probability)\n",
    "- Smooth S-shaped curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the sigmoid function step by step\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \n",
    "    Think of this as a smooth switch:\n",
    "    - Large negative x ‚Üí output close to 0 (switch OFF)\n",
    "    - x = 0 ‚Üí output = 0.5 (halfway)\n",
    "    - Large positive x ‚Üí output close to 1 (switch ON)\n",
    "    \"\"\"\n",
    "    # TODO: Implement sigmoid function\n",
    "    # Formula: 1 / (1 + e^(-x))\n",
    "    # Hint: Use np.exp() for exponential\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    return 1 / (1 + np.exp(-x))  # <-- Fill this in\n",
    "\n",
    "# Let's test it with some simple values\n",
    "test_values = [-5, -2, -1, 0, 1, 2, 5]\n",
    "print(\"üß™ Testing Sigmoid Function:\")\n",
    "print(\"Input  ‚Üí Output\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "for x in test_values:\n",
    "    result = sigmoid_function(x)\n",
    "    print(f\"{x:3d}    ‚Üí {result:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Notice how:\")\n",
    "print(\"   ‚Ä¢ Negative inputs give outputs close to 0\")\n",
    "print(\"   ‚Ä¢ Zero input gives exactly 0.5\")\n",
    "print(\"   ‚Ä¢ Positive inputs give outputs close to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the sigmoid function\n",
    "\n",
    "# Create a range of x values\n",
    "x = np.linspace(-6, 6, 100)  # 100 points from -6 to 6\n",
    "y = sigmoid_function(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the sigmoid function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=3, label='Sigmoid Function')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='y=0.5 (threshold)')\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output œÉ(x)', fontsize=12)\n",
    "plt.title('Sigmoid Function: Smooth Switch', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show some key points\n",
    "key_points_x = [-2, 0, 2]\n",
    "key_points_y = [sigmoid_function(xi) for xi in key_points_x]\n",
    "plt.plot(key_points_x, key_points_y, 'ro', markersize=8)\n",
    "\n",
    "# Add annotations\n",
    "for i, (xi, yi) in enumerate(zip(key_points_x, key_points_y)):\n",
    "    plt.annotate(f'({xi}, {yi:.2f})', (xi, yi), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Create an analogy plot for ECE students\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y, 'g-', linewidth=3, label='Signal Output')\n",
    "plt.fill_between(x, 0, y, alpha=0.3, color='green')\n",
    "plt.xlabel('Input Signal Strength', fontsize=12)\n",
    "plt.ylabel('Amplifier Output', fontsize=12)\n",
    "plt.title('ECE Analogy: Soft-Limiting Amplifier', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Insights:\")\n",
    "print(\"   ‚Ä¢ The sigmoid 'squashes' any input to between 0 and 1\")\n",
    "print(\"   ‚Ä¢ It's smooth and differentiable everywhere (good for learning)\")\n",
    "print(\"   ‚Ä¢ Acts like a soft switch - gradual transition, not abrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Concept Check: Sigmoid\n",
    "\n",
    "Before moving on, let's make sure you understand:\n",
    "\n",
    "1. **What happens when x = 0?** (Answer: sigmoid(0) = 0.5)\n",
    "2. **What happens with very large positive x?** (Answer: sigmoid approaches 1)\n",
    "3. **What happens with very large negative x?** (Answer: sigmoid approaches 0)\n",
    "4. **Why is this useful in neural networks?** (Answer: It gives a probability-like output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: The ReLU Function ‚ö°\n",
    "\n",
    "**Concept**: ReLU (Rectified Linear Unit) is like a **one-way valve** or **diode**.\n",
    "\n",
    "**Mathematical Formula**: f(x) = max(0, x)\n",
    "\n",
    "**Real-world Analogy**:\n",
    "- **ECE**: Like a perfect diode that blocks negative voltages, passes positive ones\n",
    "- **Mechanical**: Like a check valve that only allows flow in one direction\n",
    "\n",
    "**Properties**:\n",
    "- Input: Any real number\n",
    "- Output: 0 for negative inputs, x for positive inputs\n",
    "- Simple and fast to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(x):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function\n",
    "    \n",
    "    Think of this as a one-way valve:\n",
    "    - Negative x ‚Üí output = 0 (valve closed)\n",
    "    - Positive x ‚Üí output = x (valve open, signal passes through)\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU function\n",
    "    # Formula: max(0, x)\n",
    "    # Hint: Use np.maximum(0, x) for element-wise maximum\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    return np.maximum(0, x)  # <-- Fill this in\n",
    "\n",
    "# Test the ReLU function\n",
    "test_values = [-5, -2, -1, 0, 1, 2, 5]\n",
    "print(\"üß™ Testing ReLU Function:\")\n",
    "print(\"Input  ‚Üí Output\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "for x in test_values:\n",
    "    result = relu_function(x)\n",
    "    print(f\"{x:3d}    ‚Üí {result:.1f}\")\n",
    "\n",
    "print(\"\\nüí° Notice how:\")\n",
    "print(\"   ‚Ä¢ Negative inputs become 0 (blocked)\")\n",
    "print(\"   ‚Ä¢ Positive inputs pass through unchanged\")\n",
    "print(\"   ‚Ä¢ Zero stays zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU function\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y_relu = relu_function(x)\n",
    "y_sigmoid = sigmoid_function(x)  # For comparison\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# ReLU function\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, y_relu, 'r-', linewidth=3, label='ReLU Function')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output f(x)', fontsize=12)\n",
    "plt.title('ReLU: One-Way Valve', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the \"kink\" at x=0\n",
    "plt.plot(0, 0, 'ro', markersize=10, label='Kink at origin')\n",
    "plt.annotate('Kink Point\\n(not smooth)', (0, 0), \n",
    "            xytext=(20, 20), textcoords='offset points',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "# ECE Analogy: Diode characteristic\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, y_relu, 'r-', linewidth=3, label='Current vs Voltage')\n",
    "plt.fill_between(x[x>=0], 0, y_relu[x>=0], alpha=0.3, color='red')\n",
    "plt.xlabel('Voltage (V)', fontsize=12)\n",
    "plt.ylabel('Current (I)', fontsize=12)\n",
    "plt.title('ECE Analogy: Ideal Diode', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare ReLU vs Sigmoid\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, y_relu, 'r-', linewidth=3, label='ReLU (Hard switch)')\n",
    "plt.plot(x, y_sigmoid, 'b-', linewidth=3, label='Sigmoid (Soft switch)')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('Comparison: Hard vs Soft Switch', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Insights:\")\n",
    "print(\"   ‚Ä¢ ReLU is simple: just cut off negative values\")\n",
    "print(\"   ‚Ä¢ It's like a hard switch (abrupt transition at x=0)\")\n",
    "print(\"   ‚Ä¢ Very fast to compute (just a comparison and selection)\")\n",
    "print(\"   ‚Ä¢ Most popular activation function in modern neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C: Understanding Gradients (Derivatives)\n",
    "\n",
    "**Why Do We Care About Gradients?**\n",
    "\n",
    "In neural networks, gradients tell us \"how to adjust weights to improve performance.\" Think of it as:\n",
    "- **ECE**: Like finding the slope of a transfer function to optimize circuit response\n",
    "- **Mechanical**: Like finding the rate of change in a control system to adjust parameters\n",
    "\n",
    "**The Gradient Problem**: Some activation functions have gradients that become very small (vanishing) or very large (exploding), making learning difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function\n",
    "    Formula: œÉ'(x) = œÉ(x) * (1 - œÉ(x))\n",
    "    \"\"\"\n",
    "    # TODO: Implement sigmoid gradient\n",
    "    # Hint: First compute sigmoid(x), then use the formula above\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    s = sigmoid_function(x)\n",
    "    return s * (1 - s)  # <-- Fill this in\n",
    "\n",
    "def relu_gradient(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function\n",
    "    Formula: 1 if x > 0, 0 if x <= 0\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU gradient\n",
    "    # Hint: Use np.where(condition, value_if_true, value_if_false)\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    return np.where(x > 0, 1.0, 0.0)  # <-- Fill this in\n",
    "\n",
    "# Test gradients\n",
    "test_values = np.array([-3, -1, 0, 1, 3])\n",
    "\n",
    "print(\"üß™ Testing Gradient Functions:\")\n",
    "print(\"\\nSigmoid Gradients:\")\n",
    "print(\"Input  ‚Üí Gradient\")\n",
    "print(\"-\" * 17)\n",
    "for x in test_values:\n",
    "    grad = sigmoid_gradient(x)\n",
    "    print(f\"{x:3.0f}    ‚Üí {grad:.4f}\")\n",
    "\n",
    "print(\"\\nReLU Gradients:\")\n",
    "print(\"Input  ‚Üí Gradient\")\n",
    "print(\"-\" * 17)\n",
    "for x in test_values:\n",
    "    grad = relu_gradient(x)\n",
    "    print(f\"{x:3.0f}    ‚Üí {grad:.1f}\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Sigmoid gradient is maximum at x=0 (0.25)\")\n",
    "print(\"   ‚Ä¢ Sigmoid gradient approaches 0 for large |x| (vanishing gradient problem!)\")\n",
    "print(\"   ‚Ä¢ ReLU gradient is either 0 or 1 (no vanishing gradient for positive values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradients to understand the \"vanishing gradient\" problem\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sigmoid_grad = sigmoid_gradient(x)\n",
    "relu_grad = relu_gradient(x)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Sigmoid and its gradient\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, sigmoid_function(x), 'b-', linewidth=3, label='Sigmoid Function')\n",
    "plt.plot(x, sigmoid_grad, 'b--', linewidth=3, label='Sigmoid Gradient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('Sigmoid: Function vs Gradient', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the vanishing gradient problem\n",
    "vanish_x = [-4, 4]\n",
    "vanish_y = [sigmoid_gradient(xi) for xi in vanish_x]\n",
    "plt.plot(vanish_x, vanish_y, 'ro', markersize=8)\n",
    "for xi, yi in zip(vanish_x, vanish_y):\n",
    "    plt.annotate(f'Gradient ‚âà {yi:.3f}\\n(Very small!)', (xi, yi), \n",
    "                xytext=(10, 20), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "# ReLU and its gradient\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, relu_function(x), 'r-', linewidth=3, label='ReLU Function')\n",
    "plt.plot(x, relu_grad, 'r--', linewidth=3, label='ReLU Gradient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('ReLU: Function vs Gradient', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate the constant gradient\n",
    "plt.annotate('Gradient = 1\\n(Constant for x > 0)', (2, 1), \n",
    "            xytext=(10, 20), textcoords='offset points',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='green', alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "# Compare gradient magnitudes\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, sigmoid_grad, 'b-', linewidth=3, label='Sigmoid Gradient')\n",
    "plt.plot(x, relu_grad, 'r-', linewidth=3, label='ReLU Gradient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Why This Matters for Learning:\")\n",
    "print(\"   ‚Ä¢ Large gradients = fast learning\")\n",
    "print(\"   ‚Ä¢ Small gradients = slow learning (vanishing gradient problem)\")\n",
    "print(\"   ‚Ä¢ Zero gradients = no learning (dead neurons)\")\n",
    "print(\"   ‚Ä¢ This is why ReLU is so popular - it doesn't vanish for positive inputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gentle Introduction to Tensors (15 minutes)\n",
    "\n",
    "### ü§î What Are Tensors?\n",
    "\n",
    "Don't let the fancy name intimidate you! Tensors are just **multi-dimensional arrays** (like matrices, but can have more dimensions).\n",
    "\n",
    "**You already know these**:\n",
    "- **Scalar** (0D tensor): Just a single number ‚Üí `5`\n",
    "- **Vector** (1D tensor): A list of numbers ‚Üí `[1, 2, 3]`  \n",
    "- **Matrix** (2D tensor): A rectangular array ‚Üí `[[1, 2], [3, 4]]`\n",
    "- **3D Tensor**: Like a stack of matrices ‚Üí `[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]`\n",
    "\n",
    "**Engineering Analogies**:\n",
    "- **ECE**: Like multi-dimensional signals (time, frequency, spatial)\n",
    "- **Mechanical**: Like stress tensors or multi-parameter system states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create different types of tensors\n",
    "print(\"üî¢ Creating Different Types of Tensors\\n\")\n",
    "\n",
    "# Scalar (0D)\n",
    "scalar = tf.constant(5.0)\n",
    "print(f\"üìç Scalar (0D tensor):\")\n",
    "print(f\"   Value: {scalar.numpy()}\")\n",
    "print(f\"   Shape: {scalar.shape} (no dimensions)\")\n",
    "print(f\"   Think: A single measurement (temperature, voltage, etc.)\\n\")\n",
    "\n",
    "# Vector (1D)\n",
    "vector = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "print(f\"üìè Vector (1D tensor):\")\n",
    "print(f\"   Values: {vector.numpy()}\")\n",
    "print(f\"   Shape: {vector.shape} (4 elements in a line)\")\n",
    "print(f\"   Think: Time series data, signal samples, coordinates\\n\")\n",
    "\n",
    "# Matrix (2D)\n",
    "matrix = tf.constant([[1, 2, 3], \n",
    "                      [4, 5, 6]], dtype=tf.float32)\n",
    "print(f\"üî≤ Matrix (2D tensor):\")\n",
    "print(f\"   Values:\\n{matrix.numpy()}\")\n",
    "print(f\"   Shape: {matrix.shape} (2 rows, 3 columns)\")\n",
    "print(f\"   Think: Image data, transformation matrix, spreadsheet\\n\")\n",
    "\n",
    "# 3D Tensor\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], \n",
    "                         [[5, 6], [7, 8]]], dtype=tf.float32)\n",
    "print(f\"üì¶ 3D Tensor:\")\n",
    "print(f\"   Values:\\n{tensor_3d.numpy()}\")\n",
    "print(f\"   Shape: {tensor_3d.shape} (2 matrices, each 2√ó2)\")\n",
    "print(f\"   Think: Stack of images, batch of data, RGB color channels\\n\")\n",
    "\n",
    "print(\"üí° Key Point: The 'shape' tells you the dimensions\")\n",
    "print(\"   Shape (4,) = vector with 4 elements\")\n",
    "print(\"   Shape (2, 3) = matrix with 2 rows, 3 columns\")\n",
    "print(\"   Shape (2, 2, 2) = 2 matrices, each 2√ó2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2A: Basic Tensor Operations\n",
    "\n",
    "Let's do some basic operations that you'll recognize from linear algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication (you know this from linear algebra!)\n",
    "print(\"üîÑ Matrix Operations (Just Like Linear Algebra!)\\n\")\n",
    "\n",
    "# Create two simple matrices\n",
    "A = tf.constant([[1, 2], \n",
    "                 [3, 4]], dtype=tf.float32)\n",
    "B = tf.constant([[5, 6], \n",
    "                 [7, 8]], dtype=tf.float32)\n",
    "\n",
    "print(\"Matrix A:\")\nprint(A.numpy())\nprint(\"\\nMatrix B:\")\nprint(B.numpy())\n",
    "\n",
    "# Element-wise multiplication (like MATLAB .*)\n",
    "element_wise = tf.multiply(A, B)  # or just A * B\n",
    "print(\"\\nüî∏ Element-wise multiplication (A .* B in MATLAB):\")\n",
    "print(element_wise.numpy())\n",
    "print(\"   Each element: A[i,j] * B[i,j]\")\n",
    "\n",
    "# Matrix multiplication (like MATLAB *)\n",
    "matrix_mult = tf.matmul(A, B)  # or A @ B\n",
    "print(\"\\nüîπ Matrix multiplication (A * B in MATLAB):\")\n",
    "print(matrix_mult.numpy())\n",
    "print(\"   Standard linear algebra multiplication\")\n",
    "\n",
    "# Let's verify the matrix multiplication manually for first element\n",
    "manual_calc = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
    "print(f\"\\nüßÆ Manual check for element [0,0]:\")\n",
    "print(f\"   A[0,0]*B[0,0] + A[0,1]*B[1,0] = {A[0,0].numpy()}*{B[0,0].numpy()} + {A[0,1].numpy()}*{B[1,0].numpy()} = {manual_calc.numpy()}\")\n",
    "print(f\"   Result from matrix multiplication: {matrix_mult[0,0].numpy()}\")\n",
    "print(f\"   ‚úÖ Match: {abs(manual_calc.numpy() - matrix_mult[0,0].numpy()) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape manipulation (reshaping, like MATLAB reshape)\n",
    "print(\"üìê Shape Manipulation (Like MATLAB reshape)\\n\")\n",
    "\n",
    "# Start with a simple matrix\n",
    "original = tf.constant([[1, 2, 3], \n",
    "                        [4, 5, 6]])\n",
    "print(f\"Original matrix ({original.shape}):\")\n",
    "print(original.numpy())\n",
    "\n",
    "# TODO: Reshape to 3√ó2 (flip rows and columns)\n",
    "# Hint: Use tf.reshape(tensor, [new_rows, new_columns])\n",
    "reshaped_3x2 = tf.reshape(original, [3, 2])  # <-- Fill this in\n",
    "\n",
    "print(f\"\\nüìè Reshaped to 3√ó2:\")\n",
    "print(reshaped_3x2.numpy())\n",
    "\n",
    "# TODO: Flatten to 1D vector\n",
    "# Hint: Use tf.reshape(tensor, [-1]) where -1 means \"figure out this dimension\"\n",
    "flattened = tf.reshape(original, [-1])  # <-- Fill this in\n",
    "\n",
    "print(f\"\\nüìè Flattened to 1D ({flattened.shape}):\")\n",
    "print(flattened.numpy())\n",
    "\n",
    "# TODO: Transpose (flip rows and columns)\n",
    "# Hint: Use tf.transpose(tensor)\n",
    "transposed = tf.transpose(original)  # <-- Fill this in\n",
    "\n",
    "print(f\"\\nüìè Transposed ({transposed.shape}):\")\n",
    "print(transposed.numpy())\n",
    "\n",
    "print(\"\\nüí° Key Insight: The total number of elements stays the same!\")\n",
    "print(f\"   Original: {original.shape} = {original.shape[0] * original.shape[1]} elements\")\n",
    "print(f\"   3√ó2 reshape: {reshaped_3x2.shape} = {reshaped_3x2.shape[0] * reshaped_3x2.shape[1]} elements\")\n",
    "print(f\"   Flattened: {flattened.shape} = {flattened.shape[0]} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Simple Neural Layer (15 minutes)\n",
    "\n",
    "### üß† What Is a Neural Layer?\n",
    "\n",
    "A neural layer is like a **transformation box** that:\n",
    "1. Takes inputs (numbers)\n",
    "2. Multiplies by weights (learned parameters)\n",
    "3. Adds biases (shifts)\n",
    "4. Applies activation function (non-linearity)\n",
    "\n",
    "**Formula**: output = activation(input √ó weights + bias)\n",
    "\n",
    "**Engineering Analogy**:\n",
    "- **ECE**: Like an op-amp circuit with gain (weights) and offset (bias)\n",
    "- **Mechanical**: Like a control system with gain and reference point adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple neural layer step by step\n",
    "print(\"üèóÔ∏è Building a Simple Neural Layer\\n\")\n",
    "\n",
    "# Step 1: Define the layer parameters\n",
    "input_size = 3    # 3 input neurons\n",
    "output_size = 2   # 2 output neurons\n",
    "\n",
    "print(f\"üìê Layer Architecture: {input_size} inputs ‚Üí {output_size} outputs\")\n",
    "\n",
    "# Step 2: Initialize weights and biases\n",
    "# Weights: random small numbers (we'll learn better initialization later)\n",
    "weights = np.random.randn(input_size, output_size) * 0.5\n",
    "bias = np.zeros(output_size)  # Start with zero bias\n",
    "\n",
    "print(f\"\\nüéØ Parameters:\")\n",
    "print(f\"Weights shape: {weights.shape} (each input connects to each output)\")\n",
    "print(f\"Weights:\\n{weights}\")\n",
    "print(f\"\\nBias shape: {bias.shape}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "\n",
    "# Step 3: Create a test input\n",
    "test_input = np.array([1.0, 2.0, 3.0])  # Simple test values\n",
    "print(f\"\\nüì• Test Input: {test_input}\")\n",
    "\n",
    "# Step 4: Forward pass computation\n",
    "print(f\"\\nüîÑ Forward Pass Computation:\")\n",
    "\n",
    "# Linear transformation: input √ó weights + bias\n",
    "linear_output = np.dot(test_input, weights) + bias\n",
    "print(f\"1. Linear transformation (input √ó weights + bias):\")\n",
    "print(f\"   {test_input} √ó weights + {bias} = {linear_output}\")\n",
    "\n",
    "# Apply activation function (ReLU)\n",
    "final_output = relu_function(linear_output)\n",
    "print(f\"2. Apply ReLU activation:\")\n",
    "print(f\"   ReLU({linear_output}) = {final_output}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final output: {final_output}\")\n",
    "print(f\"   The layer transformed 3 inputs into 2 outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple function to do this transformation\n",
    "def simple_neural_layer(inputs, weights, bias, activation_function):\n",
    "    \"\"\"\n",
    "    A simple neural layer function\n",
    "    \n",
    "    Steps:\n",
    "    1. Linear transformation: inputs √ó weights + bias\n",
    "    2. Apply activation function\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input values\n",
    "        weights: Weight matrix \n",
    "        bias: Bias vector\n",
    "        activation_function: Function to apply (sigmoid, ReLU, etc.)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the layer computation\n",
    "    # Step 1: Linear transformation\n",
    "    linear = np.dot(inputs, weights) + bias  # <-- Fill this in\n",
    "    \n",
    "    # Step 2: Apply activation\n",
    "    output = activation_function(linear)  # <-- Fill this in\n",
    "    \n",
    "    return output, linear  # Return both for analysis\n",
    "\n",
    "# Test our layer function\n",
    "print(\"üß™ Testing Our Neural Layer Function\\n\")\n",
    "\n",
    "# Test with different inputs\n",
    "test_inputs = [\n",
    "    [1, 0, 0],    # Only first input active\n",
    "    [0, 1, 0],    # Only second input active\n",
    "    [0, 0, 1],    # Only third input active\n",
    "    [1, 1, 1],    # All inputs active\n",
    "    [-1, 2, 0.5]  # Mixed positive/negative\n",
    "]\n",
    "\n",
    "print(\"Input      ‚Üí Linear Output  ‚Üí ReLU Output\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for inp in test_inputs:\n",
    "    inp = np.array(inp)\n",
    "    relu_out, linear_out = simple_neural_layer(inp, weights, bias, relu_function)\n",
    "    print(f\"{str(inp):12} ‚Üí {linear_out} ‚Üí {relu_out}\")\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Ä¢ Different inputs produce different outputs (good!)\")\n",
    "print(\"   ‚Ä¢ ReLU sets negative values to 0\")\n",
    "print(\"   ‚Ä¢ The weights determine how inputs influence outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how the layer transforms inputs\n",
    "print(\"üìä Visualizing Neural Layer Behavior\\n\")\n",
    "\n",
    "# Create a range of inputs for the first input dimension (keeping others at 0)\n",
    "input_range = np.linspace(-3, 3, 50)\n",
    "outputs_neuron1 = []\n",
    "outputs_neuron2 = []\n",
    "\n",
    "for x in input_range:\n",
    "    test_input = np.array([x, 0, 0])  # Vary first input, keep others at 0\n",
    "    output, _ = simple_neural_layer(test_input, weights, bias, relu_function)\n",
    "    outputs_neuron1.append(output[0])\n",
    "    outputs_neuron2.append(output[1])\n",
    "\n",
    "# Plot the input-output relationship\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(input_range, outputs_neuron1, 'b-', linewidth=3, label='Output Neuron 1')\n",
    "plt.plot(input_range, outputs_neuron2, 'r-', linewidth=3, label='Output Neuron 2')\n",
    "plt.xlabel('First Input Value', fontsize=12)\n",
    "plt.ylabel('Neuron Output', fontsize=12)\n",
    "plt.title('Neural Layer Response', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Show the effect of ReLU\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compare with and without ReLU\n",
    "linear_outputs_1 = []\n",
    "linear_outputs_2 = []\n",
    "for x in input_range:\n",
    "    test_input = np.array([x, 0, 0])\n",
    "    _, linear_out = simple_neural_layer(test_input, weights, bias, lambda x: x)  # No activation\n",
    "    linear_outputs_1.append(linear_out[0])\n",
    "    linear_outputs_2.append(linear_out[1])\n",
    "\n",
    "plt.plot(input_range, linear_outputs_1, 'b--', linewidth=2, label='Linear (no ReLU)', alpha=0.7)\n",
    "plt.plot(input_range, outputs_neuron1, 'b-', linewidth=3, label='With ReLU')\n",
    "plt.xlabel('First Input Value', fontsize=12)\n",
    "plt.ylabel('Output Value', fontsize=12)\n",
    "plt.title('Effect of ReLU Activation', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà What We See:\")\n",
    "print(\"   ‚Ä¢ Without ReLU: Linear relationship (straight line)\")\n",
    "print(\"   ‚Ä¢ With ReLU: Non-linear (bent at zero)\")\n",
    "print(\"   ‚Ä¢ Different neurons can have different responses\")\n",
    "print(\"   ‚Ä¢ This non-linearity is what makes neural networks powerful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a Simple Network (10 minutes)\n",
    "\n",
    "### üèóÔ∏è Connecting Layers\n",
    "\n",
    "A neural network is just **multiple layers connected together**:\n",
    "- Output of Layer 1 ‚Üí Input of Layer 2\n",
    "- Output of Layer 2 ‚Üí Input of Layer 3\n",
    "- And so on...\n",
    "\n",
    "Think of it like a **signal processing pipeline** or a **multi-stage amplifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple 2-layer network\n",
    "print(\"üèóÔ∏è Building a Simple 2-Layer Neural Network\\n\")\n",
    "\n",
    "# Network architecture: 3 ‚Üí 4 ‚Üí 2\n",
    "# Layer 1: 3 inputs ‚Üí 4 hidden neurons\n",
    "# Layer 2: 4 inputs ‚Üí 2 output neurons\n",
    "\n",
    "print(\"üìê Network Architecture: 3 ‚Üí 4 ‚Üí 2\")\n",
    "print(\"   ‚Ä¢ Input layer: 3 neurons\")\n",
    "print(\"   ‚Ä¢ Hidden layer: 4 neurons (with ReLU)\")\n",
    "print(\"   ‚Ä¢ Output layer: 2 neurons (with ReLU)\")\n",
    "\n",
    "# Initialize weights and biases for both layers\n",
    "# Layer 1: 3 ‚Üí 4\n",
    "weights1 = np.random.randn(3, 4) * 0.5\n",
    "bias1 = np.zeros(4)\n",
    "\n",
    "# Layer 2: 4 ‚Üí 2  \n",
    "weights2 = np.random.randn(4, 2) * 0.5\n",
    "bias2 = np.zeros(2)\n",
    "\n",
    "print(f\"\\nüéØ Layer 1 weights shape: {weights1.shape}\")\n",
    "print(f\"üéØ Layer 1 bias shape: {bias1.shape}\")\n",
    "print(f\"üéØ Layer 2 weights shape: {weights2.shape}\")\n",
    "print(f\"üéØ Layer 2 bias shape: {bias2.shape}\")\n",
    "\n",
    "# Test input\n",
    "network_input = np.array([1.0, 0.5, -0.5])\n",
    "print(f\"\\nüì• Network Input: {network_input}\")\n",
    "\n",
    "# Forward pass through the network\n",
    "print(f\"\\nüîÑ Forward Pass:\")\n",
    "\n",
    "# Layer 1\n",
    "hidden_output, hidden_linear = simple_neural_layer(network_input, weights1, bias1, relu_function)\n",
    "print(f\"1. After Layer 1 (3‚Üí4): {hidden_output}\")\n",
    "\n",
    "# Layer 2\n",
    "final_output, final_linear = simple_neural_layer(hidden_output, weights2, bias2, relu_function)\n",
    "print(f\"2. After Layer 2 (4‚Üí2): {final_output}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final Network Output: {final_output}\")\n",
    "print(f\"   The network transformed 3 inputs into 2 outputs through 4 hidden neurons!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function for our complete network\n",
    "def simple_two_layer_network(inputs):\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network: 3 ‚Üí 4 ‚Üí 2\n",
    "    \"\"\"\n",
    "    # TODO: Implement the 2-layer network\n",
    "    # Layer 1: inputs ‚Üí hidden\n",
    "    hidden, _ = simple_neural_layer(inputs, weights1, bias1, relu_function)\n",
    "    \n",
    "    # Layer 2: hidden ‚Üí output\n",
    "    output, _ = simple_neural_layer(hidden, weights2, bias2, relu_function)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test the network with various inputs\n",
    "print(\"üß™ Testing Our 2-Layer Network\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    [1, 0, 0],      # First input only\n",
    "    [0, 1, 0],      # Second input only\n",
    "    [0, 0, 1],      # Third input only\n",
    "    [1, 1, 1],      # All inputs equal\n",
    "    [2, -1, 0.5],   # Mixed values\n",
    "    [-1, -1, -1],   # All negative\n",
    "]\n",
    "\n",
    "print(\"Input          ‚Üí Network Output\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for test_input in test_cases:\n",
    "    test_input = np.array(test_input)\n",
    "    output = simple_two_layer_network(test_input)\n",
    "    print(f\"{str(test_input):15} ‚Üí {output}\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ The network produces different outputs for different inputs\")\n",
    "print(\"   ‚Ä¢ Some outputs might be zero due to ReLU activation\")\n",
    "print(\"   ‚Ä¢ The network has learned to map 3D inputs to 2D outputs\")\n",
    "print(\"   ‚Ä¢ This is the foundation of pattern recognition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Simple Understanding Check\n",
    "\n",
    "Let's make sure you understand the key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick conceptual check\n",
    "print(\"ü§î Quick Understanding Check\\n\")\n",
    "\n",
    "print(\"1. What does ReLU do?\")\n",
    "print(\"   Answer: Sets negative values to 0, keeps positive values unchanged\")\n",
    "print(\"   Like a one-way valve or diode\\n\")\n",
    "\n",
    "print(\"2. What does Sigmoid do?\")\n",
    "print(\"   Answer: Squashes any input to between 0 and 1\")\n",
    "print(\"   Like a soft switch or probability converter\\n\")\n",
    "\n",
    "print(\"3. What's a neural layer?\")\n",
    "print(\"   Answer: Input √ó Weights + Bias ‚Üí Activation Function\")\n",
    "print(\"   Like an adjustable signal processor\\n\")\n",
    "\n",
    "print(\"4. What's a neural network?\")\n",
    "print(\"   Answer: Multiple layers connected together\")\n",
    "print(\"   Like a pipeline of signal processors\\n\")\n",
    "\n",
    "print(\"5. Why do we need activation functions?\")\n",
    "print(\"   Answer: To add non-linearity so the network can learn complex patterns\")\n",
    "print(\"   Without them, it would just be linear algebra (boring!)\\n\")\n",
    "\n",
    "# Simple practical test\n",
    "print(\"‚úÖ Practical Test:\")\n",
    "simple_input = np.array([1, -1, 2])\n",
    "simple_output = simple_two_layer_network(simple_input)\n",
    "print(f\"   Input {simple_input} ‚Üí Output {simple_output}\")\n",
    "print(f\"   ‚úÖ Your network successfully processed the input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Gentle Assessment\n",
    "\n",
    "Instead of complex unit tests, let's do a friendly understanding check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gentle_assessment():\n",
    "    \"\"\"\n",
    "    A friendly assessment focused on understanding rather than implementation\n",
    "    \"\"\"\n",
    "    print(\"üéì Gentle Understanding Assessment\\n\")\n",
    "    print(\"Let's check your understanding with simple questions:\\n\")\n",
    "    \n",
    "    score = 0\n",
    "    total = 5\n",
    "    \n",
    "    # Test 1: Sigmoid understanding\n",
    "    print(\"1Ô∏è‚É£ Sigmoid Function Test\")\n",
    "    try:\n",
    "        sig_zero = sigmoid_function(0)\n",
    "        if abs(sig_zero - 0.5) < 0.01:\n",
    "            print(\"   ‚úÖ Correct: sigmoid(0) ‚âà 0.5\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå Expected sigmoid(0) ‚âà 0.5, got {sig_zero}\")\n",
    "    except:\n",
    "        print(\"   ‚ùå Sigmoid function not implemented\")\n",
    "    \n",
    "    # Test 2: ReLU understanding\n",
    "    print(\"\\n2Ô∏è‚É£ ReLU Function Test\")\n",
    "    try:\n",
    "        relu_pos = relu_function(2)\n",
    "        relu_neg = relu_function(-2)\n",
    "        if relu_pos == 2 and relu_neg == 0:\n",
    "            print(\"   ‚úÖ Correct: ReLU blocks negative, passes positive\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå Expected ReLU(2)=2, ReLU(-2)=0, got {relu_pos}, {relu_neg}\")\n",
    "    except:\n",
    "        print(\"   ‚ùå ReLU function not implemented\")\n",
    "    \n",
    "    # Test 3: Tensor shape understanding\n",
    "    print(\"\\n3Ô∏è‚É£ Tensor Shape Test\")\n",
    "    test_matrix = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "    try:\n",
    "        transposed = tf.transpose(test_matrix)\n",
    "        if transposed.shape == (3, 2):\n",
    "            print(\"   ‚úÖ Correct: Understood tensor reshaping\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå Expected shape (3, 2), got {transposed.shape}\")\n",
    "    except:\n",
    "        print(\"   ‚ùå Tensor operations not completed\")\n",
    "    \n",
    "    # Test 4: Layer concept\n",
    "    print(\"\\n4Ô∏è‚É£ Neural Layer Test\")\n",
    "    try:\n",
    "        test_input = np.array([1, 0, 0])\n",
    "        layer_output = simple_two_layer_network(test_input)\n",
    "        if layer_output is not None and len(layer_output) == 2:\n",
    "            print(\"   ‚úÖ Correct: Network produces 2 outputs from 3 inputs\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(\"   ‚ùå Network doesn't produce expected output\")\n",
    "    except:\n",
    "        print(\"   ‚ùå Network not implemented\")\n",
    "    \n",
    "    # Test 5: Conceptual understanding\n",
    "    print(\"\\n5Ô∏è‚É£ Conceptual Understanding\")\n",
    "    print(\"   Question: What makes neural networks powerful?\")\n",
    "    print(\"   Answer: Non-linear activation functions allow learning complex patterns\")\n",
    "    print(\"   ‚úÖ This is a conceptual understanding - you get this point for participation!\")\n",
    "    score += 1\n",
    "    \n",
    "    # Results\n",
    "    percentage = (score / total) * 100\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä ASSESSMENT RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Score: {score}/{total} ({percentage:.0f}%)\")\n",
    "    \n",
    "    if percentage >= 80:\n",
    "        grade = \"A\"\n",
    "        message = \"üéâ Excellent understanding! You're ready for more advanced topics.\"\n",
    "    elif percentage >= 60:\n",
    "        grade = \"B\"\n",
    "        message = \"üëç Good work! You understand the basics well.\"\n",
    "    else:\n",
    "        grade = \"C\"\n",
    "        message = \"üí™ Keep learning! Review the concepts and try the exercises again.\"\n",
    "    \n",
    "    print(f\"Grade: {grade}\")\n",
    "    print(f\"{message}\")\n",
    "    \n",
    "    print(\"\\nüéØ What You've Learned:\")\n",
    "    print(\"   ‚Ä¢ How activation functions work (sigmoid, ReLU)\")\n",
    "    print(\"   ‚Ä¢ Basic tensor operations (reshape, multiply)\")\n",
    "    print(\"   ‚Ä¢ How neural layers process information\")\n",
    "    print(\"   ‚Ä¢ How to connect layers into a network\")\n",
    "    print(\"   ‚Ä¢ Why non-linearity is important\")\n",
    "    \n",
    "    return score, total\n",
    "\n",
    "# Run the gentle assessment\n",
    "assessment_score, total_possible = gentle_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü Summary & Next Steps\n",
    "\n",
    "### üéØ What You Accomplished Today:\n",
    "\n",
    "‚úÖ **Understood Activation Functions**: You learned how sigmoid and ReLU work like switches and valves\n",
    "\n",
    "‚úÖ **Mastered Basic Tensors**: You worked with matrices and understood reshaping (just like MATLAB!)\n",
    "\n",
    "‚úÖ **Built Neural Layers**: You created signal processors that transform inputs to outputs\n",
    "\n",
    "‚úÖ **Constructed Networks**: You connected layers to create a simple neural network\n",
    "\n",
    "### üîó Connection to Your Field:\n",
    "\n",
    "**For ECE Students**:\n",
    "- Neural networks are like adaptive signal processing systems\n",
    "- Activation functions are like non-linear circuit elements\n",
    "- Weights are like adjustable gains in amplifiers\n",
    "\n",
    "**For Mechanical Students**:\n",
    "- Neural networks are like adaptive control systems\n",
    "- Activation functions are like control valves with different characteristics\n",
    "- The network learns optimal control parameters\n",
    "\n",
    "### üöÄ Next Steps in Your Learning Journey:\n",
    "\n",
    "1. **Module 2**: Learn how networks adjust their weights (optimization)\n",
    "2. **Module 3**: Apply neural networks to image processing\n",
    "3. **Module 4**: Build specialized networks for pattern recognition\n",
    "4. **Module 5**: Create systems that can detect and classify objects\n",
    "\n",
    "### üí° Key Insights to Remember:\n",
    "\n",
    "- **Neural networks are just mathematical functions** that learn from examples\n",
    "- **Activation functions add the \"intelligence\"** by introducing non-linearity\n",
    "- **Layers are building blocks** - combine them to solve complex problems\n",
    "- **It's all about transforming data** from one representation to another\n",
    "\n",
    "### üìö If You Want to Learn More:\n",
    "\n",
    "- Practice with different activation functions\n",
    "- Try building networks with more layers\n",
    "- Experiment with different input patterns\n",
    "- Think about how this applies to your engineering projects\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Congratulations!\n",
    "\n",
    "You've taken your first steps into the world of neural networks. These concepts might have seemed intimidating at first, but you've learned that they're just mathematical tools - like the equations and systems you already know from your engineering background.\n",
    "\n",
    "**The key insight**: Neural networks are not magic - they're engineered systems that can be understood, analyzed, and applied to solve real problems in your field!\n",
    "\n",
    "Keep this curiosity and systematic approach as you continue learning. You're well-prepared for the more advanced topics ahead! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}