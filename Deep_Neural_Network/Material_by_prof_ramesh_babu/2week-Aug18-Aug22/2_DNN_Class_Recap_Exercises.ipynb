{"cells":[{"cell_type":"markdown","metadata":{"id":"lxsWveFILpoZ"},"source":["# ðŸ“ Recap + Exercises: Perceptron, XOR, MLP\n","\n","This notebook combines **Part 1** (Biological â†’ Perceptron â†’ XOR failure) and **Part 2** (MLP â†’ Activations â†’ UAT â†’ Gradients).\n","\n","By the end, you should:\n","- Understand **why perceptrons fail on XOR**\n","- See how **MLPs solve XOR**\n","- Know the role of **nonlinear activations**\n","- Get intuition for the **Universal Approximation Theorem**\n","- Practice **TensorFlow basics** (`GradientTape`)\n"],"id":"lxsWveFILpoZ"},{"cell_type":"markdown","metadata":{"id":"XJT7UgwfLpoc"},"source":["## ðŸ”„ Recap: Key Concepts\n","1. **Biological Neuron â†’ Perceptron**: Inputs Ã— Weights + Bias â†’ Activation â†’ Output.\n","2. **Logic Gates with Perceptron**: AND, OR are linearly separable.\n","3. **Linear Separability**: XOR is not linearly separable â†’ perceptron fails.\n","4. **MLP Solution**: Hidden layer + nonlinearity bends decision boundaries.\n","5. **Activation Functions**: Provide nonlinearity (sigmoid, tanh, ReLU).\n","6. **Universal Approximation Theorem**: MLP can approximate any function with enough units.\n","7. **Gradients & Backpropagation**: Derivatives guide weight updates to reduce loss.\n"],"id":"XJT7UgwfLpoc"},{"cell_type":"markdown","metadata":{"id":"jOnynCeDLpoc"},"source":["## ðŸ‹ï¸ Exercises\n","Try these exercises step by step. Modify code, run cells, and observe.\n","\n","### 1. Perceptron Logic Gates\n","- Implement perceptron for **NAND** gate.\n","- Plot the decision boundary.\n","- Question: Why can NAND be separated linearly?\n","\n","### 2. XOR Failure\n","- Train a perceptron (using NumPy or `sklearn`) on XOR.\n","- Print predictions vs true labels.\n","- Question: Why does it fail?\n","\n","### 3. MLP on XOR\n","- Train an MLP with 1 hidden layer (2â€“4 neurons).\n","- Change activation functions: `sigmoid`, `tanh`, `relu`.\n","- Observe accuracy and decision boundaries.\n","\n","### 4. Activation Shapes\n","- Plot step, sigmoid, tanh, and ReLU.\n","- Question: Which are bounded? Which are unbounded?\n","\n","### 5. Universal Approximation Mini Demo\n","- Modify the `sin(x)` demo: try approximating `cos(x)`.\n","- Increase hidden units from 32 â†’ 64 â†’ 128. Does fit improve?\n","\n","### 6. Gradient Descent Intuition\n","- Use `GradientTape` to compute gradient of a custom function (e.g., `y=x^3-3x`).\n","- Plot tangent lines at x=-2, 0, 2.\n","- Question: How do slopes differ across the curve?\n","\n","### 7. Bonus Challenge\n","- Combine AND, OR, and NAND perceptrons to hand-craft XOR (without training).\n","- Hint: XOR = (x1 OR x2) AND (NOT(x1 AND x2)).\n"],"id":"jOnynCeDLpoc"},{"cell_type":"markdown","metadata":{"id":"57-9TvhuLpod"},"source":["### Starter: NAND Perceptron\n","Weights for NAND can be set manually. Complete and test below:"],"id":"57-9TvhuLpod"},{"cell_type":"code","metadata":{"id":"yjMkt_y8Lpod","executionInfo":{"status":"ok","timestamp":1755470331931,"user_tz":240,"elapsed":10,"user":{"displayName":"Ramesh Babu","userId":"08320068323009125260"}},"outputId":"e41aa233-f9bd-42d1-b0c9-2f44b4686593","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["NAND Gate:\n","(0, 0) -> 1\n","(0, 1) -> 1\n","(1, 0) -> 1\n","(1, 1) -> 0\n"]}],"source":["def perceptron(x1, x2, w1, w2, b):\n","    return int(w1*x1 + w2*x2 + b >= 0)\n","\n","print(\"NAND Gate:\")\n","for x1, x2 in [(0,0),(0,1),(1,0),(1,1)]:\n","    print((x1,x2), \"->\", perceptron(x1,x2, -1, -1, 1.5))  # adjust weights/bias\n"],"id":"yjMkt_y8Lpod"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}