{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n**These materials are created by Prof. Ramesh Babu exclusively for M.Tech Students of SRM University**\n\nÂ© 2025 Prof. Ramesh Babu. All rights reserved. This material is protected by copyright and may not be reproduced, distributed, or transmitted in any form or by any means without prior written permission.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ§® T3-Exercise-2: Mathematical Operations\n",
    "**Deep Neural Network Architectures (21CSE558T) - Week 2, Day 4**  \n",
    "**M.Tech Lab Session - Duration: 30-45 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ LEARNING OBJECTIVES\n",
    "By the end of this exercise, you will:\n",
    "- âš¡ Master element-wise operations (the building blocks of neural computations)\n",
    "- ğŸ¯ Understand matrix multiplication (the heart of neural networks)\n",
    "- ğŸ”„ Learn broadcasting (making tensors work together efficiently)\n",
    "- ğŸ§  Apply mathematical operations in real neural network scenarios\n",
    "- ğŸ” Debug shape mismatches and mathematical errors\n",
    "\n",
    "## ğŸ”— CONNECTION TO NEURAL NETWORKS\n",
    "Mathematics is the **engine** that powers neural networks:\n",
    "- **Element-wise operations** â†’ Activation functions, normalization\n",
    "- **Matrix multiplication** â†’ Layer transformations (input Ã— weights)\n",
    "- **Broadcasting** â†’ Efficient batch processing\n",
    "- **Reduction operations** â†’ Loss calculation, metrics\n",
    "\n",
    "**Real Example:** When an image passes through a neural layer:  \n",
    "`output = activation(input @ weights + bias)` â† All math operations!\n",
    "\n",
    "## ğŸ“š PREREQUISITES\n",
    "- âœ… Completed T3-Exercise-1 (Tensor Fundamentals)\n",
    "- ğŸ“ Basic matrix multiplication concepts\n",
    "- ğŸ§® Understanding of element-wise vs matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## âš™ï¸ SETUP & ENVIRONMENT CHECK\n",
    "ğŸš€ Let's power up our mathematical toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": [
    "# ğŸ› ï¸ Essential imports for mathematical operations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# ğŸ”§ Environment verification\n",
    "print(\"ğŸ”§ MATHEMATICAL TOOLKIT CHECK\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"ğŸ Python: {sys.version.split()[0]}\")\n",
    "print(f\"ğŸ”¥ TensorFlow: {tf.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy: {np.__version__}\")\n",
    "\n",
    "# ğŸ® Check computational capabilities\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ğŸš€ GPU acceleration: AVAILABLE (Lightning fast!)\")\n",
    "else:\n",
    "    print(\"ğŸ’» CPU computation: READY (Perfect for learning)\")\n",
    "\n",
    "print(\"\\nğŸ‰ Ready to explore the mathematics of intelligence!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concepts"
   },
   "source": [
    "## ğŸ§  CORE CONCEPTS: The Mathematics of Neural Networks\n",
    "\n",
    "### ğŸ­ TWO TYPES OF OPERATIONS:\n",
    "\n",
    "#### 1ï¸âƒ£ **Element-wise Operations** (Broadcasting Magic)\n",
    "- **What:** Operations between corresponding elements\n",
    "- **Example:** `[1,2] + [3,4] = [4,6]`\n",
    "- **Neural Networks:** Activation functions, normalization, element-wise gates\n",
    "\n",
    "#### 2ï¸âƒ£ **Matrix Operations** (Linear Transformations)\n",
    "- **What:** Mathematical combinations following matrix rules\n",
    "- **Example:** Matrix multiplication for layer transformations\n",
    "- **Neural Networks:** Weight Ã— input computations\n",
    "\n",
    "### ğŸ”„ BROADCASTING: TensorFlow's Superpower\n",
    "**Broadcasting** lets you operate on tensors of different shapes efficiently:\n",
    "- Add a bias vector to a batch of data\n",
    "- Scale entire tensors with single values\n",
    "- Normalize across different dimensions\n",
    "\n",
    "### ğŸ¯ WHY THIS MATTERS:\n",
    "Every forward pass in a neural network is a **chain of mathematical operations**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_title"
   },
   "source": [
    "## ğŸ”¥ STEP 1: Element-wise Operations\n",
    "### ğŸ§® The Building Blocks of Neural Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_setup"
   },
   "outputs": [],
   "source": [
    "# ğŸ² Let's create sample tensors to work with\n",
    "print(\"ğŸ² Creating Sample Tensors for Mathematical Adventures\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Think of these as activations from two different neurons\n",
    "tensor_A = tf.constant([[1.0, 2.0, 3.0], \n",
    "                        [4.0, 5.0, 6.0]])\n",
    "\n",
    "tensor_B = tf.constant([[2.0, 1.0, 4.0], \n",
    "                        [3.0, 6.0, 2.0]])\n",
    "\n",
    "print(\"ğŸ…°ï¸ Tensor A (imagine: activations from layer 1):\")\n",
    "print(tensor_A)\n",
    "print(f\"   Shape: {tensor_A.shape} (2 samples, 3 features each)\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ…±ï¸ Tensor B (imagine: activations from layer 2):\")\n",
    "print(tensor_B)\n",
    "print(f\"   Shape: {tensor_B.shape} (2 samples, 3 features each)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_addition"
   },
   "outputs": [],
   "source": [
    "# â• ADDITION: Element-wise addition\n",
    "print(\"â• ELEMENT-WISE ADDITION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "addition_result = tf.add(tensor_A, tensor_B)  # or simply: tensor_A + tensor_B\n",
    "\n",
    "print(\"Formula: A + B (element by element)\")\n",
    "print(f\"Result:\\n{addition_result}\")\n",
    "print()\n",
    "print(\"ğŸ§  Neural Network Use Case:\")\n",
    "print(\"   Combining activations from different pathways\")\n",
    "print(\"   Adding residual connections (like in ResNet)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_multiplication"
   },
   "outputs": [],
   "source": [
    "# âœ–ï¸ ELEMENT-WISE MULTIPLICATION\n",
    "print(\"âœ–ï¸ ELEMENT-WISE MULTIPLICATION\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "element_mult = tf.multiply(tensor_A, tensor_B)  # or: tensor_A * tensor_B\n",
    "\n",
    "print(\"Formula: A âŠ™ B (Hadamard product)\")\n",
    "print(f\"Result:\\n{element_mult}\")\n",
    "print()\n",
    "print(\"ğŸ§  Neural Network Use Case:\")\n",
    "print(\"   Attention mechanisms (scaling features)\")\n",
    "print(\"   Gating mechanisms (LSTM, GRU gates)\")\n",
    "print(\"   Dropout masks during training\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_more_ops"
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ MORE USEFUL ELEMENT-WISE OPERATIONS\n",
    "print(\"ğŸ¯ MORE ELEMENT-WISE OPERATIONS\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Square root (useful for normalization)\n",
    "sqrt_result = tf.sqrt(tensor_A)\n",
    "print(f\"ğŸ”¢ Square root of A:\\n{sqrt_result}\")\n",
    "print(\"   Use case: Standard deviation calculations\")\n",
    "print()\n",
    "\n",
    "# Exponential (used in softmax)\n",
    "exp_result = tf.exp(tensor_A)\n",
    "print(f\"ğŸ“ˆ Exponential of A:\\n{exp_result}\")\n",
    "print(\"   Use case: Softmax activation function\")\n",
    "print()\n",
    "\n",
    "# Power operations\n",
    "power_result = tf.pow(tensor_A, 2)\n",
    "print(f\"âš¡ A squared (AÂ²):\\n{power_result}\")\n",
    "print(\"   Use case: Mean squared error calculations\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_title"
   },
   "source": [
    "## ğŸ¯ STEP 2: Matrix Multiplication - The Heart of Neural Networks\n",
    "### ğŸ’ª Where the real magic happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_setup"
   },
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ Setting up matrices for neural network simulation\n",
    "print(\"ğŸ—ï¸ MATRIX MULTIPLICATION SETUP\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Simulate input data (batch of 3 samples, each with 4 features)\n",
    "input_data = tf.constant([[1.0, 2.0, 3.0, 4.0],   # Sample 1\n",
    "                          [2.0, 3.0, 4.0, 5.0],   # Sample 2  \n",
    "                          [3.0, 4.0, 5.0, 6.0]])  # Sample 3\n",
    "\n",
    "# Simulate weight matrix (4 inputs â†’ 3 outputs)\n",
    "weights = tf.constant([[0.1, 0.2, 0.3],  # Weights for input 1 â†’ all outputs\n",
    "                       [0.4, 0.5, 0.6],  # Weights for input 2 â†’ all outputs\n",
    "                       [0.7, 0.8, 0.9],  # Weights for input 3 â†’ all outputs\n",
    "                       [0.2, 0.3, 0.4]]) # Weights for input 4 â†’ all outputs\n",
    "\n",
    "print(f\"ğŸ“Š Input data shape: {input_data.shape}\")\n",
    "print(f\"   (3 samples, 4 features each - like 3 images with 4 pixels)\")\n",
    "print()\n",
    "print(f\"âš–ï¸ Weights shape: {weights.shape}\")\n",
    "print(f\"   (4 inputs, 3 outputs - transforming 4 features to 3)\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ” Input Data:\")\n",
    "print(input_data)\n",
    "print()\n",
    "print(\"ğŸ” Weight Matrix:\")\n",
    "print(weights)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_matmul"
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ THE MAGIC: Matrix Multiplication\n",
    "print(\"ğŸ¯ MATRIX MULTIPLICATION - THE NEURAL NETWORK CORE\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# This is what happens in every neural network layer!\n",
    "output = tf.matmul(input_data, weights)\n",
    "\n",
    "print(\"ğŸ”„ Operation: input_data @ weights\")\n",
    "print(f\"ğŸ“ Shape transformation: {input_data.shape} Ã— {weights.shape} = {output.shape}\")\n",
    "print()\n",
    "print(\"âœ¨ Result (Linear transformation):\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ§  What just happened?\")\n",
    "print(\"   â€¢ Each input sample got transformed from 4 features to 3 features\")\n",
    "print(\"   â€¢ This is EXACTLY what happens in a neural network layer\")\n",
    "print(\"   â€¢ The weights learned how to combine input features meaningfully\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_shapes"
   },
   "outputs": [],
   "source": [
    "# ğŸ“ UNDERSTANDING MATRIX MULTIPLICATION SHAPES\n",
    "print(\"ğŸ“ SHAPE COMPATIBILITY CHECK\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def check_matmul_compatibility(A_shape, B_shape):\n",
    "    \"\"\"Helper function to check if matrices can be multiplied\"\"\"\n",
    "    can_multiply = A_shape[-1] == B_shape[0]\n",
    "    if can_multiply:\n",
    "        result_shape = (*A_shape[:-1], B_shape[1])\n",
    "        return True, result_shape\n",
    "    return False, None\n",
    "\n",
    "# Test different shape combinations\n",
    "test_cases = [\n",
    "    ((3, 4), (4, 3), \"âœ… Neural layer transformation\"),\n",
    "    ((32, 784), (784, 128), \"âœ… MNIST â†’ Hidden layer\"),\n",
    "    ((5, 10), (10, 1), \"âœ… Multi-class â†’ Binary output\"),\n",
    "    ((2, 3), (4, 2), \"âŒ Incompatible shapes\"),\n",
    "]\n",
    "\n",
    "for A_shape, B_shape, description in test_cases:\n",
    "    compatible, result_shape = check_matmul_compatibility(A_shape, B_shape)\n",
    "    \n",
    "    if compatible:\n",
    "        print(f\"{A_shape} Ã— {B_shape} = {result_shape} {description}\")\n",
    "    else:\n",
    "        print(f\"{A_shape} Ã— {B_shape} = IMPOSSIBLE! {description}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ Rule: For A Ã— B, the last dimension of A must equal first dimension of B\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_title"
   },
   "source": [
    "## ğŸ”„ STEP 3: Broadcasting - TensorFlow's Superpower\n",
    "### ğŸª Making tensors of different shapes work together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_bias"
   },
   "outputs": [],
   "source": [
    "# ğŸ­ BROADCASTING EXAMPLE 1: Adding Bias\n",
    "print(\"ğŸ­ BROADCASTING MAGIC: Adding Bias to Neural Network Output\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Our previous neural network output\n",
    "network_output = tf.matmul(input_data, weights)\n",
    "print(f\"ğŸ”¢ Network output shape: {network_output.shape}\")\n",
    "print(f\"Network output:\\n{network_output}\")\n",
    "print()\n",
    "\n",
    "# Bias vector (one bias per output neuron)\n",
    "bias = tf.constant([0.1, 0.2, 0.3])\n",
    "print(f\"âš–ï¸ Bias shape: {bias.shape}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print()\n",
    "\n",
    "# Broadcasting magic! bias gets added to each sample\n",
    "output_with_bias = network_output + bias\n",
    "print(f\"âœ¨ After adding bias (broadcasting {network_output.shape} + {bias.shape}):\")\n",
    "print(output_with_bias)\n",
    "print()\n",
    "print(\"ğŸª„ What happened? The bias vector was automatically\")\n",
    "print(\"   'broadcasted' (repeated) for each sample in the batch!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_scaling"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š BROADCASTING EXAMPLE 2: Scaling Operations\n",
    "print(\"ğŸ“Š BROADCASTING: Scaling Entire Tensors\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Scaling with a single number (scalar broadcasting)\n",
    "learning_rate = 0.01\n",
    "scaled_weights = weights * learning_rate\n",
    "\n",
    "print(f\"ğŸšï¸ Original weights shape: {weights.shape}\")\n",
    "print(f\"ğŸ“‰ Learning rate (scalar): {learning_rate}\")\n",
    "print(f\"âš¡ Scaled weights (for gradient descent):\")\n",
    "print(scaled_weights)\n",
    "print()\n",
    "print(\"ğŸ’¡ Use case: Gradient descent weight updates!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_normalization"
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ BROADCASTING EXAMPLE 3: Normalization\n",
    "print(\"ğŸ¯ BROADCASTING: Data Normalization\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Normalize each feature across the batch\n",
    "feature_means = tf.reduce_mean(input_data, axis=0)  # Mean of each feature\n",
    "feature_stds = tf.math.reduce_std(input_data, axis=0)  # Std of each feature\n",
    "\n",
    "print(f\"ğŸ“Š Original data shape: {input_data.shape}\")\n",
    "print(f\"ğŸ“ˆ Feature means: {feature_means} (shape: {feature_means.shape})\")\n",
    "print(f\"ğŸ“ Feature stds: {feature_stds} (shape: {feature_stds.shape})\")\n",
    "print()\n",
    "\n",
    "# Normalize using broadcasting\n",
    "normalized_data = (input_data - feature_means) / feature_stds\n",
    "\n",
    "print(\"âœ¨ Normalized data (zero mean, unit variance per feature):\")\n",
    "print(normalized_data)\n",
    "print()\n",
    "print(\"ğŸ§  Neural Network Benefit: Helps with training stability!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_title"
   },
   "source": [
    "## âš¡ STEP 4: Advanced Operations - The Neural Network Toolkit\n",
    "### ğŸ› ï¸ Operations you'll use in every neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_transpose"
   },
   "outputs": [],
   "source": [
    "# ğŸ”„ TRANSPOSE OPERATIONS\n",
    "print(\"ğŸ”„ TRANSPOSE: Flipping Matrix Dimensions\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "original_matrix = tf.constant([[1, 2, 3], \n",
    "                               [4, 5, 6]])\n",
    "transposed = tf.transpose(original_matrix)\n",
    "\n",
    "print(f\"ğŸ“‹ Original {original_matrix.shape}:\")\n",
    "print(original_matrix)\n",
    "print()\n",
    "print(f\"ğŸ”„ Transposed {transposed.shape}:\")\n",
    "print(transposed)\n",
    "print()\n",
    "print(\"ğŸ§  Neural Network Use Cases:\")\n",
    "print(\"   â€¢ Backpropagation (computing gradients)\")\n",
    "print(\"   â€¢ Weight matrix operations\")\n",
    "print(\"   â€¢ Attention mechanisms\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_reductions"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š REDUCTION OPERATIONS\n",
    "print(\"ğŸ“Š REDUCTION OPERATIONS: Summarizing Data\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Sample batch of data (like loss values or predictions)\n",
    "batch_data = tf.constant([[1.0, 2.0, 3.0], \n",
    "                          [4.0, 5.0, 6.0], \n",
    "                          [7.0, 8.0, 9.0]])\n",
    "\n",
    "print(f\"ğŸ“ˆ Batch data {batch_data.shape}:\")\n",
    "print(batch_data)\n",
    "print()\n",
    "\n",
    "# Different reduction operations\n",
    "total_sum = tf.reduce_sum(batch_data)\n",
    "batch_mean = tf.reduce_mean(batch_data)\n",
    "max_value = tf.reduce_max(batch_data)\n",
    "min_value = tf.reduce_min(batch_data)\n",
    "\n",
    "print(f\"â• Total sum: {total_sum}\")\n",
    "print(f\"ğŸ“Š Mean: {batch_mean}\")\n",
    "print(f\"â¬†ï¸ Maximum: {max_value}\")\n",
    "print(f\"â¬‡ï¸ Minimum: {min_value}\")\n",
    "print()\n",
    "\n",
    "# Axis-specific reductions\n",
    "row_sums = tf.reduce_sum(batch_data, axis=1)  # Sum across columns\n",
    "col_means = tf.reduce_mean(batch_data, axis=0)  # Mean across rows\n",
    "\n",
    "print(f\"ğŸ”½ Row sums (axis=1): {row_sums}\")\n",
    "print(f\"â¡ï¸ Column means (axis=0): {col_means}\")\n",
    "print()\n",
    "print(\"ğŸ§  Neural Network Applications:\")\n",
    "print(\"   â€¢ Loss function calculations\")\n",
    "print(\"   â€¢ Batch statistics for normalization\")\n",
    "print(\"   â€¢ Attention weight computation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_title"
   },
   "source": [
    "## ğŸ® STEP 5: Real Neural Network Simulation\n",
    "### ğŸ—ï¸ Building a complete forward pass with all operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_network"
   },
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ COMPLETE NEURAL NETWORK LAYER SIMULATION\n",
    "print(\"ğŸ—ï¸ BUILDING A COMPLETE NEURAL NETWORK LAYER\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "# Network architecture: 4 inputs â†’ 3 hidden â†’ 2 outputs\n",
    "print(\"ğŸ¯ Network Architecture: 4 â†’ 3 â†’ 2\")\n",
    "print()\n",
    "\n",
    "# Input batch (3 samples, 4 features each)\n",
    "inputs = tf.constant([[1.0, 2.0, 3.0, 4.0],\n",
    "                      [2.0, 3.0, 4.0, 5.0], \n",
    "                      [0.5, 1.5, 2.5, 3.5]])\n",
    "\n",
    "# Layer 1: Input â†’ Hidden (4 â†’ 3)\n",
    "W1 = tf.random.normal([4, 3], stddev=0.1)\n",
    "b1 = tf.constant([0.1, 0.2, 0.3])\n",
    "\n",
    "# Layer 2: Hidden â†’ Output (3 â†’ 2)\n",
    "W2 = tf.random.normal([3, 2], stddev=0.1)\n",
    "b2 = tf.constant([0.05, 0.15])\n",
    "\n",
    "print(f\"ğŸ“Š Input shape: {inputs.shape}\")\n",
    "print(f\"âš–ï¸ W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "print(f\"âš–ï¸ W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_forward"
   },
   "outputs": [],
   "source": [
    "# ğŸš€ FORWARD PASS: Step by step\n",
    "print(\"ğŸš€ FORWARD PASS EXECUTION\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "print(\"ğŸ“ Step 1: Linear transformation (Layer 1)\")\n",
    "hidden_linear = tf.matmul(inputs, W1) + b1  # Matrix mult + Broadcasting\n",
    "print(f\"   Linear output shape: {hidden_linear.shape}\")\n",
    "print(f\"   Sample values: {hidden_linear[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ Step 2: Apply activation function (ReLU)\")\n",
    "hidden_activated = tf.nn.relu(hidden_linear)  # Element-wise operation\n",
    "print(f\"   Activated shape: {hidden_activated.shape}\")\n",
    "print(f\"   Sample values: {hidden_activated[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ Step 3: Second linear transformation (Layer 2)\")\n",
    "output_linear = tf.matmul(hidden_activated, W2) + b2\n",
    "print(f\"   Output linear shape: {output_linear.shape}\")\n",
    "print(f\"   Sample values: {output_linear[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ Step 4: Final activation (Sigmoid for binary classification)\")\n",
    "final_output = tf.nn.sigmoid(output_linear)\n",
    "print(f\"   Final output shape: {final_output.shape}\")\n",
    "print(f\"   Predictions for all samples:\")\n",
    "print(final_output)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ‰ COMPLETE! We just simulated a 2-layer neural network!\")\n",
    "print(\"   Used: Matrix multiplication, broadcasting, element-wise operations\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation_title"
   },
   "source": [
    "## âœ… VALIDATION & DEBUGGING\n",
    "### ğŸ” Let's test your mathematical mastery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_shapes"
   },
   "outputs": [],
   "source": [
    "# ğŸ§© SHAPE DEBUGGING CHALLENGE\n",
    "print(\"ğŸ§© MATHEMATICAL DEBUGGING CHALLENGE\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Create some \"problematic\" scenarios\n",
    "print(\"ğŸ” Checking common neural network shape issues...\")\n",
    "print()\n",
    "\n",
    "# Test case 1: Batch size compatibility\n",
    "batch1 = tf.random.normal([32, 784])  # 32 samples, 784 features (like MNIST)\n",
    "weights1 = tf.random.normal([784, 128])  # 784 â†’ 128 transformation\n",
    "\n",
    "try:\n",
    "    result1 = tf.matmul(batch1, weights1)\n",
    "    print(f\"âœ… Test 1 PASSED: {batch1.shape} Ã— {weights1.shape} = {result1.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Test 1 FAILED: {e}\")\n",
    "\n",
    "# Test case 2: Broadcasting bias addition\n",
    "output = tf.random.normal([10, 5])  # 10 samples, 5 outputs\n",
    "bias = tf.random.normal([5])  # 5 bias values\n",
    "\n",
    "try:\n",
    "    result2 = output + bias\n",
    "    print(f\"âœ… Test 2 PASSED: Broadcasting {output.shape} + {bias.shape} = {result2.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Test 2 FAILED: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ¯ Key Debugging Skills:\")\n",
    "print(\"   â€¢ Always check tensor shapes before operations\")\n",
    "print(\"   â€¢ Remember matrix multiplication rules\")\n",
    "print(\"   â€¢ Understand broadcasting patterns\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_practical"
   },
   "outputs": [],
   "source": [
    "# ğŸª PRACTICAL SCENARIO: Image Classification\n",
    "print(\"ğŸª PRACTICAL SCENARIO: Image Classification Pipeline\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Simulate a batch of flattened images (like MNIST)\n",
    "batch_size = 5\n",
    "image_pixels = 28 * 28  # 784 pixels per image\n",
    "num_classes = 10\n",
    "\n",
    "# Fake image data\n",
    "images = tf.random.uniform([batch_size, image_pixels], 0, 1)\n",
    "print(f\"ğŸ“¸ Image batch: {images.shape} (5 images, 784 pixels each)\")\n",
    "\n",
    "# Classification weights and bias\n",
    "classifier_weights = tf.random.normal([image_pixels, num_classes], stddev=0.01)\n",
    "classifier_bias = tf.zeros([num_classes])\n",
    "\n",
    "print(f\"ğŸ¯ Classifier weights: {classifier_weights.shape}\")\n",
    "print(f\"âš–ï¸ Classifier bias: {classifier_bias.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "logits = tf.matmul(images, classifier_weights) + classifier_bias\n",
    "predictions = tf.nn.softmax(logits)  # Convert to probabilities\n",
    "\n",
    "print(f\"ğŸ“Š Raw scores (logits): {logits.shape}\")\n",
    "print(f\"ğŸ² Probability predictions: {predictions.shape}\")\n",
    "print()\n",
    "print(\"ğŸ” Sample prediction (probabilities for 10 classes):\")\n",
    "print(predictions[0])  # First image's predictions\n",
    "print(f\"ğŸ“ˆ Probabilities sum to: {tf.reduce_sum(predictions[0]):.3f}\")\n",
    "print()\n",
    "\n",
    "# Find predicted class\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "print(f\"ğŸ† Predicted classes for all 5 images: {predicted_classes}\")\n",
    "print()\n",
    "print(\"ğŸ‰ SUCCESS! You've implemented a complete classification pipeline!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "takeaways"
   },
   "source": [
    "## ğŸ” KEY TAKEAWAYS\n",
    "\n",
    "### ğŸ§® **Mathematical Operations Mastery:**\n",
    "1. **Element-wise operations** work on corresponding elements (broadcasting magic)\n",
    "2. **Matrix multiplication** transforms data between layers (the core of neural networks)\n",
    "3. **Broadcasting** lets different shaped tensors work together efficiently\n",
    "4. **Reduction operations** summarize data (losses, statistics, attention)\n",
    "\n",
    "### ğŸ§  **Neural Network Applications:**\n",
    "- **Forward pass** = Chain of matrix multiplications + activations\n",
    "- **Bias addition** uses broadcasting for efficiency\n",
    "- **Normalization** uses element-wise operations and broadcasting\n",
    "- **Shape compatibility** is crucial for debugging\n",
    "\n",
    "### ğŸ’¡ **Pro Tips:**\n",
    "- Always check tensor shapes before operations\n",
    "- Use broadcasting to avoid explicit loops\n",
    "- Matrix multiplication: `(m,n) Ã— (n,k) = (m,k)`\n",
    "- Element-wise operations preserve shape\n",
    "\n",
    "### ğŸ¤” **Questions to Ponder:**\n",
    "- How would you implement batch normalization using these operations?\n",
    "- What happens to shapes during backpropagation?\n",
    "- How do attention mechanisms use these mathematical operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_exercise"
   },
   "source": [
    "## â¡ï¸ NEXT EXERCISE PREVIEW\n",
    "\n",
    "### ğŸ­ T3-Exercise-3: Activation Functions - The Soul of Neural Networks\n",
    "\n",
    "**Get ready to explore:**\n",
    "- ğŸª **ReLU, Sigmoid, Tanh** - The classic trio\n",
    "- âš¡ **Advanced activations** - Leaky ReLU, ELU, Swish\n",
    "- ğŸ¯ **Softmax** - The probability wizard\n",
    "- ğŸ“Š **Visualization** - See how activations shape learning\n",
    "- ğŸ§  **Applications** - When to use which activation\n",
    "\n",
    "ğŸ’« **Coming up:** Transform linear operations into intelligent behavior!\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ‰ EXERCISE 2 COMPLETED!\n",
    "## ğŸ§® **You've mastered the mathematics that powers AI!**\n",
    "### ğŸš€ **Ready to add intelligence with activation functions!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}