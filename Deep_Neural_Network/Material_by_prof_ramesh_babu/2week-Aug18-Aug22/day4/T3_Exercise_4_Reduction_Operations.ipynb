{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n**These materials are created by Prof. Ramesh Babu exclusively for M.Tech Students of SRM University**\n\n© 2025 Prof. Ramesh Babu. All rights reserved. This material is protected by copyright and may not be reproduced, distributed, or transmitted in any form or by any means without prior written permission.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 📊 T3-Exercise-4: Reduction Operations - Aggregating Intelligence\n",
    "**Deep Neural Network Architectures (21CSE558T) - Week 2, Day 4**  \n",
    "**M.Tech Lab Session - Duration: 30-45 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 LEARNING OBJECTIVES\n",
    "By the end of this exercise, you will:\n",
    "- 🔢 Master the **fundamental aggregators**: Sum, Mean, Max, Min\n",
    "- 📐 Understand **axis operations** and dimensional thinking\n",
    "- 📊 Apply **statistical operations**: Variance, Standard Deviation, Moments\n",
    "- 🧠 Build **real neural network components**: Loss functions, Attention, Normalization\n",
    "- 🎯 Create **practical applications**: Metrics, Batch Statistics, Feature Selection\n",
    "- 🔍 Debug **shape-related** reduction problems like a pro\n",
    "\n",
    "## 🔗 CONNECTION TO NEURAL NETWORKS\n",
    "Reduction operations are the **aggregation engines** of AI:\n",
    "- 📉 **Loss Functions** → Reduce prediction errors to single numbers\n",
    "- 📊 **Batch Normalization** → Aggregate statistics across batches\n",
    "- 🎯 **Attention Mechanisms** → Weighted aggregation of information\n",
    "- 📈 **Metrics & Evaluation** → Summarize model performance\n",
    "- 🔍 **Feature Selection** → Find most important activations\n",
    "\n",
    "**Mind-blowing insight:** Every neural network decision involves aggregating information! 🤯\n",
    "\n",
    "## 📚 PREREQUISITES\n",
    "- ✅ T3-Exercise-1 (Tensor Fundamentals)\n",
    "- ✅ T3-Exercise-2 (Mathematical Operations) \n",
    "- ✅ T3-Exercise-3 (Activation Functions)\n",
    "- 📐 Understanding of array dimensions and shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ⚙️ SETUP & AGGREGATION TOOLKIT\n",
    "🧮 Preparing our intelligence aggregation laboratory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": [
    "# 🧮 Complete toolkit for reduction operations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Set up beautiful visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "np.random.seed(42)  # For reproducible examples\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 🔧 Environment check\n",
    "print(\"📊 REDUCTION OPERATIONS LABORATORY\")\n",
    "print(\"=\" * 39)\n",
    "print(f\"🐍 Python: {sys.version.split()[0]}\")\n",
    "print(f\"🔥 TensorFlow: {tf.__version__}\")\n",
    "print(f\"🔢 NumPy: {np.__version__}\")\n",
    "print(f\"📊 Visualization: Ready for data insights!\")\n",
    "\n",
    "# 🎮 Computational readiness\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🚀 GPU: Ready for massive parallel reductions!\")\n",
    "else:\n",
    "    print(\"💻 CPU: Perfect for learning aggregation patterns!\")\n",
    "\n",
    "print(\"\\n🧮 Ready to aggregate intelligence!\\n\")\n",
    "\n",
    "# Helper function for beautiful tensor visualization\n",
    "def visualize_tensor_reduction(tensor, title, reduction_type=\"sum\", axis=None):\n",
    "    \"\"\"Create beautiful visualizations of tensor reductions\"\"\"\n",
    "    if len(tensor.shape) == 2:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Original tensor\n",
    "        im1 = axes[0].imshow(tensor, cmap='viridis', aspect='auto')\n",
    "        axes[0].set_title(f'Original Tensor\\n{tensor.shape}', fontweight='bold')\n",
    "        axes[0].set_xlabel('Columns')\n",
    "        axes[0].set_ylabel('Rows')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "        \n",
    "        # Add values to cells\n",
    "        for i in range(tensor.shape[0]):\n",
    "            for j in range(tensor.shape[1]):\n",
    "                axes[0].text(j, i, f'{tensor[i,j]:.1f}', ha='center', va='center', \n",
    "                           color='white' if tensor[i,j] < tf.reduce_mean(tensor) else 'black')\n",
    "        \n",
    "        if axis != 1:  # Row reduction (axis=0)\n",
    "            if reduction_type == \"sum\":\n",
    "                result = tf.reduce_sum(tensor, axis=0)\n",
    "            elif reduction_type == \"mean\":\n",
    "                result = tf.reduce_mean(tensor, axis=0)\n",
    "            elif reduction_type == \"max\":\n",
    "                result = tf.reduce_max(tensor, axis=0)\n",
    "            \n",
    "            axes[1].bar(range(len(result)), result, color='coral')\n",
    "            axes[1].set_title(f'{reduction_type.capitalize()} along Rows (axis=0)\\nResult shape: {result.shape}', fontweight='bold')\n",
    "            axes[1].set_xlabel('Column Index')\n",
    "            axes[1].set_ylabel(f'{reduction_type.capitalize()} Value')\n",
    "            \n",
    "            for i, v in enumerate(result):\n",
    "                axes[1].text(i, v + 0.1, f'{v:.1f}', ha='center', fontweight='bold')\n",
    "        \n",
    "        if axis != 0:  # Column reduction (axis=1)\n",
    "            if reduction_type == \"sum\":\n",
    "                result = tf.reduce_sum(tensor, axis=1)\n",
    "            elif reduction_type == \"mean\":\n",
    "                result = tf.reduce_mean(tensor, axis=1)\n",
    "            elif reduction_type == \"max\":\n",
    "                result = tf.reduce_max(tensor, axis=1)\n",
    "            \n",
    "            axes[2].barh(range(len(result)), result, color='lightgreen')\n",
    "            axes[2].set_title(f'{reduction_type.capitalize()} along Columns (axis=1)\\nResult shape: {result.shape}', fontweight='bold')\n",
    "            axes[2].set_ylabel('Row Index')\n",
    "            axes[2].set_xlabel(f'{reduction_type.capitalize()} Value')\n",
    "            axes[2].invert_yaxis()\n",
    "            \n",
    "            for i, v in enumerate(result):\n",
    "                axes[2].text(v + 0.1, i, f'{v:.1f}', va='center', fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(f'📊 {title}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"🎨 Visualization toolkit ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concepts"
   },
   "source": [
    "## 🧠 CORE CONCEPTS: The Art of Aggregation\n",
    "\n",
    "### 🎭 **What Are Reduction Operations?**\n",
    "\n",
    "**🔄 Simple Definition:**\n",
    "Reduction operations take a tensor and **\"squeeze\"** it along one or more dimensions, aggregating information into fewer numbers.\n",
    "\n",
    "**🎯 The Magic Formula:**\n",
    "```\n",
    "Many Numbers → Intelligent Aggregation → Fewer, More Meaningful Numbers\n",
    "```\n",
    "\n",
    "### 📐 **Understanding Axes (Dimensions):**\n",
    "\n",
    "**🎪 Think of a tensor as a theater:**\n",
    "- **Axis 0** (rows): Different audience members\n",
    "- **Axis 1** (columns): Different time moments\n",
    "- **Reducing axis 0**: \"What happened on average across all audience members?\"\n",
    "- **Reducing axis 1**: \"How did each person react over time?\"\n",
    "\n",
    "### 🧮 **The Aggregation Family:**\n",
    "\n",
    "1. **➕ Sum Family** (tf.reduce_sum)\n",
    "   - Adds everything up\n",
    "   - Use case: Total loss, feature importance\n",
    "\n",
    "2. **📊 Average Family** (tf.reduce_mean)\n",
    "   - Finds the typical value\n",
    "   - Use case: Batch statistics, performance metrics\n",
    "\n",
    "3. **🏆 Extremes Family** (tf.reduce_max, tf.reduce_min)\n",
    "   - Finds champions and laggards\n",
    "   - Use case: Max pooling, feature selection\n",
    "\n",
    "4. **📈 Statistical Family** (tf.reduce_std, tf.math.reduce_variance)\n",
    "   - Measures spread and variability\n",
    "   - Use case: Normalization, uncertainty quantification\n",
    "\n",
    "### 🎯 **Why Neural Networks LOVE Reductions:**\n",
    "- **Decision Making**: Aggregate evidence to make predictions\n",
    "- **Efficiency**: Compress information without losing essence\n",
    "- **Stability**: Average out noise and focus on signal\n",
    "- **Scalability**: Handle variable-sized inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_title"
   },
   "source": [
    "## ➕ STEP 1: The Sum Family - Adding It All Up\n",
    "### 🏗️ Building the foundation of aggregation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_basic_sum"
   },
   "outputs": [],
   "source": [
    "# ➕ Basic Sum Operations\n",
    "print(\"➕ THE SUM FAMILY: Adding Intelligence Together\")\n",
    "print(\"=\" * 46)\n",
    "\n",
    "# Create a sample tensor (like a batch of feature activations)\n",
    "sample_data = tf.constant([[1.0, 2.0, 3.0, 4.0],\n",
    "                          [5.0, 6.0, 7.0, 8.0],\n",
    "                          [9.0, 10.0, 11.0, 12.0]], dtype=tf.float32)\n",
    "\n",
    "print(\"🎲 Sample Data (imagine: 3 samples, 4 features each):\")\n",
    "print(sample_data)\n",
    "print(f\"📏 Shape: {sample_data.shape}\")\n",
    "print()\n",
    "\n",
    "# Different sum operations\n",
    "total_sum = tf.reduce_sum(sample_data)\n",
    "sum_axis0 = tf.reduce_sum(sample_data, axis=0)  # Sum across samples\n",
    "sum_axis1 = tf.reduce_sum(sample_data, axis=1)  # Sum across features\n",
    "\n",
    "print(\"🔢 Sum Operations Results:\")\n",
    "print(f\"   🌍 Total sum (all elements): {total_sum.numpy()}\")\n",
    "print(f\"   ⬇️ Sum axis=0 (across samples): {sum_axis0.numpy()}\")\n",
    "print(f\"      📊 Shape: {sum_axis0.shape} - One sum per feature\")\n",
    "print(f\"   ➡️ Sum axis=1 (across features): {sum_axis1.numpy()}\")\n",
    "print(f\"      📊 Shape: {sum_axis1.shape} - One sum per sample\")\n",
    "print()\n",
    "\n",
    "print(\"🧠 Neural Network Applications:\")\n",
    "print(\"   • Total sum: Overall activation magnitude\")\n",
    "print(\"   • Sum axis=0: Feature importance across batch\")\n",
    "print(\"   • Sum axis=1: Sample activation strength\")\n",
    "print()\n",
    "\n",
    "# Visualize the sum operation\n",
    "visualize_tensor_reduction(sample_data, \"Sum Operations Visualization\", \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_loss_function"
   },
   "outputs": [],
   "source": [
    "# 🎯 Practical Application: Building a Loss Function\n",
    "print(\"🎯 BUILDING A LOSS FUNCTION WITH SUMS\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Simulate predictions and true values\n",
    "predictions = tf.constant([[0.8, 0.1, 0.1],   # Sample 1: Confident in class 0\n",
    "                          [0.3, 0.6, 0.1],   # Sample 2: Confident in class 1  \n",
    "                          [0.2, 0.2, 0.6]])  # Sample 3: Confident in class 2\n",
    "\n",
    "true_labels = tf.constant([[1.0, 0.0, 0.0],   # Sample 1: Actually class 0 ✅\n",
    "                          [0.0, 1.0, 0.0],   # Sample 2: Actually class 1 ✅\n",
    "                          [0.0, 0.0, 1.0]])  # Sample 3: Actually class 2 ✅\n",
    "\n",
    "print(\"🎲 Classification Scenario:\")\n",
    "print(f\"   Predictions shape: {predictions.shape} (3 samples, 3 classes)\")\n",
    "print(f\"   True labels shape: {true_labels.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"📊 Sample-by-sample breakdown:\")\n",
    "class_names = ['🐱 Cat', '🐶 Dog', '🐦 Bird']\n",
    "for i in range(3):\n",
    "    pred_class = tf.argmax(predictions[i])\n",
    "    true_class = tf.argmax(true_labels[i])\n",
    "    print(f\"   Sample {i+1}: Predicted {class_names[pred_class]} | True {class_names[true_class]}\")\n",
    "print()\n",
    "\n",
    "# Calculate Mean Squared Error using reductions\n",
    "squared_errors = tf.square(predictions - true_labels)\n",
    "sample_losses = tf.reduce_sum(squared_errors, axis=1)  # Sum across classes for each sample\n",
    "total_loss = tf.reduce_mean(sample_losses)  # Average across samples\n",
    "\n",
    "print(\"📉 Loss Calculation Step-by-Step:\")\n",
    "print(f\"   1️⃣ Squared errors shape: {squared_errors.shape}\")\n",
    "print(f\"   2️⃣ Sample losses (sum per sample): {sample_losses.numpy()}\")\n",
    "print(f\"   3️⃣ Final loss (mean across samples): {total_loss.numpy():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"✨ What we learned:\")\n",
    "print(\"   • Sum reduces classes dimension (3) → (1) per sample\")\n",
    "print(\"   • Mean reduces samples dimension (3) → (1) total loss\")\n",
    "print(\"   • This is how neural networks measure their mistakes!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_title"
   },
   "source": [
    "## 📊 STEP 2: The Average Family - Finding the Typical\n",
    "### 🎯 The most important aggregation in machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_mean_operations"
   },
   "outputs": [],
   "source": [
    "# 📊 Mean Operations\n",
    "print(\"📊 THE MEAN FAMILY: Finding the Typical Value\")\n",
    "print(\"=\" * 44)\n",
    "\n",
    "# Create batch data (simulate a mini-batch from training)\n",
    "batch_size = 4\n",
    "feature_size = 5\n",
    "batch_data = tf.random.normal([batch_size, feature_size], mean=10, stddev=3)\n",
    "\n",
    "print(f\"🎲 Mini-batch Data ({batch_size} samples, {feature_size} features):\")\n",
    "print(batch_data)\n",
    "print()\n",
    "\n",
    "# Different mean operations\n",
    "global_mean = tf.reduce_mean(batch_data)\n",
    "feature_means = tf.reduce_mean(batch_data, axis=0)  # Mean per feature\n",
    "sample_means = tf.reduce_mean(batch_data, axis=1)   # Mean per sample\n",
    "\n",
    "print(\"📈 Mean Analysis:\")\n",
    "print(f\"   🌍 Global mean: {global_mean.numpy():.3f}\")\n",
    "print(f\"   📊 Feature means: {feature_means.numpy()}\")\n",
    "print(f\"      📏 Shape: {feature_means.shape} (one mean per feature)\")\n",
    "print(f\"   👤 Sample means: {sample_means.numpy()}\")\n",
    "print(f\"      📏 Shape: {sample_means.shape} (one mean per sample)\")\n",
    "print()\n",
    "\n",
    "print(\"🧠 Neural Network Insights:\")\n",
    "print(\"   • Feature means: Typical activation level per feature\")\n",
    "print(\"   • Sample means: Overall activation level per sample\")\n",
    "print(\"   • Critical for batch normalization!\")\n",
    "print()\n",
    "\n",
    "# Visualize mean operations\n",
    "visualize_tensor_reduction(batch_data, \"Mean Operations Visualization\", \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_batch_norm"
   },
   "outputs": [],
   "source": [
    "# 🔧 Practical Application: Batch Normalization\n",
    "print(\"🔧 BATCH NORMALIZATION: Using Means for Stability\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate activations from a layer (before normalization)\n",
    "raw_activations = tf.random.normal([8, 6], mean=15, stddev=5)  # 8 samples, 6 neurons\n",
    "\n",
    "print(\"⚡ Raw Layer Activations (before normalization):\")\n",
    "print(f\"   Shape: {raw_activations.shape}\")\n",
    "print(f\"   Global mean: {tf.reduce_mean(raw_activations).numpy():.3f}\")\n",
    "print(f\"   Global std: {tf.math.reduce_std(raw_activations).numpy():.3f}\")\n",
    "print()\n",
    "\n",
    "# Batch normalization implementation\n",
    "batch_mean = tf.reduce_mean(raw_activations, axis=0)  # Mean per feature\n",
    "batch_var = tf.math.reduce_variance(raw_activations, axis=0)  # Variance per feature\n",
    "batch_std = tf.sqrt(batch_var + 1e-8)  # Add epsilon for numerical stability\n",
    "\n",
    "# Normalize: (x - mean) / std\n",
    "normalized_activations = (raw_activations - batch_mean) / batch_std\n",
    "\n",
    "print(\"🔧 Batch Normalization Process:\")\n",
    "print(f\"   1️⃣ Batch means per feature: {batch_mean.numpy()}\")\n",
    "print(f\"   2️⃣ Batch stds per feature: {batch_std.numpy()}\")\n",
    "print()\n",
    "\n",
    "print(\"✨ After Normalization:\")\n",
    "print(f\"   📊 New global mean: {tf.reduce_mean(normalized_activations).numpy():.6f}\")\n",
    "print(f\"   📏 New global std: {tf.math.reduce_std(normalized_activations).numpy():.6f}\")\n",
    "print(f\"   📈 Feature means: {tf.reduce_mean(normalized_activations, axis=0).numpy()}\")\n",
    "print(f\"   📐 Feature stds: {tf.math.reduce_std(normalized_activations, axis=0).numpy()}\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Batch Normalization Benefits:\")\n",
    "print(\"   • Each feature now has mean ≈ 0, std ≈ 1\")\n",
    "print(\"   • Prevents internal covariate shift\")\n",
    "print(\"   • Allows higher learning rates\")\n",
    "print(\"   • Acts as regularization\")\n",
    "print()\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before normalization\n",
    "ax1.hist(raw_activations.numpy().flatten(), bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "ax1.set_title('🔴 Before Batch Normalization', fontweight='bold')\n",
    "ax1.set_xlabel('Activation Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(tf.reduce_mean(raw_activations), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {tf.reduce_mean(raw_activations).numpy():.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# After normalization\n",
    "ax2.hist(normalized_activations.numpy().flatten(), bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.set_title('🟢 After Batch Normalization', fontweight='bold')\n",
    "ax2.set_xlabel('Activation Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(tf.reduce_mean(normalized_activations), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {tf.reduce_mean(normalized_activations).numpy():.3f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle('📊 Batch Normalization: Before vs After', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_title"
   },
   "source": [
    "## 🏆 STEP 3: The Extremes Family - Finding Champions\n",
    "### 🔍 Max and Min operations for feature selection and pooling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_max_min"
   },
   "outputs": [],
   "source": [
    "# 🏆 Max and Min Operations\n",
    "print(\"🏆 THE EXTREMES FAMILY: Finding Champions and Laggards\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "# Create feature maps (like from a convolutional layer)\n",
    "feature_maps = tf.random.uniform([3, 4], 0, 10, dtype=tf.float32)\n",
    "feature_maps = tf.round(feature_maps)  # Round for cleaner display\n",
    "\n",
    "print(\"🗺️ Feature Maps (3×4 - like small CNN feature maps):\")\n",
    "print(feature_maps)\n",
    "print()\n",
    "\n",
    "# Different max/min operations\n",
    "global_max = tf.reduce_max(feature_maps)\n",
    "global_min = tf.reduce_min(feature_maps)\n",
    "max_per_row = tf.reduce_max(feature_maps, axis=1)  # Max in each row\n",
    "max_per_col = tf.reduce_max(feature_maps, axis=0)  # Max in each column\n",
    "min_per_row = tf.reduce_min(feature_maps, axis=1)  # Min in each row\n",
    "min_per_col = tf.reduce_min(feature_maps, axis=0)  # Min in each column\n",
    "\n",
    "print(\"🏅 Extreme Value Analysis:\")\n",
    "print(f\"   🥇 Global maximum: {global_max.numpy()}\")\n",
    "print(f\"   🥉 Global minimum: {global_min.numpy()}\")\n",
    "print(f\"   ➡️ Max per row: {max_per_row.numpy()}\")\n",
    "print(f\"   ⬇️ Max per column: {max_per_col.numpy()}\")\n",
    "print(f\"   ➡️ Min per row: {min_per_row.numpy()}\")\n",
    "print(f\"   ⬇️ Min per column: {min_per_col.numpy()}\")\n",
    "print()\n",
    "\n",
    "print(\"🧠 Neural Network Applications:\")\n",
    "print(\"   • Global max/min: Overall feature activation range\")\n",
    "print(\"   • Max pooling: Dimensionality reduction in CNNs\")\n",
    "print(\"   • Feature selection: Identify most/least active features\")\n",
    "print()\n",
    "\n",
    "# Visualize max operations\n",
    "visualize_tensor_reduction(feature_maps, \"Max Operations Visualization\", \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_max_pooling"
   },
   "outputs": [],
   "source": [
    "# 🏊‍♂️ Practical Application: Max Pooling Implementation\n",
    "print(\"🏊‍♂️ MAX POOLING: CNN's Dimension Reduction Hero\")\n",
    "print(\"=\" * 46)\n",
    "\n",
    "# Simulate a larger feature map\n",
    "feature_map = tf.random.uniform([6, 6], 0, 10, dtype=tf.float32)\n",
    "feature_map = tf.round(feature_map)  # Round for clarity\n",
    "\n",
    "print(\"🗺️ Original Feature Map (6×6):\")\n",
    "print(feature_map)\n",
    "print()\n",
    "\n",
    "# Implement 2×2 max pooling manually for educational purposes\n",
    "def manual_max_pool_2x2(tensor):\n",
    "    \"\"\"Manual 2x2 max pooling for educational visualization\"\"\"\n",
    "    h, w = tensor.shape\n",
    "    pooled = []\n",
    "    \n",
    "    print(\"🔍 2×2 Max Pooling Process:\")\n",
    "    for i in range(0, h, 2):\n",
    "        row = []\n",
    "        for j in range(0, w, 2):\n",
    "            # Extract 2x2 patch\n",
    "            patch = tensor[i:i+2, j:j+2]\n",
    "            max_val = tf.reduce_max(patch)\n",
    "            row.append(max_val)\n",
    "            \n",
    "            print(f\"   Patch [{i}:{i+2}, {j}:{j+2}]: {patch.numpy().flatten()} → Max: {max_val.numpy()}\")\n",
    "        pooled.append(row)\n",
    "    \n",
    "    return tf.stack([tf.stack(row) for row in pooled])\n",
    "\n",
    "# Apply max pooling\n",
    "pooled_result = manual_max_pool_2x2(feature_map)\n",
    "\n",
    "print(\"\\n🏊‍♂️ After 2×2 Max Pooling:\")\n",
    "print(pooled_result)\n",
    "print(f\"📏 Shape reduction: {feature_map.shape} → {pooled_result.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"✨ Max Pooling Benefits:\")\n",
    "print(\"   • Reduces spatial dimensions by 2×\")\n",
    "print(\"   • Keeps strongest activations (most important features)\")\n",
    "print(\"   • Provides translation invariance\")\n",
    "print(\"   • Reduces computational load\")\n",
    "print()\n",
    "\n",
    "# Visualize the pooling operation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original feature map\n",
    "im1 = ax1.imshow(feature_map, cmap='viridis', aspect='equal')\n",
    "ax1.set_title('🗺️ Original Feature Map (6×6)', fontweight='bold')\n",
    "ax1.set_xlabel('Width')\n",
    "ax1.set_ylabel('Height')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Add grid lines to show pooling regions\n",
    "for i in range(0, 7, 2):\n",
    "    ax1.axhline(i-0.5, color='red', linewidth=2)\n",
    "    ax1.axvline(i-0.5, color='red', linewidth=2)\n",
    "\n",
    "# Add values to cells\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        ax1.text(j, i, f'{feature_map[i,j]:.0f}', ha='center', va='center', \n",
    "               color='white', fontweight='bold')\n",
    "\n",
    "# Pooled result\n",
    "im2 = ax2.imshow(pooled_result, cmap='viridis', aspect='equal')\n",
    "ax2.set_title('🏊‍♂️ After Max Pooling (3×3)', fontweight='bold')\n",
    "ax2.set_xlabel('Width')\n",
    "ax2.set_ylabel('Height')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# Add values to pooled cells\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax2.text(j, i, f'{pooled_result[i,j]:.0f}', ha='center', va='center', \n",
    "               color='white', fontweight='bold')\n",
    "\n",
    "plt.suptitle('🏊‍♂️ Max Pooling: Keeping the Champions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_title"
   },
   "source": [
    "## 📈 STEP 4: Statistical Operations - Measuring Uncertainty\n",
    "### 🎲 Variance, Standard Deviation, and Advanced Statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_statistics"
   },
   "outputs": [],
   "source": [
    "# 📈 Statistical Reduction Operations\n",
    "print(\"📈 STATISTICAL FAMILY: Measuring Spread and Uncertainty\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create sample data with different distributions\n",
    "stable_data = tf.random.normal([5, 4], mean=5, stddev=0.5)  # Low variance\n",
    "volatile_data = tf.random.normal([5, 4], mean=5, stddev=2.0)  # High variance\n",
    "\n",
    "print(\"🎲 Two Different Datasets:\")\n",
    "print(f\"📊 Stable data (low variance):\\n{stable_data}\")\n",
    "print(f\"📊 Volatile data (high variance):\\n{volatile_data}\")\n",
    "print()\n",
    "\n",
    "# Calculate various statistics\n",
    "def analyze_statistics(data, name):\n",
    "    mean_val = tf.reduce_mean(data)\n",
    "    var_val = tf.math.reduce_variance(data)\n",
    "    std_val = tf.math.reduce_std(data)\n",
    "    max_val = tf.reduce_max(data)\n",
    "    min_val = tf.reduce_min(data)\n",
    "    \n",
    "    print(f\"📊 {name} Statistics:\")\n",
    "    print(f\"   📈 Mean: {mean_val.numpy():.3f}\")\n",
    "    print(f\"   📏 Variance: {var_val.numpy():.3f}\")\n",
    "    print(f\"   📐 Standard Deviation: {std_val.numpy():.3f}\")\n",
    "    print(f\"   🏆 Max: {max_val.numpy():.3f}\")\n",
    "    print(f\"   🥉 Min: {min_val.numpy():.3f}\")\n",
    "    print(f\"   📊 Range: {(max_val - min_val).numpy():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    return mean_val, var_val, std_val\n",
    "\n",
    "stable_stats = analyze_statistics(stable_data, \"Stable\")\n",
    "volatile_stats = analyze_statistics(volatile_data, \"Volatile\")\n",
    "\n",
    "print(\"🔍 Key Insights:\")\n",
    "print(f\"   • Stable data has std ≈ {stable_stats[2].numpy():.3f} (low uncertainty)\")\n",
    "print(f\"   • Volatile data has std ≈ {volatile_stats[2].numpy():.3f} (high uncertainty)\")\n",
    "print(\"   • Standard deviation measures prediction reliability!\")\n",
    "print()\n",
    "\n",
    "# Visualize the distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stable data distribution\n",
    "ax1.hist(stable_data.numpy().flatten(), bins=15, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.axvline(stable_stats[0], color='red', linestyle='--', linewidth=2, label=f'Mean: {stable_stats[0].numpy():.2f}')\n",
    "ax1.axvline(stable_stats[0] + stable_stats[2], color='orange', linestyle=':', linewidth=2, label=f'+1σ: {(stable_stats[0] + stable_stats[2]).numpy():.2f}')\n",
    "ax1.axvline(stable_stats[0] - stable_stats[2], color='orange', linestyle=':', linewidth=2, label=f'-1σ: {(stable_stats[0] - stable_stats[2]).numpy():.2f}')\n",
    "ax1.set_title(f'📊 Stable Data (σ={stable_stats[2].numpy():.3f})', fontweight='bold')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "# Volatile data distribution\n",
    "ax2.hist(volatile_data.numpy().flatten(), bins=15, alpha=0.7, color='red', edgecolor='black')\n",
    "ax2.axvline(volatile_stats[0], color='blue', linestyle='--', linewidth=2, label=f'Mean: {volatile_stats[0].numpy():.2f}')\n",
    "ax2.axvline(volatile_stats[0] + volatile_stats[2], color='orange', linestyle=':', linewidth=2, label=f'+1σ: {(volatile_stats[0] + volatile_stats[2]).numpy():.2f}')\n",
    "ax2.axvline(volatile_stats[0] - volatile_stats[2], color='orange', linestyle=':', linewidth=2, label=f'-1σ: {(volatile_stats[0] - volatile_stats[2]).numpy():.2f}')\n",
    "ax2.set_title(f'📊 Volatile Data (σ={volatile_stats[2].numpy():.3f})', fontweight='bold')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle('📈 Understanding Variance: Stable vs Volatile Data', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_uncertainty"
   },
   "outputs": [],
   "source": [
    "# 🎲 Practical Application: Uncertainty Quantification\n",
    "print(\"🎲 UNCERTAINTY QUANTIFICATION: AI Confidence Estimation\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Simulate multiple model predictions (like in an ensemble)\n",
    "n_models = 10\n",
    "n_samples = 5\n",
    "n_classes = 3\n",
    "\n",
    "# Each model makes predictions for 5 samples across 3 classes\n",
    "ensemble_predictions = tf.random.uniform([n_models, n_samples, n_classes], 0, 1)\n",
    "# Normalize to make them proper probabilities\n",
    "ensemble_predictions = tf.nn.softmax(ensemble_predictions, axis=-1)\n",
    "\n",
    "print(f\"🤖 Ensemble of {n_models} models predicting {n_samples} samples:\")\n",
    "print(f\"   Shape: {ensemble_predictions.shape}\")\n",
    "print()\n",
    "\n",
    "# Calculate ensemble statistics\n",
    "mean_predictions = tf.reduce_mean(ensemble_predictions, axis=0)  # Average across models\n",
    "std_predictions = tf.math.reduce_std(ensemble_predictions, axis=0)  # Uncertainty across models\n",
    "var_predictions = tf.math.reduce_variance(ensemble_predictions, axis=0)\n",
    "\n",
    "print(\"📊 Ensemble Analysis:\")\n",
    "print(\"Sample\\tClass\\tMean Prob\\tStd Dev\\tConfidence\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class_names = ['🐱 Cat', '🐶 Dog', '🐦 Bird']\n",
    "for sample_idx in range(n_samples):\n",
    "    for class_idx in range(n_classes):\n",
    "        mean_prob = mean_predictions[sample_idx, class_idx]\n",
    "        std_dev = std_predictions[sample_idx, class_idx]\n",
    "        \n",
    "        # High confidence = high probability + low std\n",
    "        confidence = \"High\" if mean_prob > 0.5 and std_dev < 0.1 else \"Low\" if std_dev > 0.2 else \"Med\"\n",
    "        \n",
    "        print(f\"{sample_idx+1}\\t{class_names[class_idx]}\\t{mean_prob.numpy():.3f}\\t\\t{std_dev.numpy():.3f}\\t{confidence}\")\n",
    "    print()\n",
    "\n",
    "# Find most confident predictions\n",
    "predicted_classes = tf.argmax(mean_predictions, axis=1)\n",
    "max_probs = tf.reduce_max(mean_predictions, axis=1)\n",
    "prediction_uncertainty = tf.reduce_mean(std_predictions, axis=1)  # Average uncertainty per sample\n",
    "\n",
    "print(\"🎯 Final Predictions with Confidence:\")\n",
    "print(\"Sample\\tPrediction\\tProb\\tUncertainty\\tStatus\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(n_samples):\n",
    "    pred_class = predicted_classes[i]\n",
    "    prob = max_probs[i]\n",
    "    uncertainty = prediction_uncertainty[i]\n",
    "    \n",
    "    if prob > 0.7 and uncertainty < 0.1:\n",
    "        status = \"✅ Confident\"\n",
    "    elif uncertainty > 0.2:\n",
    "        status = \"⚠️ Uncertain\"\n",
    "    else:\n",
    "        status = \"🤔 Moderate\"\n",
    "    \n",
    "    print(f\"{i+1}\\t{class_names[pred_class]}\\t{prob.numpy():.3f}\\t{uncertainty.numpy():.3f}\\t\\t{status}\")\n",
    "\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "print(\"   • Low standard deviation = High model agreement = High confidence\")\n",
    "print(\"   • High standard deviation = Model disagreement = Uncertainty\")\n",
    "print(\"   • This helps AI systems know when they don't know!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_title"
   },
   "source": [
    "## 🎯 STEP 5: Advanced Applications - Attention & Feature Selection\n",
    "### 🧠 Where reduction operations become the brain of AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_attention"
   },
   "outputs": [],
   "source": [
    "# 🎯 Attention Mechanism Implementation\n",
    "print(\"🎯 ATTENTION MECHANISM: Weighted Aggregation Intelligence\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Simulate sequence data (like words in a sentence)\n",
    "sequence_length = 6\n",
    "feature_dim = 4\n",
    "\n",
    "# Create sequence representations (like word embeddings)\n",
    "sequence = tf.random.normal([sequence_length, feature_dim], mean=0, stddev=1)\n",
    "words = ['🌟', '🚀', '🧠', '💡', '⚡', '🎯']  # Emoji representations\n",
    "\n",
    "print(f\"📝 Input Sequence ({sequence_length} tokens, {feature_dim} features each):\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"   {word} Token {i}: {sequence[i].numpy()}\")\n",
    "print()\n",
    "\n",
    "# Calculate attention scores (simplified self-attention)\n",
    "query = sequence[2]  # Let's focus on the 3rd token (🧠)\n",
    "print(f\"🔍 Query Token: {words[2]} {query.numpy()}\")\n",
    "print()\n",
    "\n",
    "# Calculate attention scores using dot product\n",
    "attention_scores = tf.reduce_sum(sequence * query, axis=1)  # Dot product with query\n",
    "attention_weights = tf.nn.softmax(attention_scores)  # Convert to probabilities\n",
    "\n",
    "print(\"🎭 Attention Analysis:\")\n",
    "print(\"Token\\tScore\\tWeight\\tAttention\")\n",
    "print(\"-\" * 35)\n",
    "for i, (word, score, weight) in enumerate(zip(words, attention_scores, attention_weights)):\n",
    "    attention_level = \"🔥\" if weight > 0.25 else \"⚡\" if weight > 0.15 else \"💫\"\n",
    "    print(f\"{word}\\t{score.numpy():.3f}\\t{weight.numpy():.3f}\\t{attention_level}\")\n",
    "\n",
    "print()\n",
    "print(f\"✅ Attention weights sum: {tf.reduce_sum(attention_weights).numpy():.6f}\")\n",
    "print()\n",
    "\n",
    "# Calculate attended representation (weighted sum)\n",
    "attended_representation = tf.reduce_sum(\n",
    "    sequence * tf.expand_dims(attention_weights, 1),  # Broadcast weights\n",
    "    axis=0  # Sum across sequence length\n",
    ")\n",
    "\n",
    "print(\"🧠 Attention Output:\")\n",
    "print(f\"   📊 Original query: {query.numpy()}\")\n",
    "print(f\"   ✨ Attended repr: {attended_representation.numpy()}\")\n",
    "print()\n",
    "\n",
    "print(\"💡 What Just Happened?\")\n",
    "print(\"   • Each token got an attention score based on similarity to query\")\n",
    "print(\"   • Scores converted to probabilities (softmax)\")\n",
    "print(\"   • Final representation = weighted sum of all tokens\")\n",
    "print(\"   • This is how transformers focus on relevant information!\")\n",
    "print()\n",
    "\n",
    "# Visualize attention weights\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Attention weights visualization\n",
    "bars = ax1.bar(words, attention_weights.numpy(), color='skyblue', edgecolor='navy')\n",
    "ax1.set_title(f'🎯 Attention Weights for Query {words[2]}', fontweight='bold')\n",
    "ax1.set_ylabel('Attention Weight')\n",
    "ax1.set_xlabel('Tokens')\n",
    "\n",
    "# Highlight the query token\n",
    "bars[2].set_color('orange')\n",
    "bars[2].set_edgecolor('red')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, weight) in enumerate(zip(bars, attention_weights)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{weight.numpy():.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Attention heatmap\n",
    "attention_matrix = tf.expand_dims(attention_weights, 0)\n",
    "im = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
    "ax2.set_title('🔥 Attention Heatmap', fontweight='bold')\n",
    "ax2.set_xticks(range(len(words)))\n",
    "ax2.set_xticklabels(words)\n",
    "ax2.set_yticks([0])\n",
    "ax2.set_yticklabels([f'Query {words[2]}'])\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.suptitle('🎯 Attention Mechanism: How AI Focuses', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_feature_selection"
   },
   "outputs": [],
   "source": [
    "# 🔍 Feature Selection Using Reduction Operations\n",
    "print(\"🔍 FEATURE SELECTION: Finding the Most Important Features\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Simulate high-dimensional data (like gene expression or image features)\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "feature_data = tf.random.normal([n_samples, n_features], mean=0, stddev=1)\n",
    "\n",
    "# Add some informative features (simulate signal)\n",
    "# Make features 3, 7, 12, 18 more informative\n",
    "informative_indices = [3, 7, 12, 18]\n",
    "for idx in informative_indices:\n",
    "    feature_data = tf.tensor_scatter_nd_add(\n",
    "        feature_data, \n",
    "        [[i, idx] for i in range(n_samples)],\n",
    "        tf.random.normal([n_samples], mean=2, stddev=0.5)\n",
    "    )\n",
    "\n",
    "print(f\"🎲 Dataset: {n_samples} samples × {n_features} features\")\n",
    "print(f\"🎯 Informative features (with signal): {informative_indices}\")\n",
    "print()\n",
    "\n",
    "# Feature importance analysis using various reduction metrics\n",
    "feature_means = tf.reduce_mean(tf.abs(feature_data), axis=0)  # Mean absolute values\n",
    "feature_stds = tf.math.reduce_std(feature_data, axis=0)       # Standard deviations\n",
    "feature_max = tf.reduce_max(tf.abs(feature_data), axis=0)     # Maximum absolute values\n",
    "feature_variance = tf.math.reduce_variance(feature_data, axis=0)  # Variances\n",
    "\n",
    "print(\"📊 Feature Importance Analysis:\")\n",
    "print(\"Feature\\tMean|x|\\tStd Dev\\tMax|x|\\tVariance\\tScore\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Combined importance score\n",
    "importance_scores = feature_means * feature_stds  # Simple combination\n",
    "\n",
    "for i in range(n_features):\n",
    "    marker = \"🎯\" if i in informative_indices else \"📊\"\n",
    "    print(f\"{marker} {i:2d}\\t{feature_means[i].numpy():.3f}\\t{feature_stds[i].numpy():.3f}\\t{feature_max[i].numpy():.3f}\\t{feature_variance[i].numpy():.3f}\\t{importance_scores[i].numpy():.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Rank features by importance\n",
    "top_k = 5\n",
    "top_indices = tf.nn.top_k(importance_scores, k=top_k).indices\n",
    "\n",
    "print(f\"🏆 Top {top_k} Most Important Features:\")\n",
    "for rank, idx in enumerate(top_indices.numpy()):\n",
    "    medal = \"🥇\" if rank == 0 else \"🥈\" if rank == 1 else \"🥉\" if rank == 2 else \"🏅\"\n",
    "    signal = \"✅ SIGNAL\" if idx in informative_indices else \"❌ noise\"\n",
    "    print(f\"   {medal} Rank {rank+1}: Feature {idx} (score: {importance_scores[idx].numpy():.3f}) - {signal}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculate detection accuracy\n",
    "detected_informative = sum(1 for idx in top_indices.numpy() if idx in informative_indices)\n",
    "detection_rate = detected_informative / len(informative_indices)\n",
    "\n",
    "print(f\"🎯 Feature Selection Performance:\")\n",
    "print(f\"   Detected {detected_informative}/{len(informative_indices)} informative features\")\n",
    "print(f\"   Detection rate: {detection_rate:.2%}\")\n",
    "print()\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Feature importance scores\n",
    "colors = ['red' if i in informative_indices else 'lightblue' for i in range(n_features)]\n",
    "bars = ax1.bar(range(n_features), importance_scores.numpy(), color=colors, edgecolor='black')\n",
    "ax1.set_title('🔍 Feature Importance Scores', fontweight='bold')\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('Importance Score')\n",
    "ax1.axhline(tf.reduce_mean(importance_scores), color='green', linestyle='--', \n",
    "           label=f'Mean: {tf.reduce_mean(importance_scores).numpy():.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Highlight top features\n",
    "for idx in top_indices.numpy():\n",
    "    bars[idx].set_edgecolor('gold')\n",
    "    bars[idx].set_linewidth(3)\n",
    "\n",
    "# Feature distribution comparison\n",
    "informative_data = tf.gather(feature_data, informative_indices, axis=1)\n",
    "noise_indices = [i for i in range(n_features) if i not in informative_indices]\n",
    "noise_data = tf.gather(feature_data, noise_indices[:4], axis=1)  # Sample 4 noise features\n",
    "\n",
    "ax2.hist(informative_data.numpy().flatten(), bins=30, alpha=0.7, label='🎯 Informative Features', \n",
    "         color='red', density=True)\n",
    "ax2.hist(noise_data.numpy().flatten(), bins=30, alpha=0.7, label='📊 Noise Features', \n",
    "         color='lightblue', density=True)\n",
    "ax2.set_title('🎲 Feature Value Distributions', fontweight='bold')\n",
    "ax2.set_xlabel('Feature Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Feature Selection Insights:\")\n",
    "print(\"   • Reduction operations help identify patterns in high-dimensional data\")\n",
    "print(\"   • Mean and std dev together capture signal strength\")\n",
    "print(\"   • This is the foundation of feature engineering in ML!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation_title"
   },
   "source": [
    "## ✅ PRACTICAL VALIDATION & DEBUGGING\n",
    "### 🔧 Master the common challenges and debug like a pro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_debugging"
   },
   "outputs": [],
   "source": [
    "# 🔧 Reduction Operations Debugging Challenge\n",
    "print(\"🔧 DEBUGGING CHALLENGE: Common Reduction Pitfalls\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create test scenarios that often confuse students\n",
    "test_tensor = tf.random.normal([3, 4, 5])  # 3D tensor\n",
    "\n",
    "print(f\"🎲 Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(\"   Think: (batch_size=3, height=4, width=5)\")\n",
    "print()\n",
    "\n",
    "# Test different reduction scenarios\n",
    "scenarios = [\n",
    "    (\"No axis specified\", lambda x: tf.reduce_sum(x)),\n",
    "    (\"Axis=0 (across batches)\", lambda x: tf.reduce_sum(x, axis=0)),\n",
    "    (\"Axis=1 (across height)\", lambda x: tf.reduce_sum(x, axis=1)),\n",
    "    (\"Axis=2 (across width)\", lambda x: tf.reduce_sum(x, axis=2)),\n",
    "    (\"Axis=[0,1] (across batch and height)\", lambda x: tf.reduce_sum(x, axis=[0,1])),\n",
    "    (\"Axis=[1,2] (across height and width)\", lambda x: tf.reduce_sum(x, axis=[1,2])),\n",
    "    (\"Keep dims (axis=1, keepdims=True)\", lambda x: tf.reduce_sum(x, axis=1, keepdims=True))\n",
    "]\n",
    "\n",
    "print(\"🧪 Reduction Scenarios Analysis:\")\n",
    "print(\"Description\\t\\t\\tResult Shape\\tDimensions Explanation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for desc, operation in scenarios:\n",
    "    try:\n",
    "        result = operation(test_tensor)\n",
    "        explanation = \"\"\n",
    "        if len(result.shape) == 0:\n",
    "            explanation = \"Scalar (all dimensions reduced)\"\n",
    "        elif len(result.shape) == 1:\n",
    "            explanation = f\"1D vector of length {result.shape[0]}\"\n",
    "        elif len(result.shape) == 2:\n",
    "            explanation = f\"2D matrix {result.shape[0]}×{result.shape[1]}\"\n",
    "        elif len(result.shape) == 3:\n",
    "            explanation = f\"3D tensor {result.shape[0]}×{result.shape[1]}×{result.shape[2]}\"\n",
    "        \n",
    "        print(f\"{desc:<30}\\t{str(result.shape):<12}\\t{explanation}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{desc:<30}\\tERROR\\t\\t{str(e)[:50]}...\")\n",
    "\n",
    "print()\n",
    "print(\"🎯 Key Debugging Tips:\")\n",
    "print(\"   • axis=None → Reduces ALL dimensions to scalar\")\n",
    "print(\"   • axis=0 → Reduces first dimension\")\n",
    "print(\"   • axis=-1 → Reduces last dimension\")\n",
    "print(\"   • axis=[0,1] → Reduces multiple dimensions\")\n",
    "print(\"   • keepdims=True → Keeps dimensions as size 1\")\n",
    "print()\n",
    "\n",
    "# Common mistake demonstration\n",
    "print(\"⚠️ COMMON MISTAKE DEMONSTRATION:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Simulating a common batch processing error\n",
    "batch_predictions = tf.random.uniform([10, 5], 0, 1)  # 10 samples, 5 classes\n",
    "batch_predictions = tf.nn.softmax(batch_predictions)\n",
    "\n",
    "print(f\"📊 Batch predictions shape: {batch_predictions.shape}\")\n",
    "print()\n",
    "\n",
    "# Wrong way (reduces wrong axis)\n",
    "wrong_avg = tf.reduce_mean(batch_predictions, axis=1)  # Average across classes\n",
    "print(f\"❌ WRONG: Mean across classes (axis=1): {wrong_avg.shape}\")\n",
    "print(f\"   Result: {wrong_avg.numpy()[:5]}  # First 5 values\")\n",
    "print(\"   This gives average probability per sample (not useful!)\")\n",
    "print()\n",
    "\n",
    "# Right way (reduces correct axis)\n",
    "right_avg = tf.reduce_mean(batch_predictions, axis=0)  # Average across samples\n",
    "print(f\"✅ CORRECT: Mean across samples (axis=0): {right_avg.shape}\")\n",
    "print(f\"   Result: {right_avg.numpy()}\")\n",
    "print(\"   This gives average probability per class (useful!)\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Remember: Always think about WHAT you're averaging!\")\n",
    "print(\"   • Across samples (axis=0): Gets typical behavior per feature\")\n",
    "print(\"   • Across features (axis=1): Gets summary per sample\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_complete"
   },
   "outputs": [],
   "source": [
    "# 🎉 Complete Neural Network Pipeline with Reductions\n",
    "print(\"🎉 COMPLETE PIPELINE: Neural Network with All Reduction Types\")\n",
    "print(\"=\" * 63)\n",
    "\n",
    "# Simulate end-to-end training scenario\n",
    "batch_size = 8\n",
    "input_features = 10\n",
    "hidden_size = 6\n",
    "output_classes = 3\n",
    "\n",
    "# Generate sample data\n",
    "inputs = tf.random.normal([batch_size, input_features])\n",
    "true_labels = tf.random.uniform([batch_size], 0, output_classes, dtype=tf.int32)\n",
    "true_labels_onehot = tf.one_hot(true_labels, output_classes)\n",
    "\n",
    "# Simple neural network\n",
    "W1 = tf.Variable(tf.random.normal([input_features, hidden_size], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_size, output_classes], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([output_classes]))\n",
    "\n",
    "print(f\"🏗️ Network Architecture: {input_features} → {hidden_size} → {output_classes}\")\n",
    "print(f\"📊 Batch size: {batch_size}\")\n",
    "print()\n",
    "\n",
    "# Forward pass with reduction operations\n",
    "print(\"🚀 Forward Pass with Reduction Operations:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Layer 1\n",
    "hidden_pre = tf.matmul(inputs, W1) + b1\n",
    "hidden_post = tf.nn.relu(hidden_pre)\n",
    "\n",
    "# Batch statistics (like in batch normalization)\n",
    "hidden_mean = tf.reduce_mean(hidden_post, axis=0)\n",
    "hidden_std = tf.math.reduce_std(hidden_post, axis=0)\n",
    "sparsity = tf.reduce_mean(tf.cast(hidden_post == 0, tf.float32))\n",
    "\n",
    "print(f\"🧠 Hidden Layer Analysis:\")\n",
    "print(f\"   📊 Mean activations per neuron: {hidden_mean.numpy()}\")\n",
    "print(f\"   📏 Std per neuron: {hidden_std.numpy()}\")\n",
    "print(f\"   🔥 Sparsity (% zeros): {sparsity.numpy():.2%}\")\n",
    "print()\n",
    "\n",
    "# Layer 2 + Output\n",
    "logits = tf.matmul(hidden_post, W2) + b2\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# Loss calculation using reductions\n",
    "cross_entropy = -tf.reduce_sum(true_labels_onehot * tf.math.log(predictions + 1e-8), axis=1)\n",
    "total_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Accuracy calculation\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "correct_predictions = tf.cast(tf.equal(predicted_classes, tf.cast(true_labels, tf.int64)), tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_predictions)\n",
    "\n",
    "print(f\"📉 Loss & Metrics:\")\n",
    "print(f\"   💔 Individual losses: {cross_entropy.numpy()}\")\n",
    "print(f\"   📉 Total loss (mean): {total_loss.numpy():.4f}\")\n",
    "print(f\"   🎯 Accuracy: {accuracy.numpy():.2%}\")\n",
    "print()\n",
    "\n",
    "# Advanced analytics using reductions\n",
    "confidence_scores = tf.reduce_max(predictions, axis=1)  # Highest probability per sample\n",
    "entropy_scores = -tf.reduce_sum(predictions * tf.math.log(predictions + 1e-8), axis=1)  # Uncertainty\n",
    "\n",
    "print(f\"🔍 Prediction Analysis:\")\n",
    "print(\"Sample\\tTrue\\tPred\\tConfidence\\tEntropy\\tStatus\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(batch_size):\n",
    "    confidence = confidence_scores[i]\n",
    "    entropy = entropy_scores[i]\n",
    "    correct = \"✅\" if correct_predictions[i] == 1 else \"❌\"\n",
    "    \n",
    "    status = \"🔥\" if confidence > 0.8 else \"⚡\" if confidence > 0.5 else \"💫\"\n",
    "    \n",
    "    print(f\"{i+1}\\t{true_labels[i].numpy()}\\t{predicted_classes[i].numpy()}\\t{confidence.numpy():.3f}\\t\\t{entropy.numpy():.3f}\\t{correct}{status}\")\n",
    "\n",
    "print()\n",
    "print(f\"📊 Batch Statistics:\")\n",
    "print(f\"   📈 Mean confidence: {tf.reduce_mean(confidence_scores).numpy():.3f}\")\n",
    "print(f\"   🎲 Mean entropy: {tf.reduce_mean(entropy_scores).numpy():.3f}\")\n",
    "print(f\"   🏆 Max confidence: {tf.reduce_max(confidence_scores).numpy():.3f}\")\n",
    "print(f\"   🤔 Min confidence: {tf.reduce_min(confidence_scores).numpy():.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"🎉 REDUCTION OPERATIONS MASTERY ACHIEVED!\")\n",
    "print(\"You've seen how reductions power:\")\n",
    "print(\"   ➕ Loss functions (mean across samples)\")\n",
    "print(\"   📊 Batch normalization (mean/std per feature)\")\n",
    "print(\"   🏆 Max pooling (max across spatial dimensions)\")\n",
    "print(\"   🎯 Attention (weighted sum across sequence)\")\n",
    "print(\"   🔍 Feature selection (importance scores)\")\n",
    "print(\"   📈 Metrics and analytics (mean, max, min)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "takeaways"
   },
   "source": [
    "## 🔍 KEY TAKEAWAYS\n",
    "\n",
    "### 📊 **Reduction Operations Mastery:**\n",
    "\n",
    "1. **➕ Sum Family** - Building blocks of aggregation\n",
    "   - `tf.reduce_sum()` - Adds everything up\n",
    "   - Perfect for loss functions and feature importance\n",
    "   - Axis parameter controls which dimensions to reduce\n",
    "\n",
    "2. **📊 Mean Family** - Finding the typical\n",
    "   - `tf.reduce_mean()` - Most important aggregation in ML\n",
    "   - Essential for batch normalization and metrics\n",
    "   - Provides stability and noise reduction\n",
    "\n",
    "3. **🏆 Extremes Family** - Champions and selection\n",
    "   - `tf.reduce_max()`, `tf.reduce_min()` - Find extremes\n",
    "   - Power max pooling in CNNs\n",
    "   - Enable feature selection and sparsity\n",
    "\n",
    "4. **📈 Statistical Family** - Measuring uncertainty\n",
    "   - `tf.math.reduce_std()`, `tf.math.reduce_variance()` - Quantify spread\n",
    "   - Critical for normalization and uncertainty estimation\n",
    "   - Help AI systems know when they don't know\n",
    "\n",
    "### 🧠 **Neural Network Applications:**\n",
    "- **Loss Functions**: Aggregate errors across samples and features\n",
    "- **Batch Normalization**: Stabilize training with mean/std statistics\n",
    "- **Attention Mechanisms**: Weighted aggregation of information\n",
    "- **Feature Selection**: Identify most important patterns\n",
    "- **Max Pooling**: Dimensionality reduction with information preservation\n",
    "- **Metrics**: Performance evaluation and monitoring\n",
    "\n",
    "### 💡 **Axis Understanding (Critical!):**\n",
    "- **axis=None**: Reduce all dimensions → scalar\n",
    "- **axis=0**: Reduce across samples → per-feature statistics\n",
    "- **axis=1**: Reduce across features → per-sample statistics\n",
    "- **axis=[0,1]**: Reduce multiple dimensions\n",
    "- **keepdims=True**: Maintain dimensionality structure\n",
    "\n",
    "### 🔧 **Debugging Wisdom:**\n",
    "- Always verify shapes before and after reductions\n",
    "- Think about what you're aggregating (samples vs features)\n",
    "- Use reductions to convert tensors to meaningful summaries\n",
    "- Combine multiple reduction types for comprehensive analysis\n",
    "\n",
    "### 🤔 **Advanced Questions:**\n",
    "- How do different reduction operations affect gradient flow?\n",
    "- When should you use keepdims=True vs False?\n",
    "- How can reductions help with model interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_exercise"
   },
   "source": [
    "## ➡️ NEXT EXERCISE PREVIEW\n",
    "\n",
    "### 🚀 T3-Exercise-5: Neural Network Forward Pass - Connecting It All\n",
    "\n",
    "**Get ready to build:**\n",
    "- 🏗️ **Complete Neural Networks** - From inputs to predictions\n",
    "- 🔄 **Multi-layer architectures** - Stacking the building blocks\n",
    "- 🎯 **Real classification tasks** - Solving actual problems\n",
    "- 📊 **End-to-end pipelines** - Data → Processing → Decisions\n",
    "- 🧠 **Architecture design** - Choosing layers, activations, and operations\n",
    "\n",
    "🌟 **Coming up:** Combine tensors, operations, activations, and reductions into intelligent systems!\n",
    "\n",
    "---\n",
    "\n",
    "# 🎉 EXERCISE 4 COMPLETED!\n",
    "## 📊 **You've mastered the aggregation engines of AI!**\n",
    "### 🧮 **You understand how neural networks summarize and decide!**\n",
    "#### 🚀 **Ready to build complete intelligent systems!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}