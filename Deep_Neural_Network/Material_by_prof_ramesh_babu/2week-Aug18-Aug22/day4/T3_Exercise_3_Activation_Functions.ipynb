{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n**These materials are created by Prof. Ramesh Babu exclusively for M.Tech Students of SRM University**\n\n© 2025 Prof. Ramesh Babu. All rights reserved. This material is protected by copyright and may not be reproduced, distributed, or transmitted in any form or by any means without prior written permission.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🎭 T3-Exercise-3: Activation Functions - The Soul of Neural Networks\n",
    "**Deep Neural Network Architectures (21CSE558T) - Week 2, Day 4**  \n",
    "**M.Tech Lab Session - Duration: 30-45 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 LEARNING OBJECTIVES\n",
    "By the end of this exercise, you will:\n",
    "- 🎪 Master the classic activation trinity: **ReLU, Sigmoid, Tanh**\n",
    "- ⚡ Explore advanced activations: **Leaky ReLU, ELU, Swish, GELU**\n",
    "- 🎲 Understand **Softmax** - the probability wizard\n",
    "- 📊 **Visualize** activation behaviors and their impact\n",
    "- 🧠 Choose the **right activation** for different scenarios\n",
    "- 🔥 Apply activations in **real neural network** contexts\n",
    "\n",
    "## 🔗 CONNECTION TO NEURAL NETWORKS\n",
    "Activation functions are the **soul** of neural networks:\n",
    "- 🎭 **Without activations** → Just linear transformations (boring!)\n",
    "- ✨ **With activations** → Universal function approximators (magic!)\n",
    "- 🧬 **They introduce non-linearity** → Enable complex pattern learning\n",
    "- 🎯 **Different tasks need different activations** → Classification vs Regression\n",
    "\n",
    "**Mind-blowing fact:** A neural network without activation functions is just a glorified linear regression! 🤯\n",
    "\n",
    "## 📚 PREREQUISITES\n",
    "- ✅ Completed T3-Exercise-1 (Tensor Fundamentals)\n",
    "- ✅ Completed T3-Exercise-2 (Mathematical Operations)\n",
    "- 📈 Basic understanding of function graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ⚙️ SETUP & ACTIVATION TOOLKIT\n",
    "🎨 Let's prepare our visualization and computation tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": [
    "# 🎨 Complete toolkit for activation function exploration\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 🔧 Environment check\n",
    "print(\"🎭 ACTIVATION FUNCTION LABORATORY\")\n",
    "print(\"=\" * 38)\n",
    "print(f\"🐍 Python: {sys.version.split()[0]}\")\n",
    "print(f\"🔥 TensorFlow: {tf.__version__}\")\n",
    "print(f\"🔢 NumPy: {np.__version__}\")\n",
    "print(f\"📊 Matplotlib: Ready for beautiful plots!\")\n",
    "\n",
    "# 🎮 Computational power check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🚀 GPU: Ready for lightning-fast activations!\")\n",
    "else:\n",
    "    print(\"💻 CPU: Perfect for learning and visualization!\")\n",
    "\n",
    "print(\"\\n🎨 Ready to bring neural networks to life!\\n\")\n",
    "\n",
    "# Helper function for beautiful plots\n",
    "def plot_activation(x, y, title, color='blue', derivative=None):\n",
    "    \"\"\"Create beautiful activation function plots\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if derivative is not None:\n",
    "        plt.subplot(1, 2, 1)\n",
    "    \n",
    "    plt.plot(x, y, color=color, linewidth=3, label=f'{title}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('Input (x)', fontsize=12)\n",
    "    plt.ylabel('Output f(x)', fontsize=12)\n",
    "    plt.title(f'🎭 {title} Activation Function', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    if derivative is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, derivative, color='red', linewidth=3, label=f\"{title} Derivative\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlabel('Input (x)', fontsize=12)\n",
    "        plt.ylabel(\"Derivative f'(x)\", fontsize=12)\n",
    "        plt.title(f'📈 {title} Gradient', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"🎨 Plotting toolkit ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concepts"
   },
   "source": [
    "## 🧠 CORE CONCEPTS: Why Activation Functions Matter\n",
    "\n",
    "### 🎭 **The Drama of Non-linearity:**\n",
    "\n",
    "**🎬 Scene 1: Life Without Activations**\n",
    "```\n",
    "Layer1: input × W1 + b1 = linear_output1\n",
    "Layer2: linear_output1 × W2 + b2 = linear_output2\n",
    "Result: Still just one big linear transformation! 😴\n",
    "```\n",
    "\n",
    "**🎪 Scene 2: Life With Activations**\n",
    "```\n",
    "Layer1: activation(input × W1 + b1) = non_linear_output1\n",
    "Layer2: activation(non_linear_output1 × W2 + b2) = complex_patterns! ✨\n",
    "Result: Universal function approximation! 🚀\n",
    "```\n",
    "\n",
    "### 🎯 **Activation Function Categories:**\n",
    "\n",
    "1. **🔥 Rectified Family** (ReLU, Leaky ReLU, ELU)\n",
    "   - Sparse activation (many zeros)\n",
    "   - Computationally efficient\n",
    "   - Great for hidden layers\n",
    "\n",
    "2. **🌊 Smooth Functions** (Sigmoid, Tanh)\n",
    "   - Smooth gradients everywhere\n",
    "   - Bounded outputs\n",
    "   - Classic but can saturate\n",
    "\n",
    "3. **🎲 Probability Functions** (Softmax)\n",
    "   - Outputs sum to 1\n",
    "   - Perfect for classification\n",
    "   - Converts scores to probabilities\n",
    "\n",
    "4. **⚡ Modern Innovations** (Swish, GELU, Mish)\n",
    "   - State-of-the-art performance\n",
    "   - Self-gating properties\n",
    "   - Used in transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_title"
   },
   "source": [
    "## 🔥 STEP 1: The Rectified Family - ReLU & Friends\n",
    "### 💪 The workhorses of modern deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_relu"
   },
   "outputs": [],
   "source": [
    "# 🔥 ReLU - The Game Changer\n",
    "print(\"🔥 ReLU: RECTIFIED LINEAR UNIT\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Create input range for visualization\n",
    "x = tf.linspace(-5.0, 5.0, 1000)\n",
    "\n",
    "# ReLU function: f(x) = max(0, x)\n",
    "relu_output = tf.nn.relu(x)\n",
    "\n",
    "print(\"📝 Mathematical Definition: f(x) = max(0, x)\")\n",
    "print(\"🎯 Key Properties:\")\n",
    "print(\"   • Simple and fast computation\")\n",
    "print(\"   • Sparse activation (50% neurons typically inactive)\")\n",
    "print(\"   • No saturation for positive values\")\n",
    "print(\"   • Gradient is either 0 or 1\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "test_values = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "relu_test = tf.nn.relu(test_values)\n",
    "\n",
    "print(\"🧪 Test Values:\")\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"   ReLU({test_values[i].numpy():4.1f}) = {relu_test[i].numpy():4.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"🧠 Neural Network Impact:\")\n",
    "print(\"   • Solves vanishing gradient problem\")\n",
    "print(\"   • Creates sparse representations\")\n",
    "print(\"   • Enables very deep networks\")\n",
    "print()\n",
    "\n",
    "# Visualize ReLU\n",
    "plot_activation(x.numpy(), relu_output.numpy(), \"ReLU\", color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_leaky_relu"
   },
   "outputs": [],
   "source": [
    "# ⚡ Leaky ReLU - The Improved Version\n",
    "print(\"⚡ LEAKY ReLU: Never Completely Dead!\")\n",
    "print(\"=\" * 39)\n",
    "\n",
    "# Leaky ReLU: f(x) = max(αx, x) where α is small (e.g., 0.01)\n",
    "alpha = 0.01\n",
    "leaky_relu_output = tf.nn.leaky_relu(x, alpha=alpha)\n",
    "\n",
    "print(f\"📝 Mathematical Definition: f(x) = max({alpha}x, x)\")\n",
    "print(\"🎯 Key Advantages:\")\n",
    "print(\"   • Prevents 'dying ReLU' problem\")\n",
    "print(\"   • Small gradient for negative values\")\n",
    "print(\"   • All neurons can contribute to learning\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "leaky_test = tf.nn.leaky_relu(test_values, alpha=alpha)\n",
    "\n",
    "print(\"🧪 Comparison with ReLU:\")\n",
    "print(\"Input\\tReLU\\tLeaky ReLU\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"{test_values[i].numpy():4.1f}\\t{relu_test[i].numpy():4.1f}\\t{leaky_test[i].numpy():8.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"🎨 Visual difference: Notice the small slope for negative values!\")\n",
    "\n",
    "# Visualize Leaky ReLU\n",
    "plot_activation(x.numpy(), leaky_relu_output.numpy(), \"Leaky ReLU\", color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step1_elu"
   },
   "outputs": [],
   "source": [
    "# 🌟 ELU - Exponential Linear Unit\n",
    "print(\"🌟 ELU: EXPONENTIAL LINEAR UNIT\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# ELU: f(x) = x if x > 0, α(e^x - 1) if x ≤ 0\n",
    "alpha_elu = 1.0\n",
    "elu_output = tf.nn.elu(x)\n",
    "\n",
    "print(\"📝 Mathematical Definition:\")\n",
    "print(\"   f(x) = x           if x > 0\")\n",
    "print(\"   f(x) = α(e^x - 1)  if x ≤ 0\")\n",
    "print()\n",
    "print(\"🎯 Special Properties:\")\n",
    "print(\"   • Smooth function everywhere\")\n",
    "print(\"   • Negative values saturate to -α\")\n",
    "print(\"   • Can produce negative outputs\")\n",
    "print(\"   • Helps with zero-centered activations\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "elu_test = tf.nn.elu(test_values)\n",
    "\n",
    "print(\"🧪 Activation Comparison:\")\n",
    "print(\"Input\\tReLU\\tLeaky\\tELU\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"{test_values[i].numpy():4.1f}\\t{relu_test[i].numpy():4.1f}\\t{leaky_test[i].numpy():5.2f}\\t{elu_test[i].numpy():6.3f}\")\n",
    "\n",
    "# Visualize ELU\n",
    "plot_activation(x.numpy(), elu_output.numpy(), \"ELU\", color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_title"
   },
   "source": [
    "## 🌊 STEP 2: The Smooth Classics - Sigmoid & Tanh\n",
    "### 📈 The smooth operators of neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_sigmoid"
   },
   "outputs": [],
   "source": [
    "# 🌊 Sigmoid - The Probability Pioneer\n",
    "print(\"🌊 SIGMOID: The Classic Probability Function\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# Sigmoid: f(x) = 1 / (1 + e^(-x))\n",
    "sigmoid_output = tf.nn.sigmoid(x)\n",
    "\n",
    "print(\"📝 Mathematical Definition: f(x) = 1 / (1 + e^(-x))\")\n",
    "print(\"🎯 Key Properties:\")\n",
    "print(\"   • Output range: (0, 1) - perfect for probabilities!\")\n",
    "print(\"   • Smooth and differentiable everywhere\")\n",
    "print(\"   • S-shaped curve\")\n",
    "print(\"   • Can saturate (vanishing gradients)\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "sigmoid_test = tf.nn.sigmoid(test_values)\n",
    "\n",
    "print(\"🧪 Sigmoid Outputs (probabilities):\")\n",
    "for i in range(len(test_values)):\n",
    "    prob = sigmoid_test[i].numpy()\n",
    "    print(f\"   Sigmoid({test_values[i].numpy():4.1f}) = {prob:.3f} ({prob*100:.1f}%)\")\n",
    "\n",
    "print()\n",
    "print(\"🧠 Perfect for:\")\n",
    "print(\"   • Binary classification (output layer)\")\n",
    "print(\"   • Gate mechanisms (LSTM forget gates)\")\n",
    "print(\"   • Converting logits to probabilities\")\n",
    "print()\n",
    "\n",
    "# Calculate derivative for gradient visualization\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    sigmoid_out = tf.nn.sigmoid(x)\n",
    "sigmoid_grad = tape.gradient(sigmoid_out, x)\n",
    "\n",
    "# Visualize Sigmoid with its derivative\n",
    "plot_activation(x.numpy(), sigmoid_output.numpy(), \"Sigmoid\", color='purple', \n",
    "               derivative=sigmoid_grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step2_tanh"
   },
   "outputs": [],
   "source": [
    "# 🎭 Tanh - The Zero-Centered Champion\n",
    "print(\"🎭 TANH: The Zero-Centered Activation\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Tanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "tanh_output = tf.nn.tanh(x)\n",
    "\n",
    "print(\"📝 Mathematical Definition: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\")\n",
    "print(\"🎯 Key Properties:\")\n",
    "print(\"   • Output range: (-1, 1) - zero-centered!\")\n",
    "print(\"   • Stronger gradients than sigmoid\")\n",
    "print(\"   • Symmetric around origin\")\n",
    "print(\"   • Scaled and shifted sigmoid\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "tanh_test = tf.nn.tanh(test_values)\n",
    "\n",
    "print(\"🧪 Sigmoid vs Tanh Comparison:\")\n",
    "print(\"Input\\tSigmoid\\tTanh\")\n",
    "print(\"-\" * 20)\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"{test_values[i].numpy():4.1f}\\t{sigmoid_test[i].numpy():6.3f}\\t{tanh_test[i].numpy():6.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"🧠 Advantages of Tanh:\")\n",
    "print(\"   • Zero-centered outputs (better for hidden layers)\")\n",
    "print(\"   • Stronger gradients in the linear region\")\n",
    "print(\"   • Better than sigmoid for hidden layers\")\n",
    "print()\n",
    "\n",
    "# Calculate derivative\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    tanh_out = tf.nn.tanh(x)\n",
    "tanh_grad = tape.gradient(tanh_out, x)\n",
    "\n",
    "# Visualize Tanh with its derivative\n",
    "plot_activation(x.numpy(), tanh_output.numpy(), \"Tanh\", color='teal', \n",
    "               derivative=tanh_grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_title"
   },
   "source": [
    "## 🎲 STEP 3: Softmax - The Probability Wizard\n",
    "### 🧙‍♂️ Converting scores into beautiful probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_softmax"
   },
   "outputs": [],
   "source": [
    "# 🎲 Softmax - The Classification Master\n",
    "print(\"🎲 SOFTMAX: The Probability Distribution Creator\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "print(\"📝 Mathematical Definition:\")\n",
    "print(\"   softmax(x_i) = e^(x_i) / Σ(e^(x_j)) for all j\")\n",
    "print()\n",
    "print(\"🎯 Magic Properties:\")\n",
    "print(\"   • All outputs are positive\")\n",
    "print(\"   • All outputs sum to exactly 1.0\")\n",
    "print(\"   • Emphasizes the largest input\")\n",
    "print(\"   • Perfect for multi-class classification\")\n",
    "print()\n",
    "\n",
    "# Example: Multi-class classification scenario\n",
    "print(\"🎪 DEMO: Image Classification with 5 Classes\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Simulate raw scores (logits) from a classifier\n",
    "class_names = ['🐱 Cat', '🐶 Dog', '🐦 Bird', '🐟 Fish', '🐸 Frog']\n",
    "logits = tf.constant([[2.5, 1.2, 0.8, 3.1, 0.5],  # Sample 1: Fish likely\n",
    "                      [4.2, 1.0, 2.1, 0.3, 0.8],  # Sample 2: Cat likely\n",
    "                      [1.1, 1.2, 4.8, 0.9, 1.3]]) # Sample 3: Bird likely\n",
    "\n",
    "print(\"🔢 Raw Logits (classifier outputs):\")\n",
    "for i, sample in enumerate(logits):\n",
    "    print(f\"   Sample {i+1}: {sample.numpy()}\")\n",
    "print()\n",
    "\n",
    "# Apply softmax\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "print(\"✨ After Softmax (probabilities):\")\n",
    "for i, (sample_logits, sample_probs) in enumerate(zip(logits, probabilities)):\n",
    "    print(f\"\\n📊 Sample {i+1}:\")\n",
    "    print(\"   Class\\t\\tLogit\\tProbability\")\n",
    "    print(\"   \" + \"-\"*35)\n",
    "    \n",
    "    for j, (name, logit, prob) in enumerate(zip(class_names, sample_logits, sample_probs)):\n",
    "        print(f\"   {name}\\t{logit.numpy():.1f}\\t{prob.numpy():.3f} ({prob.numpy()*100:.1f}%)\")\n",
    "    \n",
    "    # Show prediction\n",
    "    predicted_class = tf.argmax(sample_probs)\n",
    "    confidence = tf.reduce_max(sample_probs)\n",
    "    print(f\"   \\n🏆 Prediction: {class_names[predicted_class]} with {confidence.numpy()*100:.1f}% confidence\")\n",
    "    \n",
    "    # Verify probabilities sum to 1\n",
    "    prob_sum = tf.reduce_sum(sample_probs)\n",
    "    print(f\"   ✅ Probability sum: {prob_sum.numpy():.6f}\")\n",
    "\n",
    "print(\"\\n🧠 Key Insights:\")\n",
    "print(\"   • Higher logits → Higher probabilities\")\n",
    "print(\"   • Softmax amplifies differences between classes\")\n",
    "print(\"   • Always produces valid probability distributions\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step3_softmax_viz"
   },
   "outputs": [],
   "source": [
    "# 📊 Visualizing Softmax Behavior\n",
    "print(\"📊 SOFTMAX BEHAVIOR VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create a simple 3-class example for visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Three different scenarios\n",
    "scenarios = [\n",
    "    ([1.0, 1.0, 1.0], \"Equal Logits\", \"Uniform distribution\"),\n",
    "    ([3.0, 1.0, 1.0], \"One High Logit\", \"Clear winner\"),\n",
    "    ([10.0, 1.0, 1.0], \"Very High Logit\", \"Dominant class\")\n",
    "]\n",
    "\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "class_labels = ['Class A', 'Class B', 'Class C']\n",
    "\n",
    "for idx, (logits_list, title, description) in enumerate(scenarios):\n",
    "    # Calculate softmax\n",
    "    logits_tensor = tf.constant(logits_list)\n",
    "    probs = tf.nn.softmax(logits_tensor)\n",
    "    \n",
    "    # Create bar plot\n",
    "    axes[idx].bar(class_labels, probs.numpy(), color=colors)\n",
    "    axes[idx].set_title(f'{title}\\n{description}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Probability')\n",
    "    axes[idx].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, prob in enumerate(probs.numpy()):\n",
    "        axes[idx].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Add logits as subtitle\n",
    "    axes[idx].text(0.5, -0.15, f'Logits: {logits_list}', ha='center', \n",
    "                   transform=axes[idx].transAxes, fontsize=10)\n",
    "\n",
    "plt.suptitle('🎲 Softmax Behavior: From Logits to Probabilities', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Observations:\")\n",
    "print(\"   • Equal logits → Equal probabilities (1/n for n classes)\")\n",
    "print(\"   • Higher logits → Much higher probabilities\")\n",
    "print(\"   • Softmax amplifies differences exponentially!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_title"
   },
   "source": [
    "## ⚡ STEP 4: Modern Activations - The New Generation\n",
    "### 🚀 State-of-the-art activations powering today's AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_swish"
   },
   "outputs": [],
   "source": [
    "# ⚡ Swish - Google's Self-Gating Activation\n",
    "print(\"⚡ SWISH: The Self-Gating Innovation\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# Swish: f(x) = x * sigmoid(x)\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "swish_output = swish(x)\n",
    "\n",
    "print(\"📝 Mathematical Definition: f(x) = x * sigmoid(x)\")\n",
    "print(\"🎯 Revolutionary Properties:\")\n",
    "print(\"   • Self-gating: uses its own value as gate\")\n",
    "print(\"   • Smooth and non-monotonic\")\n",
    "print(\"   • Better than ReLU in many tasks\")\n",
    "print(\"   • Used in EfficientNet and other SOTA models\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "swish_test = swish(test_values)\n",
    "\n",
    "print(\"🧪 Activation Comparison:\")\n",
    "print(\"Input\\tReLU\\tSwish\")\n",
    "print(\"-\" * 18)\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"{test_values[i].numpy():4.1f}\\t{relu_test[i].numpy():4.1f}\\t{swish_test[i].numpy():6.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"🔍 Key Insight: Notice how Swish allows small negative values!\")\n",
    "print(\"   This helps with gradient flow and information preservation.\")\n",
    "\n",
    "# Visualize Swish\n",
    "plot_activation(x.numpy(), swish_output.numpy(), \"Swish\", color='magenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step4_gelu"
   },
   "outputs": [],
   "source": [
    "# 🌟 GELU - The Transformer's Favorite\n",
    "print(\"🌟 GELU: Gaussian Error Linear Unit\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# GELU: f(x) = x * Φ(x) where Φ is the cumulative distribution function of the standard normal distribution\n",
    "# Approximation: f(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))\n",
    "def gelu_approx(x):\n",
    "    return 0.5 * x * (1 + tf.nn.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "gelu_output = gelu_approx(x)\n",
    "\n",
    "print(\"📝 Mathematical Concept: f(x) = x * P(X ≤ x) where X ~ N(0,1)\")\n",
    "print(\"🎯 Transformer Power:\")\n",
    "print(\"   • Probabilistic interpretation\")\n",
    "print(\"   • Smooth approximation to ReLU\")\n",
    "print(\"   • Standard in BERT, GPT, and other transformers\")\n",
    "print(\"   • Balances between ReLU and Swish\")\n",
    "print()\n",
    "\n",
    "# Test with sample values\n",
    "gelu_test = gelu_approx(test_values)\n",
    "\n",
    "print(\"🧪 Modern Activation Comparison:\")\n",
    "print(\"Input\\tReLU\\tSwish\\tGELU\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(test_values)):\n",
    "    print(f\"{test_values[i].numpy():4.1f}\\t{relu_test[i].numpy():4.1f}\\t{swish_test[i].numpy():5.2f}\\t{gelu_test[i].numpy():5.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"🤖 AI Breakthrough: GELU helps transformers understand language better!\")\n",
    "\n",
    "# Visualize GELU\n",
    "plot_activation(x.numpy(), gelu_output.numpy(), \"GELU\", color='indigo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_title"
   },
   "source": [
    "## 🎨 STEP 5: The Grand Comparison - All Activations Together\n",
    "### 🌈 See how different activations shape neural network behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_comparison"
   },
   "outputs": [],
   "source": [
    "# 🎨 The Ultimate Activation Function Comparison\n",
    "print(\"🎨 THE ULTIMATE ACTIVATION COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate all activations\n",
    "activations = {\n",
    "    'ReLU': tf.nn.relu(x),\n",
    "    'Leaky ReLU': tf.nn.leaky_relu(x, alpha=0.01),\n",
    "    'ELU': tf.nn.elu(x),\n",
    "    'Sigmoid': tf.nn.sigmoid(x),\n",
    "    'Tanh': tf.nn.tanh(x),\n",
    "    'Swish': swish(x),\n",
    "    'GELU': gelu_approx(x)\n",
    "}\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'purple', 'teal', 'magenta', 'indigo']\n",
    "\n",
    "for i, (name, activation) in enumerate(activations.items()):\n",
    "    plt.plot(x.numpy(), activation.numpy(), label=name, linewidth=3, color=colors[i])\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=14)\n",
    "plt.ylabel('Output f(x)', fontsize=14)\n",
    "plt.title('🎨 The Complete Activation Function Family', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='upper left')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "# Add some annotations\n",
    "plt.annotate('ReLU family\\n(sparse)', xy=(2, 2), xytext=(3, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
    "            fontsize=10, ha='center', bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.annotate('Smooth classics\\n(bounded)', xy=(-2, -0.8), xytext=(-3.5, -1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='purple', alpha=0.7),\n",
    "            fontsize=10, ha='center', bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "plt.annotate('Modern marvels\\n(self-gating)', xy=(1, 0.5), xytext=(1, 2),\n",
    "            arrowprops=dict(arrowstyle='->', color='magenta', alpha=0.7),\n",
    "            fontsize=10, ha='center', bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🌈 Visual Insights:\")\n",
    "print(\"   🔥 ReLU family: Sharp transitions, sparse activation\")\n",
    "print(\"   🌊 Classics: Smooth curves, bounded outputs\")\n",
    "print(\"   ⚡ Modern: Self-gating, smooth approximations to ReLU\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "step5_performance"
   },
   "outputs": [],
   "source": [
    "# ⚡ Performance and Use Case Guide\n",
    "print(\"⚡ ACTIVATION FUNCTION SELECTION GUIDE\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Create a comprehensive guide\n",
    "guide_data = {\n",
    "    'Activation': ['ReLU', 'Leaky ReLU', 'ELU', 'Sigmoid', 'Tanh', 'Swish', 'GELU'],\n",
    "    'Speed': ['⚡⚡⚡', '⚡⚡⚡', '⚡⚡', '⚡⚡', '⚡⚡', '⚡', '⚡'],\n",
    "    'Gradient Flow': ['⚠️', '✅', '✅', '⚠️', '⚠️', '✅', '✅'],\n",
    "    'Hidden Layers': ['✅', '✅', '✅', '❌', '✅', '✅', '✅'],\n",
    "    'Output Layer': ['❌', '❌', '❌', '✅ (binary)', '❌', '❌', '❌'],\n",
    "    'Best For': ['General purpose', 'Avoiding dead neurons', 'Zero-centered', 'Binary classification', 'Hidden layers', 'High performance', 'Transformers']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(guide_data)\n",
    "\n",
    "print(\"📊 Quick Reference Table:\")\n",
    "print(df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"🎯 CHOOSING THE RIGHT ACTIVATION:\")\n",
    "print()\n",
    "print(\"🏗️ For Hidden Layers:\")\n",
    "print(\"   1st choice: ReLU (fast, simple, works well)\")\n",
    "print(\"   2nd choice: Leaky ReLU (if ReLU neurons die)\")\n",
    "print(\"   3rd choice: Swish/GELU (for state-of-the-art performance)\")\n",
    "print()\n",
    "print(\"🎲 For Output Layers:\")\n",
    "print(\"   Binary classification: Sigmoid\")\n",
    "print(\"   Multi-class classification: Softmax\")\n",
    "print(\"   Regression: None (linear) or ReLU (if outputs should be positive)\")\n",
    "print()\n",
    "print(\"🚀 For Modern Architectures:\")\n",
    "print(\"   Transformers: GELU\")\n",
    "print(\"   CNNs: ReLU or Swish\")\n",
    "print(\"   RNNs: Tanh (traditional) or modern variants\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation_title"
   },
   "source": [
    "## ✅ PRACTICAL APPLICATION & VALIDATION\n",
    "### 🎪 Let's build a complete neural network with different activations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_network"
   },
   "outputs": [],
   "source": [
    "# 🎪 Building Networks with Different Activations\n",
    "print(\"🎪 ACTIVATION FUNCTION BATTLE: Same Network, Different Activations\")\n",
    "print(\"=\" * 68)\n",
    "\n",
    "# Create sample data (XOR problem - perfect for testing activations)\n",
    "X = tf.constant([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "y_true = tf.constant([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "print(\"🧩 Problem: XOR Truth Table\")\n",
    "print(\"Input\\tOutput\")\n",
    "print(\"-\" * 12)\n",
    "for i in range(4):\n",
    "    print(f\"{X[i].numpy()}\\t{y_true[i].numpy()[0]}\")\n",
    "print()\n",
    "\n",
    "def create_xor_network(activation_fn, activation_name):\n",
    "    \"\"\"Create a simple XOR network with specified activation\"\"\"\n",
    "    \n",
    "    # Network architecture: 2 → 4 → 1\n",
    "    W1 = tf.Variable(tf.random.normal([2, 4], stddev=0.5))\n",
    "    b1 = tf.Variable(tf.zeros([4]))\n",
    "    W2 = tf.Variable(tf.random.normal([4, 1], stddev=0.5))\n",
    "    b2 = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    # Forward pass\n",
    "    hidden = activation_fn(tf.matmul(X, W1) + b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, W2) + b2)  # Sigmoid for output\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = tf.reduce_mean(tf.square(y_true - output))\n",
    "    \n",
    "    return output, loss, hidden\n",
    "\n",
    "# Test different activations\n",
    "activation_tests = [\n",
    "    (tf.nn.relu, \"ReLU\"),\n",
    "    (tf.nn.tanh, \"Tanh\"),\n",
    "    (lambda x: tf.nn.leaky_relu(x, alpha=0.01), \"Leaky ReLU\"),\n",
    "    (swish, \"Swish\")\n",
    "]\n",
    "\n",
    "print(\"🔬 Testing Different Activations on XOR Problem:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "for activation_fn, name in activation_tests:\n",
    "    output, loss, hidden = create_xor_network(activation_fn, name)\n",
    "    \n",
    "    print(f\"\\n🎭 {name} Activation:\")\n",
    "    print(f\"   📉 Loss: {loss.numpy():.4f}\")\n",
    "    print(f\"   🧠 Hidden layer activations (sample):\")\n",
    "    print(f\"      Input [0,0] → {hidden[0].numpy()}\")\n",
    "    print(f\"   🎯 Predictions:\")\n",
    "    \n",
    "    for i in range(4):\n",
    "        pred = output[i].numpy()[0]\n",
    "        true_val = y_true[i].numpy()[0]\n",
    "        error = abs(pred - true_val)\n",
    "        print(f\"      {X[i].numpy()} → {pred:.3f} (target: {true_val}, error: {error:.3f})\")\n",
    "    \n",
    "    results.append((name, loss.numpy()))\n",
    "\n",
    "print(\"\\n🏆 ACTIVATION PERFORMANCE RANKING:\")\n",
    "print(\"=\" * 35)\n",
    "results.sort(key=lambda x: x[1])  # Sort by loss\n",
    "for i, (name, loss) in enumerate(results, 1):\n",
    "    medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else \"📊\"\n",
    "    print(f\"{medal} {i}. {name}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"\\n💡 Remember: This is just one random initialization!\")\n",
    "print(\"   Real performance depends on training, data, and architecture.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "takeaways"
   },
   "source": [
    "## 🔍 KEY TAKEAWAYS\n",
    "\n",
    "### 🎭 **Activation Function Mastery:**\n",
    "\n",
    "1. **🔥 ReLU Family** - The workhorses of deep learning\n",
    "   - Simple, fast, and effective\n",
    "   - Watch out for dying ReLU problem\n",
    "   - Leaky ReLU and ELU solve the dying neuron issue\n",
    "\n",
    "2. **🌊 Smooth Classics** - Still relevant in specific contexts\n",
    "   - Sigmoid: Perfect for binary classification output\n",
    "   - Tanh: Better than sigmoid for hidden layers (zero-centered)\n",
    "   - Both can suffer from vanishing gradients\n",
    "\n",
    "3. **🎲 Softmax** - The probability master\n",
    "   - Converts any vector to valid probability distribution\n",
    "   - Essential for multi-class classification\n",
    "   - Amplifies differences between classes\n",
    "\n",
    "4. **⚡ Modern Innovations** - State-of-the-art performance\n",
    "   - Swish: Self-gating, smooth, non-monotonic\n",
    "   - GELU: The transformer standard, probabilistic interpretation\n",
    "   - Often outperform ReLU in complex tasks\n",
    "\n",
    "### 🎯 **Selection Guidelines:**\n",
    "- **Hidden layers:** Start with ReLU, upgrade to Swish/GELU for performance\n",
    "- **Binary output:** Sigmoid\n",
    "- **Multi-class output:** Softmax\n",
    "- **Regression output:** Linear (no activation) or ReLU if positive\n",
    "\n",
    "### 🧠 **Deep Understanding:**\n",
    "- Activations introduce non-linearity (the magic ingredient!)\n",
    "- Different activations create different learning dynamics\n",
    "- Gradient flow is crucial for deep networks\n",
    "- Modern activations often outperform classics\n",
    "\n",
    "### 🤔 **Questions to Explore:**\n",
    "- How do activation choices affect training speed?\n",
    "- Why do transformers prefer GELU over ReLU?\n",
    "- What happens if you use different activations in different layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_exercise"
   },
   "source": [
    "## ➡️ NEXT EXERCISE PREVIEW\n",
    "\n",
    "### 📊 T3-Exercise-4: Reduction Operations - Aggregating Intelligence\n",
    "\n",
    "**Get ready to master:**\n",
    "- 🔢 **Sum, Mean, Max, Min** - The fundamental aggregators\n",
    "- 📐 **Axis operations** - Understanding dimensions and broadcasting\n",
    "- 📊 **Statistical operations** - Variance, standard deviation, moments\n",
    "- 🎯 **Neural network applications** - Loss functions, batch statistics, attention\n",
    "- 🧠 **Practical scenarios** - Building actual loss functions and metrics\n",
    "\n",
    "🔮 **Coming up:** Learn how neural networks summarize and aggregate information!\n",
    "\n",
    "---\n",
    "\n",
    "# 🎉 EXERCISE 3 COMPLETED!\n",
    "## 🎭 **You've mastered the soul of neural networks!**\n",
    "### ⚡ **Now you understand how linear operations become intelligent behavior!**\n",
    "#### 🚀 **Ready to aggregate intelligence with reduction operations!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}